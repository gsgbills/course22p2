{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes\n",
    "L14 2022p2\n",
    "\n",
    "See also [Simple Neural Net Backward Pass - Deriving the math of the backward pass for a simple neural net](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preliminaries: imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the MNIST data as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture\n",
    "Lets start by defining a few variables: `n` is the number of training examples, `m` is the number of pixels, `c` is the number of possible values of our digits.\n",
    "Here there are 50,000 training samples, 784 pixels and 10 possible outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1   # number of values\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide (ahead of time) how many \"line segments\" to add up. \n",
    "The number in a layer is the *number of hidden nodes or activations*, `nh`.\n",
    "Lets arbitrarily decide `nh=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create lots of \"lines\", which we are then going to truncate at zero we do a matrix multiplication. \n",
    "Later we're going to have 50000x784 to multiply by a 784x10.\n",
    "But to simplify our starting point, lets we give layer 2 just 1 output, so we can use MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)\n",
    "b2 = torch.zeros(1)\n",
    "w1.shape, b1.shape, w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also lets use the smaller `x_valid` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear layer `lin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call it and should return a `[10000,50]` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 50]),\n",
       " tensor([[ -0.09,  11.87, -11.39,  ...,   5.48,   2.14,  15.30],\n",
       "         [  5.38,  10.21, -14.49,  ...,   0.88,   0.08,  20.23],\n",
       "         [  3.31,   0.12,   3.10,  ...,  16.89,  -6.05,  24.74],\n",
       "         ...,\n",
       "         [  4.01,  10.35, -11.25,  ...,   0.23,  -5.30,  18.28],\n",
       "         [ 10.62,  -4.27,  10.72,  ...,  -2.87,  -2.87,  18.23],\n",
       "         [  2.84,  -0.22,   1.43,  ...,  -3.91,   5.75,   2.12]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_valid, w1, b1)\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n",
       "        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n",
       "        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n",
       "        ...,\n",
       "        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n",
       "        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n",
       "        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(x): return x.clamp_min(0.)\n",
    "\n",
    "t = relu(t); t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define our basic MLP from scratch.  It would be:\n",
    "```python\n",
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compressed model\n",
    "def model(xb):\n",
    "    return lin(relu(lin(xb, w1, b1)), w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: `mse` is not a suitable loss function for multi-class classification; We'll use `mse` for now to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just substract, broadcasting creates a problem as it creates a huge matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-y_valid).shape   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of that trailing axis of `res` (,1), in order to use `mse`.\n",
    "We either use the single column of `res` or we `squeeze` `res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,0].shape  # either use the single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.squeeze().shape  # Or use squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res[:,0]-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use MSE we need the values of the labels `y` to be floats, but they are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4), tensor(4.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2], y_train[2].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our training and validation into floats because we're using MSE.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate our predictions for the training set, `x_train`, which is `[50000,1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an `mse` function that does the subtraction of the passed arguments, squares it `.pow(2)` and takes the mean.\n",
    "\n",
    "And apply this `mse` loss function to the predictions of our model `preds`, and the labels of the training set, `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4308.76)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n",
    "\n",
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass\n",
    "\n",
    "See also [Simple Neural Net Backward Pass - Deriving the math of the backward pass for a simple neural net.](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [SymPy](https://www.sympy.org/en/index.html) a Python library for symbolic mathematics. \n",
    "Wolfram Alpha does something similar. \n",
    "With SimPy we can do it inside a notebook and include it in prose.<br>\n",
    "For example, we define two symbols `x` and `y`, \n",
    "we tell it to differentiate $x^3$ with respect to `x`, and SimPy will answer $3x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 3 x^{2}$"
      ],
      "text/plain": [
       "3*x**2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**3, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To differentiate $3x^2 + 9$ with respect to $x$:  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x$"
      ],
      "text/plain": [
       "6*x"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lin_grad` computes the gradient of a linear layer.\n",
    "Per the chain rule, we need: the input `inp`, output `out`, weights `w`, and the biases `b` of the layer.\n",
    "We will store the gradients of our input in `inp.g`, \n",
    "which is `out.g @ w.t()` the gradients of `out` with respect to the input times the weights.\n",
    "A matrix multiplier is a whole bunch of linear functions, so each one slope is just its weight.  \n",
    "But we have to multiply it by the gradient of the outputs because of the chain rule.  \n",
    "The gradient of the outputs with respect to the weights, `w.g`, is the input times the output summed up.\n",
    "Every input weights has to be multiplied by the outputs, that's why we have to  do an `unsqueeze(-1)`. \n",
    "The  derivatives of the bias, `b.g`, is the gradients of the output added together\n",
    "because the bias is just a constant value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):  #the gradient of a linear layer\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    #import pdb; pdb.set_trace()\n",
    "    i, o = inp.unsqueeze(-1) , out.g.unsqueeze(1)\n",
    "    w.g = (i * o).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **forward** pass is where we calculate the `loss`, which is diff (the output `out` \n",
    "of the neural net minus our target) squared and then take the mean. \n",
    "`out` is the output of the 2nd linear layer `l2`. \n",
    "The input to `l2` is the ReLU, and the ReLU's input is the first layer, `l1`. \n",
    "We take the input,`inp` put it through a linear layer `l1`, through a ReLU, \n",
    "through a linear layer `l2` and calculate the MSE.\n",
    "\n",
    "In the **backward** pass, we store the gradients of each layer (e.g., loss with respect to inputs), in the layer itself.\n",
    "We define a new attribute, `g`. \n",
    "We define a new attribute called out.g, to contain the gradients.\n",
    "In `out.g = 2.*diff[:,None] / inp.shape[0]`\n",
    "the derivative is two times the difference because we've got difference squared.  \n",
    "We took the mean when computing the loss, so we have to do the same thing here, i.e., divided by the input shape.\n",
    "Now we need to multiply by the gradients of the previous layer, `l2`.\n",
    "To compute the gradients of a linear layer we use `lin_grad` above.\n",
    "Per the chain rule, we need: the weights `w2` and the biases `b2` of the `l2` layer, \n",
    "and also the input `l2` and the output `out` from the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]  # the gradients saved\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save all the gradients for `w1,w2,b1,b2,x_train`,\n",
    "in a list `grads` (for testing against the Pytorch equivalents later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = map(get_grad, chks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save all the gradients for `w12,w22,b12,b22,xt2`, the equivalents to before in a list `ptgrads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = map(mkgrad, chks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We just run it all through PyTorch and check that their derivatives `ptgrads` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cheat a little bit and use PyTorch autograd to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the calculated derivatives by comparing them with the same derivatives calculated by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(grads, ptgrads): test_close(a.grad, b, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refactor and simplify by using classes and invoking them as functions.\n",
    "Lets illustrate by a class just to print hello.\n",
    "We create an instance of that class and then we can call it as if it was a function.\n",
    "In Python we can change how a class behaves, make it work like a function, by defining `__call__`. \n",
    "It is a syntactic sugar to treat a class as if it's a function without any method  at all. \n",
    "We can still do it the method way, but why do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi j\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __call__(self, x): print(f'hi {x}')\n",
    "    \n",
    "a = A()\n",
    "a(\"j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets simplify and define classes for `ReLU` and for the linear function `Lin`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the ReLU class and add `__call__` so we can treat it as a function. \n",
    "Note that the backward pass has to know about the intermediate calculations\n",
    "because of the chain rule, and because of how the derivatives are calculated.\n",
    "We need to store each of the layer intermediate calculations.\n",
    "The ReLU class stores its output and its input, so when we call the `backward` method, \n",
    "we know how to calculate that. \n",
    "We set the inputs gradient, `self.inp.g` by the chain rule the product of 2 derivatives,\n",
    "`(self.inp>0).float() * self.out.g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same thing for a linear layer class `Lin`. \n",
    "A linear layer needs some additional state to be passed: weights and  biases. (ReLU doesn't). \n",
    "We indicate its weights `w` and biases `b`, and store them.\n",
    "When we `__call__` it on the forward pass we store the input `inp`, then \n",
    "compute the output, store it in `self.out`, and `return` it. <br>\n",
    "For the backward pass, the input gradients we calculate as before. \n",
    "`.t()` is the same as `T` is as a property: the transpose.  \n",
    "We calculate the gradients of `inp, w, b` and store them in the appropriate `.g` places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below to compute the derivation of the gradients in `backward()`:\n",
    "```python      \n",
    "dJ_dZ = self.out.g  # Gradient of loss with respect to the output     \n",
    "\n",
    "self.w.g = dJ_dW = self.inp.t() @ dJ_dZ # Gradient of loss with respect to w_j     \n",
    "\n",
    "self.b.g = dJ_db = dJ_dZ.sum(0)   #Gradient of loss with respect to the bias b\n",
    "\n",
    "self.inp.g = dJ_dX = dJ_dZ @ self.w.t() # Gradient of loss with respect to X\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()  # See Gradient of loss with respect to X   \n",
    "        self.w.g = self.inp.t() @ self.out.g # See Gradient of loss with respect to w_j\n",
    "        self.b.g = self.out.g.sum(0) # See Gradient of loss with respect to the bias b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward function of the Mse class below computes an estimate of how the loss function changes as the input activations change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MSE we do the same thing we calculate it and store it in `self.out`. \n",
    "MSE needs input and target, so we store them. \n",
    "In the backward pass we can calculate its gradient of the input as being two times the difference. \n",
    "For the backward we compute it as:\n",
    "```python\n",
    "N = self.targ.shape[0] ;  A = self.inp ; Y = self.targ\n",
    "self.inp.g = dJ_dA = (2./N) * (A.squeeze() - Y).unsqueeze(-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th model is easier to define as a list of layers, `[Lin(w1,b1), Relu(), Lin(w2,b2)]`. \n",
    "We store in `self.loss` an instance of `Mse()`.\n",
    "NB: These are not calls, just instances of the classes (`Lin, Relu, Mse`) being stored, \n",
    "so when we call the model we pass it our inputs and our target. \n",
    "In `__call__` we go through each layer, set `x` equal to the result of calling that layer, \n",
    "and then pass that to the `loss`. \n",
    "<br>\n",
    "Notice that we don't have two separate functions, the loss function being applied to a separate neural net.\n",
    "Rather we integrated the loss function into the model, i.e., the loss is calculated inside the model.\n",
    "That is different, neither better nor worse than having it separately.\n",
    "HuggingFace stuff does it this way, it puts the `loss` inside the `forward`.\n",
    "Fastai and other libraries does it separately, i.e., loss is a whole separate function, \n",
    "and the model only returns the result of putting it through the layers.\n",
    "For this model the loss function is inside the model.\n",
    "<br>\n",
    "For backward, `self.loss` is the `Mse()` object. \n",
    "So that's going to call `loss.backward()`, and it's stored when it was called here.  \n",
    "It stores  the inputs, the targets, the outputs, so it can calculate the `backward()`.\n",
    "Then we go through each layer in reverse, the **back propagation**, `backwards` `reversed`, \n",
    "calling `l.backward()` on each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: if we just return the `loss` above in the `__call__`, how do you get predictions?  \n",
    "A: HuggingFace models return not just the `loss`, but a dictionary, \n",
    "i.e., `dict(loss=..., preds=...)`, something like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the `Model`, calculate the `loss`, and call `backward`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "\n",
    "loss = model(x_train, y_train)\n",
    "\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can check that each of the gradients that we stored earlier are equal to each of our new gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()\n",
    "\n",
    "Repeated code,  e.g., `self.inp=inp`, etc., is a sign that we can refactor things.  \n",
    "Lets define a new class `Module()` to do those things that are repeated. \n",
    "It's going to store the inputs, call `self.forward` to create  the `self.out`, and then return it. \n",
    "There's going to be a `forward` which in this class it doesn't do anything,\n",
    "as the purpose of this Module is to be inherited. \n",
    "When we call backward, it's going to call `self.bwd` passing in 2 arguments:\n",
    "(1) `self.out` because all `backwards()` wanted to get `self.out` because of the chain rule,\n",
    "and (2)the arguments that we stored earlier. \n",
    "`*` in a signature (e.g., `def __call__(self, *args)`\n",
    "means take all of the arguments, regardless of number, and put them into a list. \n",
    "When we call a function using `*`, e.g., `self.bwd(self.out, *self.args)`\n",
    "it says take this list and expand them into separate arguments,\n",
    "and pass them (e.g. to `self.bwd`)  each one separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are often opportunities to manually speed-up by defining custom Pytorch autograd functions.\n",
    "For example, in `Mse` a calculation  `inp.squeeze() - targ` is being done twice.\n",
    "At the cost of some memory, we could instead store that calculation as, e.g., `self.diff`.\n",
    "And at the cost of that memory, we could now remove this redundant calculation \n",
    "because we've done it once before already and  stored it and just use it directly. \n",
    "This is something that you can often do in neural nets, \n",
    "a compromise between memory use and then the computational speedup of not having to recalculate it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call it in the same way, create the model, passing in all of those layers. \n",
    "The model  hasn't changed at this point. \n",
    "The definition was up here, we just pass in the weights for the layers,\n",
    "calculate the loss, call backward, and it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "\n",
    "loss = model(x_train, y_train)\n",
    "\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "PyTorch has all this, and since we've reimplemented it, we can use PyTorch's version, `nn.Module`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a `Linear` layer we inherit from `nn.Module`.\n",
    "Here rather than passing in the already randomized weights, we generate the random weights and the zeroed biases.\n",
    "We define `forward` but we don't need to define `backward`, as PyTorch \n",
    "knows all the derivatives and the chain rule, it will do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a `Model` class that uses `nn.Module`, it's the same as before,\n",
    "but now we use PyTorch's `F.mse_loss()`.<br>\n",
    "NB: We need the extra axis in `targ[:,None]` as we saw the problem if we don't have it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model, call backward.\n",
    "We stored our gradients in `.g`, PyTorch stores them in `.grad`.\n",
    "The same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n",
       "          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n",
       "        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n",
       "        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we've created from scratch:\n",
    "* a matrix multiplication\n",
    "* linear layers\n",
    "* a complete backprop system of modules \n",
    "\n",
    "We can now calculate both the forward pass and the  backward pass for linear layers and values so\n",
    "we can create a multilayer perceptron, and we can train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
