{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch,random\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil\n",
    "import matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from fastcore.test import test_close\n",
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "ds_builder.info.features\n",
    "xl,yl = ds_builder.info.features\n",
    "num_classes = ds_builder.info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cf6c9d88be4e82a6ad048884adb217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#xl,yl = 'image','label'\n",
    "#name = \"fashion_mnist\"\n",
    "bs = 1024\n",
    "xmean,xstd = 0.28, 0.35\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "iw = partial(init_weights, leaky=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "lr,epochs = 6e-2,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Going wider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the convnet and replaced it with a Resnet, so it's effectively twice as deep,\n",
    "as each conv block has 2 convolutions. \n",
    "But Resnets train better than convNets, so we could go deeper and wider.\n",
    "Previously we were going from 8 up to 256 filters. \n",
    "Could we get up to 512?\n",
    "We can make the first `ResBlock` layer have a kernel size `ks=5` (instead of 3), and `stride=1`.\n",
    "Each grid is 5x5, 25 inputs, so it is fair to have 16 outputs.\n",
    "Then as we keep doubling (`nfs`) we end up at 512 filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), nn.Linear(nfs[-1], num_classes, bias=False), nn.BatchNorm1d(num_classes)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training 5 epochs with `lr=1e-2` accuracy is .925. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [1/5 05:41&lt;22:47]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.823</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.846</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='26' class='' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      44.07% [26/59 02:32&lt;03:13 0.375]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:190\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epochs, train, valid, cbs, lr)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_func: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr)\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m cbs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs\u001b[38;5;241m.\u001b[39mremove(cb)\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:141\u001b[0m, in \u001b[0;36mwith_cbs.__call__.<locals>._f\u001b[0;34m(o, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mException\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:178\u001b[0m, in \u001b[0;36mLearner._fit\u001b[0;34m(self, train, valid)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, train, valid):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m train: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m valid: torch\u001b[38;5;241m.\u001b[39mno_grad()(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_epoch)(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:173\u001b[0m, in \u001b[0;36mLearner.one_epoch\u001b[0;34m(self, training)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(training)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mvalid\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:141\u001b[0m, in \u001b[0;36mwith_cbs.__call__.<locals>._f\u001b[0;34m(o, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mException\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:168\u001b[0m, in \u001b[0;36mLearner._one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_one_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:141\u001b[0m, in \u001b[0;36mwith_cbs.__call__.<locals>._f\u001b[0;34m(o, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     o\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mException\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:160\u001b[0m, in \u001b[0;36mLearner._one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_backward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:207\u001b[0m, in \u001b[0;36mTrainLearner.backward\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make the Resnet more flexible.\n",
    "The current Resnet requires the number of stride-2 layers to be big enough so \n",
    "that the last `ResBlock` ends with a 1x1 output, so we can `Flatten` it and do the `Linear`.\n",
    "That's not flexible, e.g., if we've got images of a different size. \n",
    "28x28 is a pretty small image size.\n",
    "Lets create `get_model2` which has one less layer, only goes up to 256 despite starting at 16. \n",
    "Because it's got one less layer, the last `ResBlock` will be 2x2 not 1x1. \n",
    "We can the take the mean over the 2x2, returning a tensor of batch size (1024) by Channel's output (256), i.e., 1024x256.\n",
    "which we can then pass into the `Linear` layer.\n",
    "This \"mean\" is called (Keras naming) a *Global Average Pooling* `GlobalAvgPool` layer.\n",
    "In Pytorch it is called an *Adaptive Average Pooling* layer, and can have an output other than 1x1.\n",
    "(Nobody uses it that way so they're basically the same.)\n",
    "`GlobalAvgPool` is more convenient (than Pytorch's version) because we don't have to `Flatten` it.\n",
    "After our last `ResBlock` which returns a 2x2 output we have a `GlobalAvgPool()` layer,\n",
    "and then we can do the `Linear` and `BatchNorm1d` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool(nn.Module):\n",
    "    def forward(self, x): return x.mean((-2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model2(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [ResBlock(nfs[-1], nfs[-1]*2, act=act, norm=norm), GlobalAvgPool()]\n",
    "    layers += [nn.Linear(nfs[-1]*2, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate of Computation to improve Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets improve the `Learner.summary()` with a `@patch` to include an estimate of computation, \"MFLOPS\".\n",
    "This calculation is not exact, (it's not flops), rather an indicative approximation \n",
    "count of the number of multiplications.\n",
    "The `_flops` function gets a weight matrix `x`, the height `h` and the width `w` of the grid.\n",
    "If the number of dimensions `x.dim() < 3`, then we're doing e.g., a linear layer, \n",
    "and we return the number of elements `x.numel()` as estimate of the number of \"flops\".\n",
    "If we're doing a convolution, `x.dim==4`, we return `x.numel()*h*w`.\n",
    "That's how we calculate and return a proxy for flops... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _flops(x, h, w):\n",
    "    if x.dim()<3: return x.numel()\n",
    "    if x.dim()==4: return x.numel()*h*w\n",
    "    raise Exception()\n",
    "\n",
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num params|MFLOPS|\\n|--|--|--|--|--|\\n'\n",
    "    totp,totf = 0,0\n",
    "    def _f(hook, mod, inp, outp):\n",
    "        nonlocal res,totp,totf\n",
    "        nparms = sum(o.numel() for o in mod.parameters())\n",
    "        totp += nparms\n",
    "        *_,h,w = outp.shape\n",
    "        flops = sum(_flops(o, h, w) for o in mod.parameters())/1e6\n",
    "        totf += flops\n",
    "        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparms}|{flops:.1f}|\\n'\n",
    "    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, cbs=SingleBatchCB())\n",
    "    print(f\"Tot params: {totp}; MFLOPS: {totf:.1f}\")\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else: print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run `model2` the number of parameters went from 1.2M up to 4.9M,\n",
    "because of the last `ResBlock` that gets up to 512 filters (adds 3.6M parameters). \n",
    "The last `ResBlock` has a default `stride=1` layer, so it stayed at 2x2, \n",
    "to make it as similar as possible with the last ones.\n",
    "It's got the same 512 final number of filters, so most of the parameters are in that last block.\n",
    "NB: The last one has 73% of the total parameters, but only 45% of the MFLOPS. \n",
    "This is because the first (input) layer has to be done 28x28 times, \n",
    "while the last ResBlock only has to be done 2x2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model2(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model2(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training `model2` got a similar result .927"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model3 Reduce number of Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would reduce the number of parameters (memory) and megaflots (compute)?\n",
    "In `model3` lets remove the last ResBlock (that takes it up to 512), \n",
    "and the number of parameters goes down to (25%)  (4.9M to 1.2M).\n",
    "Not as big an impact on the megaflops (55%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [GlobalAvgPool(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model3(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first `ResBlock`, we have 5.3 MFLOPS because the 2nd conv is 16x16 by 5x5.<br>\n",
    "This is showing how to investigate what's going on in a model.\n",
    "TODO: try these investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in get_model3()[0].parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train it, and even though it's only a quarter of the size we get about the same accuracy .926."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model3(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model4 Make it faster by replacing 1 ResBlock by one convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make it faster?\n",
    "The first (input) `ResBlock` has the bulk of the compute.\n",
    "There the 2nd conv is 16x16 channels out by 5x5 kernels, and it's doing it across the whole 28x28 grid.\n",
    "We can replace the first `ResBlock` with just one convolution, and we've got rid of the 16x16 by 5x5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model4(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [conv(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in get_model4()[0].parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just got the 16x1 by 5x5 and the Total MFLOPS is down to 73% from 18.3 to 13.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model4(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters hasn't changed much (it was only 6884).\n",
    "<mark>When a \"model has less parameters\", it doesn't mean it's faster.</mark>\n",
    "The relationship between parameters and speed doesn't take account the time for things moving through memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model4(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model4` has less compute (Megaflops) and about the same accuracy.\n",
    "We've built something that is fast small and accurate: \n",
    "has less parameters and less Megaflops and has the same accuracy.\n",
    "An important thing to keep in mind.\n",
    "This model is better than the resnet18d from `timm`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we train for longer? \n",
    "If we train for 20 epochs the training accuracy gets up to .999, but the validation accuracy is worse, .924.\n",
    "After 20 epochs it's seen the same image so many times it's memorizing them, and things go downhill.\n",
    "We need to regularize.\n",
    "We have claimed that we can regularize using weight decay, but here weight Decay doesn't regularize at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 20 epochs without augmentation:\n",
    "\n",
    "```\n",
    "{'accuracy': '0.999', 'loss': '0.012', 'epoch': 19, 'train': True}\n",
    "{'accuracy': '0.924', 'loss': '0.284', 'epoch': 19, 'train': False}\n",
    "```\n",
    "\n",
    "With batchnorm, weight decay doesn't really regularize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> (*for years people didn't notice this, until a paper pointed this out*) </span>\n",
    "A batchNorm layer has a single set of coefficients which multiplies an entire layer.\n",
    "That set of coefficients could just be e.g., the number 100 in every place.\n",
    "That's going to multiply the entire previous (weights or convolution kernel) matrix by 100.\n",
    "As far as weight Decay is concerned that's not much of an impact,\n",
    "because the batchNorm layer has very few weights.\n",
    "So it doesn't really have a huge impact on weight decay, but it increases the effective scale of the weight Matrix.\n",
    "<br>\n",
    "batchNorm lets the ANN \"cheat\" by increasing the coefficients the parameters \n",
    "nearly as much as it wants indirectly just by changing the batch normally as weights.\n",
    "<mark> Weight decay is not going to save us with batchNorm layers.</mark> \n",
    "Important to recognize, JH doesn't see the point of it.\n",
    "Studies of BatchNorm show that it has some 2nd order effects on the LR.\n",
    "We should use a scheduler for changing the LR rather than 2nd order effects caused by weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to modify every image a little bit by a random change,\n",
    "so that the ANN doesn't see the same image each time.\n",
    "As this is a separate area, we don't implement these from scratch...\n",
    "(Can look them up in FastAI vision augment, e.g., flip, cropping, padding, etc.\n",
    "FastAI probably has the best implementation but `torchvision`'s are fine, so we'll use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created before a `BatchTransformCB` callback for normalization.\n",
    "We could create a transform batch function `tfm_batch` which transforms the inputs `tfm_x` \n",
    "and transforms the outputs `tfm_y` using two different functions.\n",
    "That would be an augmentation callback.\n",
    "We want to transform our X's using `tfms`, a `Sequential` module,\n",
    "which first does a random crop `RandomCrop` and then a random horizontal flip `RandomHorizontalFlip`.\n",
    "It is weird to randomly crop a 28x28 image to get a 28x28 image,  \n",
    "but we can add padding to it and so it's going to randomly add padding on one or both sides\n",
    "to do this kind of random crop.\n",
    "\n",
    "The `BatchTransformCB` has `on_train` and `on_val` arguments (see `BatchTransformCB??`) so that it only does it \n",
    "if we say we want to do it on training and it's training,\n",
    "or we want to do it on validation and it's not training.\n",
    "\n",
    "Data augmentation shouldn't be done on validation so we set `on_valid=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n",
    "\n",
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=4),\n",
    "                     transforms.RandomHorizontalFlip())\n",
    "\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n",
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[SingleBatchCB(), augcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation shouldn't be done on validation so we set `on_val=False`.\n",
    "First we use `SingleBbatchCB()` and `fit(1)` just doing training.\n",
    "The we grab the batch out of the learner, to see exactly what the model sees.\n",
    "This is not relying on any approximations.\n",
    "When we fit it puts it in the batch that looks at into learn.batch.\n",
    "So if we fit for a single batch we can then grab that batch back, and we can show images.\n",
    "We can see a little crop it's added.\n",
    "Notice that every single image in this batch (lets grab the first 16) has exactly the same augmentation,\n",
    "as we're applying a batch transform.\n",
    "This is good because it is running on the GPU, and it's really hard to get enough CPU to feed a fast GPU.\n",
    "Particularly on some platforms (kaggle, collab) that are really underpowered for CPU.\n",
    "This way all of Transformations augmentation is happening on the GPU.\n",
    "On the downside, there's less variety as every minibatch has the same augmentation.\n",
    "The downside may not matter because it's going to see lots of minibatches, and\n",
    "each minibatch is going to have a different augmentation.\n",
    "If we run this multiple times it's got a different augmentation in each minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "@fc.delegates(show_images)\n",
    "def show_image_batch(self:Learner, max_n=9, cbs=None, **kwargs):\n",
    "    self.fit(1, cbs=[SingleBatchCB()]+fc.L(cbs))\n",
    "    show_images(self.batch[0][:max_n], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_image_batch(max_n=16, imsize=(1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use one padding, a small amount of data augmentation, 20 epochs using onecycleLR.\n",
    "Takes a while to train and gets .940."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asked Twitter if anybody beat this in 20 epochs any model/Library and nobody's got close.\n",
    "In papers with code there are better models but use 250 or more epochs.\n",
    "TODO: find a way to beat this in 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom collation function could let you do per-item transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this takes a while to train, we should `torch.save` it so we can load that back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "torch.save(learn.model, mdl_path/'data_aug.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTA does the batch transform callback on validation too.\n",
    "Here we do a simple TTA, adding a batch transform callback that runs on validate.\n",
    "It's not random, just does a horizontal flip.\n",
    "A new callback `CapturePreds()`, after each batch appends to a list `all_preds` the predictions, \n",
    "and appends to `all_targs` the targets. <br>\n",
    "<mark> NB: The code below was changed after the class, added `after_fit`, `@fc.patch`, etc </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CapturePreds(Callback):\n",
    "    def before_fit(self, learn): self.all_inps,self.all_preds,self.all_targs = [],[],[]\n",
    "    def after_batch(self, learn):\n",
    "        self.all_inps. append(to_cpu(learn.batch[0]))\n",
    "        self.all_preds.append(to_cpu(learn.preds))\n",
    "        self.all_targs.append(to_cpu(learn.batch[1]))\n",
    "    def after_fit(self, learn): \n",
    "        self.all_preds,self.all_targs,self.all_inps = map(torch.cat, [self.all_preds,self.all_targs,self.all_inps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "def capture_preds(self: Learner, cbs=None, inps=False):\n",
    "    cp = CapturePreds()\n",
    "    self.fit(1, train=False, cbs=[cp]+fc.L(cbs))\n",
    "    res = cp.all_preds,cp.all_targs\n",
    "    if inps: res = res+(cp.all_inps,)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `learn.fit(1, train=False, .....)` and it will show us the accuracy.\n",
    "This is the same number that we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1, at = learn.capture_preds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the same thing but this time with a different callback,\n",
    "the horizontal flip callback, `TF.hflip`. <br>\n",
    "It does the same thing as before, but every time it's going to do a horizontal flip.\n",
    "Accuracy is slightly higher (??)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttacb = BatchTransformCB(partial(tfm_batch, tfm_x=TF.hflip), on_val=True)\n",
    "ap2, at = learn.capture_preds(cbs=[ttacb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've now got two sets of predictions: with and without the flipped version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1.shape,ap2.shape,at.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stack those together and take the average of the flipped and unflipped predictions.\n",
    "That gives us a better result .94, which is better because we are looking at\n",
    "the image from multiple (here 2) different directions, (flipped and unflipped), \n",
    "it gives it more opportunities to understand what this is a picture of.\n",
    "This is a bit like bagging, getting multiple predictions and bringing them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = torch.stack([ap1,ap2]).mean(0).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".942 is the best 20 epoch result and notice we didn't have to do any additional training.\n",
    "It still counts as a 20 Epoch result.\n",
    "We can do test time augmentation with a much wider range of different augmentations that you trained with.\n",
    "And then you can use them at test time too, crops, rotations, warps, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((ap==at).float().mean().item(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to delete a little bit of each picture and replace it with some random gaussian noise.\n",
    "In this case we've just got one patch but eventually we're going to do more than one patch.\n",
    "We have to implement everything from scratch and this one's a bit less trivial than the previous transforms.\n",
    "\n",
    "Let's grab the first 16 images of a batch out of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm,xs = xbt.mean(),xbt.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt.min(), xbt.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting a patch would change the statistics.\n",
    "But if we replace the image with exactly the same mean and standard deviation pixels that the picture has,\n",
    "then it won't change the statistics.\n",
    "Let's say we want to delete `pct=0.2` of the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how big that size is.\n",
    "0.2 of the shape is the height and of the width that's the size of the X and Y.\n",
    "Forn the starting point we're just going to randomly grab some starting point.\n",
    "The starting points for X and Y, `stx=14` and `sty=0`, and the patch is a 5x5 spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szx = int(pct*xbt.shape[-2])\n",
    "szy = int(pct*xbt.shape[-1])\n",
    "stx = int(random.random()*(1-pct)*xbt.shape[-2])\n",
    "sty = int(random.random()*(1-pct)*xbt.shape[-1])\n",
    "stx,sty,szx,szy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a gaussian or normal initialization of our minibatch.\n",
    "Everything in the batch, every channel for this x slice this y slice,\n",
    "and we're going to initialize it with this mean and standard deviation normal random noise.\n",
    "\n",
    "<mark> Don't start by writing a function, start by writing single lines of code that can run independently,\n",
    "make sure that they work, e.g., look at the pictures.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.normal_(xbt[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that looks wrong is that some of the backgrounds looks black and others gray.\n",
    "At first this was confusing, what's the change, as the original images didn't look like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the minimum and the maximum have changed, from -0.8,2  to now -3,3. \n",
    "The noise we added has the same mean and standard deviation but it doesn't have the same range \n",
    "because the pixels were not normally distributed originally.\n",
    "So normally distributed noise is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt.min(), xbt.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_rand_erase1` fixes this. It does all the same as before,\n",
    "but it `x.clamps_` the random pixels to be between `mn` and `mx`.\n",
    "It makes sure that it doesn't change the range, important as the range impacts the activations a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _rand_erase1(x, pct, xm, xs, mn, mx):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    init.normal_(x[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs)\n",
    "#    import pdb; pdb.set_trace()\n",
    "    x.clamp_(float(mn), float(mx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the backgrounds have that nice black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "_rand_erase1(xbt, 0.2, xbt.mean(), xbt.std(), xbt.min(), xbt.max())\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's still giving us random pixels.\n",
    "Because of the clamping the mean and standard deviation aren't 0-1, but they're very very close.\n",
    "And `min` and `max` haven't changed because we clamped them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt.mean(),xbt.std(),xbt.min(), xbt.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a  `rand_erase` that will randomly choose up to (e.g., 4 blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rand_erase(x, pct=0.2, max_num = 4):\n",
    "    xm,xs,mn,mx = x.mean(),x.std(),x.min(),x.max()\n",
    "    num = random.randint(0, max_num)\n",
    "    #num = random.randint(1, max_num)\n",
    "    for i in range(num): _rand_erase1(x, pct, xm, xs, mn, mx)\n",
    "    # print(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what this data augmentation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "rand_erase(xbt, 0.2, 4)\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `RandErase` will do this data augmentation.\n",
    "We pass in what percentage to do in each block, what is the maximum number of blocks to have it store that away.\n",
    "In `forward()` we call `rand_erase()` function passing in the input `x`, and the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RandErase(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_erase(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `RandomCrop` `RandomHorizontalFlip` and `RandErase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     RandErase())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\n",
    "learn.fit(1)\n",
    "xb,yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to go all the way up to 50 epochs, and we get .946."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 2e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that we don't end up with zero-one, and clamping feels a bit weird.\n",
    "How do we replace the pixels with something that is guaranteed to be the correct distribution?\n",
    "A: we can copy another part of the picture to have the correct distribution of pixels.\n",
    "This is *Random copying*, lets implement it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our `xb` minibatch and the sizes, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szx = int(pct*xbt.shape[-2])\n",
    "szy = int(pct*xbt.shape[-1])\n",
    "stx1 = int(random.random()*(1-pct)*xbt.shape[-2])\n",
    "sty1 = int(random.random()*(1-pct)*xbt.shape[-1])\n",
    "stx2 = int(random.random()*(1-pct)*xbt.shape[-2])\n",
    "sty2 = int(random.random()*(1-pct)*xbt.shape[-1])\n",
    "stx1,sty1,stx2,sty2,szx,szy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are copying, not erasing.\n",
    "Now instead of random noise we say replace this (left) slice of the batch with this (right) slice of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt[:,:,stx1:stx1+szx,sty1:sty1+szy] = xbt[:,:,stx2:stx2+szx,sty2:sty2+szy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with copied little bits across some.\n",
    "We can't really see it all, some of them are black and it's replaced black..\n",
    "I guess it's knocked off the end of this shoe, added a little bit extra here, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll randomly `_rand_copy1` get a different X Y to copy from.\n",
    "Lets turn it into a function... once it was tested above) (in the repl??)\n",
    "to make sure the function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _rand_copy1(x, pct):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx1 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty1 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    stx2 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty2 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    x[:,:,stx1:stx1+szx,sty1:sty1+szy] = x[:,:,stx2:stx2+szx,sty2:sty2+szy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's copying it, notice that frequently the copy is from something that's largely black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "_rand_copy1(xbt, 0.2)\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we do it multiple times, and get a couple of random copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rand_copy(x, pct=0.2, max_num = 4):\n",
    "    num = random.randint(0, max_num)\n",
    "    for i in range(num): _rand_copy1(x, pct)\n",
    "#     print(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:16]\n",
    "rand_copy(xbt, 0.2, 4)\n",
    "show_images(xbt, imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn that into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RandCopy(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_copy(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create our transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     RandCopy())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a batch to make sure it looks sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\n",
    "learn.fit(1)\n",
    "xb,yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit for 25 epochs it gets to .94."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "epochs = 25\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After above training for 25 epochs, we can train a whole new model for a different 25 epochs, and put in a different Learner `learn2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn2 = TrainLearner(model2, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn2.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learn2` is .941 and `learn` was 94. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "torch.save(learn.model,  mdl_path/'randcopy1.pkl')\n",
    "torch.save(learn2.model, mdl_path/'randcopy2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ensemble them, grab the predictions of `learn` and `learn2`, \n",
    "stack them up and take their mean. \n",
    "The ensemble returns .944, better than either of the models.\n",
    "We still didn't beat our best but it's a particularly useful trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp1 = CapturePreds()\n",
    "learn.fit(1, train=False, cbs=cp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp2 = CapturePreds()\n",
    "learn2.fit(1, train=False, cbs=cp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp2.all_preds.shape, cp1.all_targs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Possible error in doing torch.cat unecessarily? </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ap1 = torch.cat(cp1.all_preds)\n",
    "#ap2 = torch.cat(cp2.all_preds)\n",
    "#at = torch.cat(cp1.all_targs)\n",
    "ap1 = cp1.all_preds\n",
    "ap2 = cp2.all_preds\n",
    "at = cp1.all_targs\n",
    "ap1.shape,ap2.shape, at.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = torch.stack([ap1,ap2]).mean(0).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((ap==at).float().mean().item(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was trying to improve by using ensembling with same number of epochs,\n",
    "instead of training for longer.\n",
    "We couldn't, maybe because the random copy is not as good, \n",
    "or maybe we are using too much augmentation.\n",
    "\"cut mix\" is similar to this,  cut mix copies it from different images rather\n",
    "than from the same image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "p = 0.1\n",
    "dist = distributions.binomial.Binomial(probs=1-p)\n",
    "dist.sample((10,))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        dist = distributions.binomial.Binomial(tensor(1.0).to(x.device), probs=1-self.p)\n",
    "        return x * dist.sample(x.size()) * 1/(1-self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropmodel(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d, drop=0.0):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm), nn.Dropout2d(drop)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), Dropout(drop), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs=5\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_dropmodel(act_gr, norm=nn.BatchNorm2d, drop=0.1).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTD_CB(Callback):\n",
    "    def before_epoch(self, learn):\n",
    "        learn.model.apply(lambda m: m.train() if isinstance(m, (nn.Dropout,nn.Dropout2d)) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JH added this new section which replaces the transform `transformi` with something\n",
    "that goes from -1 to 1.\n",
    "Then creates the data loaders `dls` and then trains a model \n",
    "that can classify fashion, and saved the trained model as `data_aug2.pkl`.\n",
    "It is the same as before, but it is a fashion classifier where the inputs are expected to be between -1 and 1.\n",
    "BUT, our image samples are NOT between -1 and 1.\n",
    "In notebook 17_ ddpm2 we use `TF.totensor()` and that makes images that are between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o)*2-1) for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/data_aug2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gone from scratch step by step to state-of-the-art models \n",
    "where we build everything ourselves, and it runs this quickly.\n",
    "We use our own custom Resnet and common sense at every stage.\n",
    "Going up to larger datasets nothing changes on these techniques.\n",
    "We should do most (99%) of the research on very small data sets,\n",
    "because we can iterate much more quickly, we can understand them better.\n",
    "And on a bigger dataset the findings continue to hold true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: create own own schedulers that work with Pythorch optimizers,\n",
    "to make sure we understand the Pytorch API well.\n",
    "Create our own cosine annealing scheduler from scratch, then OneCycle scheduler from scratch.\n",
    "Ensure that they work correctly with this batch scheduler callback.\n",
    "Hopefully getting extremely frustrated as things don't work the way you hoped,\n",
    "they would and being mystified for a while and then working through it \n",
    "using a step-by-step approach, lots of experimentation lots of exploration.\n",
    "And then figuring it out that's that's the journey.\n",
    "\n",
    "JH hopes we'll find it tricky to get it all working properly,\n",
    "and in the process of doing so you're going to have to do a lot of exploration and experimentation.\n",
    "If it doesn't work first time it's not because there's something that you didn't learn in graduate school. \n",
    "We just need to dig through slowly and carefully to see how it all works.\n",
    "\n",
    "Homework2: beat JH on the 5e park or the 20 Epoch or the 50 Epoch fashion mnist, \n",
    "ideally using mini AI with things that we added.\n",
    "Or try other libraries. If another library can beat JH approach \n",
    "try to re-implement that library that way you are still within the spirit of the game.\n",
    "\n",
    "In our next couple of lessons jono, tanishq and JH are going to be putting this all together \n",
    "to create a diffusion model from scratch.\n",
    "Not just a diffusion model but a variety of interesting generative approaches.\n",
    "So we're kind of starting to come full circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
