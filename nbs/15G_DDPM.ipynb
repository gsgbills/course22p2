{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3d1652",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models with miniai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9060758-4a15-4d74-a8f6-9d187ea91873",
   "metadata": {},
   "source": [
    "Now that we written our own barebones training library, let's make some progress towards exploring diffusion model and building Stable Diffusion from scratch.\n",
    "\n",
    "We'll start with building and training the model described in the seminal 2020 paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM). For more context, while diffusion models were technically invented [back in 2015](https://arxiv.org/abs/1503.03585), diffusion models flew under the radar until this 2020 paper since they were complicated and difficult to train. The 2020 paper introducing DDPMs made some crucial assumptions that significantly simplify the model training and generation processes, as we will see here. Later versions of diffusion models all build upon the same framework introduced in this paper.\n",
    "\n",
    "Let's get started and train our own DDPM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c4f01",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93867089-17d2-40cb-a52c-a088b2352929",
   "metadata": {},
   "source": [
    "We'll start with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb2d883-6db2-4a1f-b699-ff8343df0fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <AE5A0901-5B6C-3028-ADEE-0C068D0474D9> /Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <2323D885-BE27-397B-B74E-2A78902E48D2> /Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,random,logging\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8273fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e945bc-26a4-4194-ba12-4cbb7b79e49d",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We will load the dataset from HuggingFace Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99edd708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff457ecf3de442079742f25fc4971272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc543e-1fb5-494d-a703-e40ced8e7e70",
   "metadata": {},
   "source": [
    "To make life simpler (mostly with the model architecture), we'll resize the 28x28 images to 32x32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c14d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [TF.resize(TF.to_tensor(o), (32,32)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02a81e-a728-48ff-9ca4-64f6bde91504",
   "metadata": {},
   "source": [
    "Let's set our batch size and create our DataLoaders with this batch size. we can confirm the shapes are correct. Note that while we do get the labels for the dataset, we don't care about that for our task of **unconditional** image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee14c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 32, 32]), tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "bs = 128\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=8)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49740e0c",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06abe23",
   "metadata": {},
   "source": [
    "We will create a U-net. A U-net looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8359b2d",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/blog/assets/78_annotated-diffusion/unet_architecture.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff4bef-3f20-4002-8040-eb014bfbe27b",
   "metadata": {},
   "source": [
    "The DDPM U-net is a modification of this with some modern tricks like using attention.\n",
    "\n",
    "We will cover how U-nets are created and how modules like attention work in future lessons. For now, we'll import the U-net from the diffusers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563d47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a6eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f26317",
   "metadata": {},
   "source": [
    "## Training - easy with a callback!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c3b3c",
   "metadata": {},
   "source": [
    "DDPM is trained quite simply in a few steps:\n",
    "1. randomly select some timesteps in an iterative noising process.\n",
    "2. Add noise corresponding to this timestep to the original image. For increasing timesteps, the variance of the noise increases.\n",
    "3. Pass in this noisy image and the timestep to our model\n",
    "4. Model is trained with an MSE loss between the model output and the amount of noise added to the image\n",
    "\n",
    "We implement this in a callback `DDPMCB` that will randomly select the timestep and create the noisy image \n",
    "before setting up our input and ground truth tensors for the model forward pass and loss calculation.\n",
    "<br>\n",
    "After training, we need to sample from this model. \n",
    "This is an iterative denoising process starting from pure noise. \n",
    "We keep removing noise predicted by the neural network, with an expected noise schedule \n",
    "that is reverse of what we saw during training. \n",
    "This is also done in our callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa916302-00c5-4ec0-ac69-de4dccce755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMCB(TrainCB):\n",
    "    order = DeviceCB.order+1\n",
    "    def __init__(self, n_steps, beta_min, beta_max):\n",
    "        self.n_steps,self.βmin,self.βmax = n_steps,beta_min,beta_max\n",
    "        # variance schedule, linearly increased with timestep\n",
    "        self.β = torch.linspace(self.βmin, self.βmax, self.n_steps)\n",
    "        self.α = 1. - self.β \n",
    "        self.ᾱ = torch.cumprod(self.α, dim=0)\n",
    "        self.σ = self.β.sqrt()\n",
    "\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[0]).sample\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        device = learn.batch[0].device\n",
    "        ε = torch.randn(learn.batch[0].shape, device=device)  # noise, x_T\n",
    "        x0 = learn.batch[0] # original images, x_0\n",
    "        self.ᾱ = self.ᾱ.to(device)\n",
    "        n = x0.shape[0]\n",
    "        # select random timesteps\n",
    "        t = torch.randint(0, self.n_steps, (n,), device=device, dtype=torch.long)\n",
    "        ᾱ_t = self.ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "        xt =  torch.sqrt(ᾱ_t)*x0 + torch.sqrt(1-ᾱ_t)*ε #noisify the image\n",
    "        # input to our model is noisy image and timestep, ground truth is the noise \n",
    "        learn.batch = ((xt, t), ε)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, sz):\n",
    "        device = next(model.parameters()).device\n",
    "        x_t = torch.randn(sz, device=device)\n",
    "        preds = []\n",
    "        for t in reversed(range(self.n_steps)):\n",
    "            t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "            z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(device)\n",
    "            ᾱ_t1 = self.ᾱ[t-1]  if t > 0 else torch.tensor(1)\n",
    "            b̄_t = 1 - self.ᾱ[t]\n",
    "            b̄_t1 = 1 - ᾱ_t1\n",
    "            noise_pred = learn.model(x_t, t_batch).sample\n",
    "            x_0_hat = ((x_t - b̄_t.sqrt() * noise_pred)/self.ᾱ[t].sqrt()).clamp(-1,1)\n",
    "            x0_coeff = ᾱ_t1.sqrt()*(1-self.α[t])/b̄_t\n",
    "            xt_coeff = self.α[t].sqrt()*b̄_t1/b̄_t\n",
    "            x_t = x_0_hat*x0_coeff + x_t*xt_coeff + self.σ[t]*z\n",
    "            preds.append(x_t.cpu())\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d36b3d-7beb-423e-8c43-6c469983a922",
   "metadata": {},
   "source": [
    "Okay now we're ready to train a model!\n",
    "\n",
    "Let's create our `Learner`. We'll add our callbacks and train with MSE loss.\n",
    "\n",
    "We specify the number of timesteps and the minimum and maximum variance for the DDPM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57313151-68c6-421d-a133-dd021b986533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<miniai.learner.Learner at 0x10bfd1670>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906dabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 4e-3\n",
    "epochs = 5\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "ddpm_cb = DDPMCB(n_steps=1000, beta_min=0.0001, beta_max=0.02)\n",
    "cbs = [ddpm_cb, DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched)]\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed349459-a2f9-4f8f-9ea2-40b3ebfb0984",
   "metadata": {},
   "source": [
    "Now let's run the fit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd0bbc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/469 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DDPMCB' object has no attribute 'n_inp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:136\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epochs, train, valid, cbs, lr)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcb_ctx(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs:\n\u001b[0;32m--> 136\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m train: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m valid: torch\u001b[38;5;241m.\u001b[39mno_grad()(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_epoch)(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:117\u001b[0m, in \u001b[0;36mLearner.one_epoch\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcb_ctx(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:145\u001b[0m, in \u001b[0;36mLearner.callback\u001b[0;34m(self, method_nm)\u001b[0m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_nm): \u001b[43mrun_cbs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_nm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:47\u001b[0m, in \u001b[0;36mrun_cbs\u001b[0;34m(cbs, method_nm, learn)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(cbs, key\u001b[38;5;241m=\u001b[39mattrgetter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     46\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(cb, method_nm, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/course22p2/nbs/miniai/learner.py:154\u001b[0m, in \u001b[0;36mTrainCB.get_loss\u001b[0;34m(self, learn)\u001b[0m\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): learn\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mloss_func(learn\u001b[38;5;241m.\u001b[39mpreds, \u001b[38;5;241m*\u001b[39mlearn\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_inp\u001b[49m:])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DDPMCB' object has no attribute 'n_inp'"
     ]
    }
   ],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023eba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "mdl_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228cf5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, mdl_path/'fashion_ddpm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = torch.load(mdl_path/'fashion_ddpm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c8daf-6645-4923-bf82-78f82adcddfd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a4cd9-df5a-45a0-8469-b0e699b63ab5",
   "metadata": {},
   "source": [
    "Now that we've trained our model, let's generate some images with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98b94f-38c5-4474-9e49-721201f2a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "samples = ddpm_cb.sample(learn.model, (16, 1, 32, 32))\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(-samples[-1], figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac4018-fd51-43b9-afc4-230fe7ea8be3",
   "metadata": {},
   "source": [
    "Let's visualize the sampling process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d60054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(3,3))\n",
    "def _show_i(i): return show_image(-samples[i][9], ax=ax, animated=True).get_images()\n",
    "r = L.range(800,990, 5)+L.range(990,1000)+[999]*10\n",
    "ims = r.map(_show_i)\n",
    "\n",
    "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=3000)\n",
    "display(HTML(animate.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3860f0-da13-4e23-bff8-a56eddc9db4c",
   "metadata": {},
   "source": [
    "Note that I only take the steps between 800 and 1000 since most of the previous steps are actually quite noisy. This is a limitation of the noise schedule used for small images, and papers like [Improved DDPM](https://arxiv.org/abs/2102.09672) suggest other noise schedules for this purpose! (Some potential homework: try out the noise schedule from Improved DDPM and see if it helps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b6eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
