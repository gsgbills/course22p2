{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9313c9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f01ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "from fastcore.foundation import L, store_attr\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1794251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image URLs for demos. Change as desired.\n",
    "face_url = \"https://images.pexels.com/photos/2690323/pexels-photo-2690323.jpeg?w=256\"\n",
    "spiderweb_url = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5f031",
   "metadata": {},
   "source": [
    "# Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e0fad67-d477-4964-b982-902a57d1a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    imgb = fc.urlread(url, decode=False) \n",
    "    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61ef1220-79db-4c59-8019-0b9f33616ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0851ea3-ab16-4503-97ad-1623ee9b151a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m content_im \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_url\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(def_device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_im.shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, content_im\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m show_image(content_im);\n",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m, in \u001b[0;36mdownload_image\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_image\u001b[39m(url):\n\u001b[0;32m----> 2\u001b[0m     imgb \u001b[38;5;241m=\u001b[39m \u001b[43mfc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlread\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, decode=False) \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (tensor(\u001b[38;5;28mlist\u001b[39m(imgb), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/fastcore/net.py:122\u001b[0m, in \u001b[0;36murlread\u001b[0;34m(url, data, headers, decode, return_json, return_headers, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m: \u001b[38;5;28;01mraise\u001b[39;00m ExceptionsHTTP[e\u001b[38;5;241m.\u001b[39mcode](e\u001b[38;5;241m.\u001b[39murl, e\u001b[38;5;241m.\u001b[39mhdrs, e\u001b[38;5;241m.\u001b[39mfp, msg\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mmsg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode: res \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_json: res \u001b[38;5;241m=\u001b[39m loads(res)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (res,\u001b[38;5;28mdict\u001b[39m(hdrs)) \u001b[38;5;28;01mif\u001b[39;00m return_headers \u001b[38;5;28;01melse\u001b[39;00m res\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "content_im = download_image(face_url).to(def_device)\n",
    "print('content_im.shape:', content_im.shape)\n",
    "show_image(content_im);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4225ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_im.min(),content_im.max() # Check bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b50605",
   "metadata": {},
   "source": [
    "# Optimizing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f3bed-16a3-4937-b61b-eb938154d69d",
   "metadata": {},
   "source": [
    "GOAL: We want to make the image to continue to look like that lady,\n",
    "and change the style to match the style of another picture.\n",
    "In the optimization loop, each step will be moving the style closer.\n",
    "We loop through steps of a optimization loop (not thru different images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d657d2-2ec6-494d-bca9-3781f55c01ea",
   "metadata": {},
   "source": [
    "We don't have a dataset with training examples. \n",
    "We just have a single target and a single thing we're optimizing.\n",
    "We define a `LengthDataset()` class to follow the dataset standard, with `__getitem__` and `__len__`. \n",
    "But for `__getitem__` it returns 0 0, as we do not care about the content from this dataset.\n",
    "We just need something that we can pass to the learner to do a number of training iterations.\n",
    "We define a fake dummy dataloaders `get_dummy_dls(length=100)` with a hundred items.\n",
    "It creates the dataloaders and gives us a way to train for a number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de53881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthDataset():\n",
    "    def __init__(self, length=1): self.length=length\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx): return 0,0\n",
    "\n",
    "def get_dummy_dls(length=100):\n",
    "    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1), # Train\n",
    "                       DataLoader(LengthDataset(1), batch_size=1))      # Valid (length 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b0d1b-b239-404a-80d6-b5dafd6bb836",
   "metadata": {},
   "source": [
    "We define a `TensorModel` `Module` class which just has a tensor `t` as its parameter.\n",
    "There is no actual neural network.\n",
    "We are just going to pass in a random image or some image shaped tensor, with a set of numbers that we can optimize.\n",
    "<br>\n",
    "Putting something in an `nn.Parameter` doesn't change it in any way, it is just a normal tensor, \n",
    "but it is stored inside the Module as being a tensor to optimize.\n",
    "We're not optimizing a model, we're optimizing the pixels of an image directly.\n",
    "Because it's in a parameter if we look at it e.g., `model.t` it does `require_grad` as that's already set up because the `nn.Module` \n",
    "is going to look for any parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b67006-7663-4d5f-a431-2de04505bf3d",
   "metadata": {},
   "source": [
    "The shape of the parameters that we're optimizing is the same shape as the image.\n",
    "That tensor will be optimized if we pass it into a learner fit method.\n",
    "This model has an argument (`x=0`) passed to forward, which we ignore.\n",
    "We just make it easier ourselves by making the model look the way our learner expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c152c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorModel(nn.Module):\n",
    "    def __init__(self, t):\n",
    "        super().__init__()\n",
    "        self.t = nn.Parameter(t.clone())\n",
    "    def forward(self, x=0): return self.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "778de91b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_im' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m TensorModel(torch\u001b[38;5;241m.\u001b[39mrand_like(\u001b[43mcontent_im\u001b[49m))\n\u001b[1;32m      2\u001b[0m show_image(model());\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content_im' is not defined"
     ]
    }
   ],
   "source": [
    "model = TensorModel(torch.rand_like(content_im))\n",
    "show_image(model());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30783b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.shape for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5e891-7fdc-497f-b880-e730ca34af6f",
   "metadata": {},
   "source": [
    "Alternatively, we could use e.g. `TrainCB`, set it up with a custom `predict` method that \n",
    "calls the model forward method with no parameters, calling the loss function on just the predictions.\n",
    "But if we want to skip this, as we take `x=0` and never use it.\n",
    "That should also work without this callback, either way is fine.\n",
    "It is a nice approach if you already have an existing model which expects some number of parameters,\n",
    "just modify the `TrainCB` callback, but we don't need to in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead333aa-c7ce-4748-b626-da28e12d6c60",
   "metadata": {},
   "source": [
    "Let's put this in a Learner, let's optimize it with some loss function.\n",
    "To clarify the `get_loss` had to change because normally we pass a Target to the loss function.\n",
    "Let's learn our grids and then `learn.batch`.\n",
    "We could remove that if we wanted to have the loss function take a target that we then ignore.\n",
    "\n",
    "We build on the idea of modifying the training callback in the ddpm example.\n",
    "In `ImageOptCB` it is a 2 lines change.\n",
    "To get our model predictions `preds` we call the `forward` method which returns an image that we're optimizing.\n",
    "We evaluate this according to a loss function that just takes in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOptCB(TrainCB):\n",
    "    def predict(self, learn): learn.preds = learn.model()\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ddcd8-2aac-40cb-a604-cd24a7aace2c",
   "metadata": {},
   "source": [
    "For the first loss function `loss_fn_mse` we use Mean Squared Error between the image that we are generating (output of the model)\n",
    "and the content image that is our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb41c9-a448-4ef2-a4fc-e72a4aebf141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_mse(im):\n",
    "    return F.mse_loss(im, content_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b7683-ebed-4c3f-9901-24fd90c64999",
   "metadata": {},
   "source": [
    "We set up our model and start it with a random image like the one above.\n",
    "We create a learner with a dummy dataloader for 100 steps, `loss_fn_mes` as the loss function, an `lr` and  `optim.Adam`.\n",
    "(default optimizer probably works fine).\n",
    "We run it and the loss goes from a non-zero number to close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74649c75-1139-4b4e-ae04-19fcbfd603b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorModel(torch.rand_like(content_im))\n",
    "cbs = [ImageOptCB(), ProgressCB(), MetricsCB(), DeviceCB()]\n",
    "learn = Learner(model, get_dummy_dls(100), loss_fn_mse, \n",
    "                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57756cd1-1cd4-4755-abd2-310b96fb1cb6",
   "metadata": {},
   "source": [
    "We look at the final result (call learn.model) and show it as an image and compare it with the source input image.\n",
    "They look identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result (left) vs target image (right):\n",
    "show_images([learn.model().clip(0, 1), content_im]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99a7b8-a6de-43e3-8cef-ff017f6db6ce",
   "metadata": {},
   "source": [
    "JH: To clarify this \"pointless\" example, where we started with a noisy image and used SGD to make the pixels get closer \n",
    "to the source image (lady in the sunglasses).\n",
    "This is just to show that we can turn noisy pixels into something else by having it follow a loss function.\n",
    "And this loss function makes the pixels look as the source image.\n",
    "It's a very simple loss a one Direction that you update, almost trivial to solve.\n",
    "It helps us get the framework in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a1944",
   "metadata": {},
   "source": [
    "## Viewing progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44363000-0a32-457c-97e5-3c6440a07e8e",
   "metadata": {},
   "source": [
    "Just seeing that result is not instructive.. maybe is a bug in the code that just duplicated the image.\n",
    "We need a way of observing progress. \n",
    "We define a login callback `ImageLogCB` that after every batch stores the output as an image.\n",
    "Every `log_every` iterations it's going to append the current image in a list, to be shown after the training.\n",
    "All else is as before but passing in `ImageLogCB` callback it will show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLogCB(Callback):\n",
    "    order = ProgressCB.order + 1\n",
    "    def __init__(self, log_every=10): store_attr(); self.images=[]; self.i=0\n",
    "    def after_batch(self, learn): \n",
    "        if self.i%self.log_every == 0: self.images.append(to_cpu(learn.preds.clip(0, 1)))\n",
    "        self.i += 1\n",
    "    def after_fit(self, learn): show_images(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b18e52-444c-4359-83c4-f2ecda987382",
   "metadata": {},
   "source": [
    "Starting from noise after a few iterations already most noise is gone.\n",
    "By the end it looks exactly like the content image.\n",
    "<br>\n",
    "We have the infrastructure in place to create a variety of interesting outputs, e.g., artistic, \n",
    "image reconstruction, super resolution, colorization, etc.\n",
    "We just have to modify the loss function and we first created the easiest, checked it before we start doing fancy stuff.\n",
    "Now that is all in place, we will be able to visually see if things are going wrong, and we know what we need to modify.\n",
    "\n",
    "It would be great if we could see what is happening over time. \n",
    "You could save individual images and turn them into a video, \n",
    "but for quick feedback we can also log images every few iterations and display them in a grid in `after_fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorModel(torch.rand_like(content_im))\n",
    "learn = Learner(model, get_dummy_dls(150), loss_fn_mse, \n",
    "                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\n",
    "learn.fit(1, cbs=[ImageLogCB(30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ab0f7",
   "metadata": {},
   "source": [
    "## Getting Features from VGG16\n",
    "\n",
    "Maybe we want to match an image but also have a particular overall color, or something more complicated.\n",
    "To get a richer measure of what the image looks like, we extract features  from a pre-trained network.\n",
    "This is the core idea of this notebook: \n",
    "We feed in an image, and then have downsampling convolutions with Maxpooling until some final prediction.\n",
    "<br>\n",
    "JH: A big difference: that 7x7x512 nowadays we use Adaptive or Global pooling to get down to 1x1x512.\n",
    "Vgg16 is unusual (by today's standards), it just flattens that out into a 1x1x4096.\n",
    "This might be an interesting property. \n",
    "We should consider training resNets without global pooling and instead do flattening.\n",
    "We don't do flattening because that very last linear layer from 1x1x4096 to 1x1x1000 is for an Imagenet model.\n",
    "It is going to need a big weight Matrix, 4096x1000, hence very memory intensive for a poor performing model by modern standards.\n",
    "But, doing that actually also has some some potential benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2366de",
   "metadata": {},
   "source": [
    "We're going to peek inside a small CNN and extract the outputs of different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6794f",
   "metadata": {},
   "source": [
    "### Load VGG network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f99425",
   "metadata": {},
   "source": [
    "![vgg diag](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6d3c623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n"
     ]
    }
   ],
   "source": [
    "print(timm.list_models('*vgg*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadd944-cbec-4b1e-bfa4-09f3e4cdfc56",
   "metadata": {},
   "source": [
    "Let's load the model, look at what the layers are and then in the next section\n",
    "we can see what kinds of images work when we optimize towards different layers in there.\n",
    "In the `vgg16` network we have convolutions, RELUs, Maxpooling, etc, all in one `nn.Sequential`.\n",
    "It doesn't have the head so we said `.features` so this is the features subNetwork.\n",
    "That's everything up until some point and then you have the flattening and the classification which we're throwing away.\n",
    "This is the the body of the network and we're going to tag into various layers and extract the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc35e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/germangoldszmidt/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    }
   ],
   "source": [
    "vgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe122af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ff854",
   "metadata": {},
   "source": [
    "### Normalize Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747b048-d3d2-4cc5-810e-da6089dafbc8",
   "metadata": {},
   "source": [
    "This model was trained on a normalized version of Imagenet, using the dataset mean and std to normalize the images.\n",
    "To match what the data looked like during training we need to match that normalization step.\n",
    "We've done this on grayscale images where we just subtract the mean divide by the standard deviation.\n",
    "But here these are 3 Channel RGB image, we can't get away with just subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a3fee-f24e-45a7-be2f-1b6abaa67cc3",
   "metadata": {},
   "source": [
    "This model expacts images normalized with the same stats as those used during training, which in this case requires the stats of the ImageNet dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3659224",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = tensor([0.485, 0.456, 0.406])\n",
    "imagenet_std = tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ca0d9-dedc-4477-990d-4ab08e1bde88",
   "metadata": {},
   "source": [
    "We now need to be more careful than we can with just a scalar value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "011a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 1 (won't work):\n",
    "#(content_im - imagenet_mean) / imagenet_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f015157-4be6-4ea2-9710-0622bcac8e56",
   "metadata": {},
   "source": [
    "With single-channel images normalizing is straightforward. With 3 channels, we must consider\n",
    "the shapes and broadcasting rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af011ec6-7425-45cc-8824-a50fdab94986",
   "metadata": {},
   "source": [
    "The `imagenet_mean` has 3 values, one for each RGB channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dc1a5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8f976-d6f9-4f2a-9a8b-14324b524635",
   "metadata": {},
   "source": [
    "The content image has 3 channels 256x256. \n",
    "If we just say `(content_im - imagenet_mean)` going from right to left and find the first non-unit axis.\n",
    "Anything with a size > 1 it's going to try and line those up, in this case the 3 and the 256, which don't\n",
    "match, so we get an error.\n",
    "More perniciously if the shape did happen to match, it might not be what we intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92d78641",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_im' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontent_im\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content_im' is not defined"
     ]
    }
   ],
   "source": [
    "content_im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec90ef-9648-4552-9cb8-dd7888f74020",
   "metadata": {},
   "source": [
    "We want the 3 channels mapped to the 3 channels of the image and then somehow expand those values out across the two other dimensions.\n",
    "For this, we add two additional dimensions with `None` on the right of the `imagenet.mean`.\n",
    "Alternatively, we  could use `.unsqueeze(-1)` (twice?) but `None` is what we are using.\n",
    "Now the shapes match, from right to left, if it is a unit Dimension size one it expands it out to match the other tensor.\n",
    "If it is a non-unit dimension then the shapes must match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab55d04-2356-4d97-aca7-329ada6bb06c",
   "metadata": {},
   "source": [
    "With this reshaping operation we can write a `normalize(im)` to apply to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ca4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 2:\n",
    "def normalize(im):\n",
    "    imagenet_mean = tensor([0.485, 0.456, 0.406])[:,None,None].to(im.device)\n",
    "    imagenet_std = tensor([0.229, 0.224, 0.225])[:,None,None].to(im.device)\n",
    "    return (im - imagenet_mean) / imagenet_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ba461-ab3d-4e6f-947d-9ec330f1c545",
   "metadata": {},
   "source": [
    "We check their `min` and `max` to ensure that they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508fc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(content_im).min(), normalize(content_im).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae544430-3f25-4170-a036-018e6def0180",
   "metadata": {},
   "source": [
    "We check the `mean` to ensure that it is somewhat close to zero.\n",
    "Our Blue channel is brighter than the others and if we go back and look at our image it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(content_im).mean(dim=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910795bb-cf61-4598-b1d1-21eafaa8019a",
   "metadata": {},
   "source": [
    "As we've implemented it, we can use `torchvision.transforms.Normalize`.\n",
    "We pass the mean and standard deviation `std` to it and it ensures that the devices and shapes match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with torchvision transforms:\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad1f6f-323b-4919-9474-27abba6df056",
   "metadata": {},
   "source": [
    "The Min, Max and Means are the same, i.e., our function does the same as the torchvision normalized transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc65ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(content_im).min(), normalize(content_im).max(), normalize(content_im).mean(dim=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd96f2",
   "metadata": {},
   "source": [
    "### Get intermediate representations, take 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f1926",
   "metadata": {},
   "source": [
    "We want to feed some data through the network, storing the outputs of different layers. Here's one way to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d02f08-5fd6-458f-9767-6e346c9df9a9",
   "metadata": {},
   "source": [
    "To extract the features from this network we are going to normalize our inputs `imgs`, \n",
    "and run through each `layer` in the sequential stack.\n",
    "We pass the result of the layer `x` through each layer.\n",
    "If we're in one of the special `target_layers`, append the outputs of that layer to a list `feats`.\n",
    "The term *features* here means the activations of a layer, in this case two particular layers 18 and 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc49e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(imgs, target_layers=(18, 25)): \n",
    "    x = normalize(imgs)\n",
    "    feats = []\n",
    "    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n",
    "        x = layer(x)\n",
    "        if i in target_layers:\n",
    "            feats.append(x.clone())\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bcd05-1642-42e1-8602-0f05332959c7",
   "metadata": {},
   "source": [
    "<mark> NB: There's a problem (Python?/Pytorch?) using a list as a default, so we should use tuples.\n",
    "When we use a mutable type like a list in a python default parameter it keeps it around.\n",
    "And if we change it later then it modifies the function.\n",
    "Suggest never using a list as a default parameter because at some point it will create weird problems.\n",
    "    </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e67af7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_im' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testing it out to see the shapes of the resulting feature maps for the 18 and 25 layers:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m feats \u001b[38;5;241m=\u001b[39m calc_features(\u001b[43mcontent_im\u001b[49m)\n\u001b[1;32m      3\u001b[0m [f\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feats]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content_im' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing it out to see the shapes of the resulting feature maps for the 18 and 25 layers:\n",
    "feats = calc_features(content_im)\n",
    "[f.shape for f in feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6dbc1-086e-4faf-bdb8-4ff87abdc555",
   "metadata": {},
   "source": [
    "**TODO Homework**: Do this using Hooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeb55f",
   "metadata": {},
   "source": [
    "### What's the point?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8e8ec",
   "metadata": {},
   "source": [
    "You may remember us looking at [Feature visualization](https://distill.pub/2017/feature-visualization/) and talking about how deep CNNs 'learn' to classify images. Early layers tend to capture gradients and textures, while later layers more complex types of features. We exploit this complexity hierarchy for artistic purposes.\n",
    "Being able to choose what kind of features to use when comparing images has a number of other useful applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f939b15",
   "metadata": {},
   "source": [
    "# Optimizing an Image with Content Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40469ff0",
   "metadata": {},
   "source": [
    "Let's start optimizing an image by comparing it's features (from two later layers) with those from the target image. If our theory is right, we should see the structure of the target emerge from the noise without necessarily seeing a perfect re-production of the target like we did in the previous MSE loss example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe9dfc-d845-4f4d-9f3c-ff00b79aa660",
   "metadata": {},
   "source": [
    "We're feeding in an image that's 256x256 and the first layer that we're looking at is (18).\n",
    "It's getting half to 128x128 then to 64x64, and 32x32x512.\n",
    "(The ones in the VGG diagram are different sizes because they have a different starting size.)\n",
    "Those are the features for layer 18, a tensor of shape 512x32x32.\n",
    "For every spatial location in that 32x32 grid we have the output from 512 different filters, which \n",
    "are the \"features\" (ie channels in a single convolution).\n",
    "We want to capture different concepts at different layers.\n",
    "To get a feel for this we just compare these feature Maps, we institute a \"Content loss\" also called a \"perceptual loss\".\n",
    "We focus on a couple of later layers we pass-in `target_im`, eg the content image,\n",
    "and we calculate those features in those target layers.\n",
    "In the `__call__` we compare to our inputs and calculate the features of our inputs \n",
    "and do the MSE between those and our target features.\n",
    "<br>\n",
    "`ContentLossToTarget` is a loss function which has a `__call__` method (a callable in Python).\n",
    "(In a Module we call it forward but in normal Python it is `__call__`.)\n",
    "It takes one input which is the way we set up the image training callback earlier.\n",
    "It's just going to pass in the input `target_im` which is the image as it's been optimized to so far.\n",
    "Initially it's going to be random noise, and then the loss we are calculating is the MSE\n",
    "of how far away is `input_im` from the target image `target_im`.\n",
    "The MSE for each of the layers (default 18 and 25).\n",
    "It is a bit strange weird to call (eg 2) different neural network features,\n",
    "not because that's the model we're optimizing, but because it's the loss function it's how far away are we.\n",
    "If we SGD optimize that loss function we are not going to get the same pixels, \n",
    "we get pixels which have the same activations of those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0770e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLossToTarget():\n",
    "    def __init__(self, target_im, target_layers=(18, 25)):\n",
    "        fc.store_attr()\n",
    "        with torch.no_grad():\n",
    "            self.target_features = calc_features(target_im, target_layers)\n",
    "    def __call__(self, input_im): \n",
    "        return sum((f1-f2).pow(2).mean() for f1, f2 in \n",
    "               zip(calc_features(input_im, self.target_layers), self.target_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cc0a3-e758-4d6f-afeb-caef8b20e0b9",
   "metadata": {},
   "source": [
    "If we run it we can see the shape of the source image person, but it doesn't match on a color and style basis.\n",
    "18 and 25 are fairly deep (there are 31 layers).\n",
    "Color often doesn't have much of a semantic property, that may be why it doesn't care much about color.\n",
    "E.g., it's still going to be an eyeball whether it is green or blue.\n",
    "\n",
    "We aren't constraining the tensor that we're optimizing to be in the same bounds as a normal image\n",
    "so some of these may also be <0  or >1. \n",
    "Almost hacking the ANN to get the same features that those deep layers have \n",
    "by passing in something that it's never seen during training.\n",
    "For display we're clipping it to be an image,\n",
    "But we may want to have either some sort of sigmoid function or some other way to clamp \n",
    "the tensor model and to have outputs that are within the allowed range.\n",
    "Notice that the background hasn't changed much.\n",
    "The reason for that would be that the VGG model we're using in the loss function was trained on Imagenet.\n",
    "Imagenet is good at recognizing a single big object like a dog or a boat.\n",
    "It's not going to care about the background, so the background \n",
    "isn't going to have much in the way of features at all.\n",
    "That is why it hasn't changed the background yeah.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa72403",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_perceptual = ContentLossToTarget(content_im)\n",
    "model = TensorModel(torch.rand_like(content_im))\n",
    "learn = Learner(model, get_dummy_dls(150), loss_function_perceptual, \n",
    "                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\n",
    "learn.fit(1, cbs=[ImageLogCB(log_every=30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae383f-7a00-415b-8726-4be24b26ce9c",
   "metadata": {},
   "source": [
    "Interesting to see how little it looks like the image, at the same time we can recognize it.\n",
    "We can also try passing-in them earlier list right and comparing on those earlier layers and\n",
    "we get a different result because now we're optimizing to some image that is a lot closer to the original.\n",
    "Still it doesn't look the same, so there are a few things that are worth noting.\n",
    "We're hooking into these RELU layers which might mean,\n",
    "for example, that if we are looking at the very early layers we're missing out on some \n",
    "features that was one of my guesses as to why this didn't have as dark as the input image.\n",
    "Also we might be going out of bounds to get the same kinds of features.\n",
    "\n",
    "We can see how by looking at really deep layers we really don't care about the color or Texture.\n",
    "We are just getting like sunglasses bits, and by looking at the earlier layers\n",
    "we have a more rigid adherence to the the lower level features.\n",
    "It gives us a tunable way to compare two images.\n",
    "We can say do we care that they match exactly on pixels, then we can use MSE,\n",
    "but if we care about the exact match then we can use some early layers.\n",
    "If we only care about the overall semantics we can go to deeper layers and we can experiment with.\n",
    "\n",
    "This is like the technique that Xyler and Fergus and the distilled papers use to identify what do filters look at. \n",
    "We can optimize an image to maximize a particular filter. \n",
    "For example that would be a similar loss function to the one we've built here.\n",
    "Fun project: calculate these feature maps, pick one of the 512 features,\n",
    "and optimize the image to maximize that activation.\n",
    "By default we might quite a noisy weird result like almost an adversarial input.\n",
    "\"Feature Visualization\" people add things like augmentations.\n",
    "So we are optimizing an image that under some augmentations still activates that feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f155077",
   "metadata": {},
   "source": [
    "**Choosing the layers determines the kind of features that are important:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_perceptual = ContentLossToTarget(content_im, target_layers=(1, 6))\n",
    "model = TensorModel(torch.rand_like(content_im))\n",
    "learn = Learner(model, get_dummy_dls(150), loss_function_perceptual, \n",
    "                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\n",
    "learn.fit(1, cbs=[ImageLogCB(log_every=30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537f2ad-1e76-4291-bdd6-5c051151f538",
   "metadata": {},
   "source": [
    "We know how to optimize an image, how to extract features from this neural network.\n",
    "This is for comparing, for different types of features how similar are two images.\n",
    "For a full style transfer artistic application we need to keep the structure\n",
    "of the source image, and have the style (eg., style, textures) come from a different image.\n",
    "<br>\n",
    "The feature maps have a spatial component.\n",
    "For example, we had a 32x32x512 feature map, each location in the 32x32 grid correspond to some part of the input image.\n",
    "If we just do MSE for the activations from some early layers, \n",
    "that would be asking for the same types of features in the same locations.\n",
    "We want to get an image that matches the source image, with the same colors and textures,\n",
    "but they may  be in different parts of the image, i.e., we want to get rid of the spatial aspect. \n",
    "For example,  give us the image in the style of Van Gogh Starry Night.\n",
    "We're not asking where in the image there should be something with this texture,\n",
    "just that the kinds of textures that are used anywhere in that image should\n",
    "also appear in in our version, but not necessarily in the same place.\n",
    "The solution that Seller? and Ferguson proposed uses a Gram Matrix.\n",
    "We want some measure of what kinds of styles are present without worrying about where they are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692048f",
   "metadata": {},
   "source": [
    "# Style Loss with Gram Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9bf193",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "Style transfer was first invented in the Gattys paper.\n",
    "Seiler and Ferguson did the feature visualization.\n",
    "Lets open the original paper and use Jono's explanation to understand the paper, which has some nice pictures. \n",
    "Gram Matrix inner product between the vectorized feature map, but it doesn't explain how the gram Matrix works.\n",
    "People have used Gram Matrices in other contexts for similar kinds of measures.\n",
    "Pytorch has named parameters, we can name layers of a sequential model.\n",
    "\n",
    "\n",
    "We know how to extract feature maps. The next thing we'd like to do is find a way to capture the **style** of an input image, based on those early layers and the kinds of textural feature that they learn. Unfortunately, we can't just compare the feature maps from some early layers since these 'maps' encode information spatially - which we don't want!"
   ]
  },
  {
   "attachments": {
    "feature_map_extraction.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e3253cbe",
   "metadata": {},
   "source": [
    "![feature_map_extraction.png](attachment:feature_map_extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd0de",
   "metadata": {},
   "source": [
    "So, we need a way to measure what **kinds** of style features are present, and ideally which kinds occur together, without worrying about **where** these features occur in the image. \n",
    "\n",
    "Enter something called the Gram Matrix. The idea here is that we'll measure the **correlation** between features. Given a feature map with `f` features in an `h` x `w` grid, we'll flatten out the spatial component and then for every feature we'll take the dot product of that row with itself, giving an `f` x `f` matrix as the result. Each entry in this matrix quantifies how correlated the relevant pair of features are and how frequently they occur - exactly what we want. In this diagram each feature is represented as a colored dot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945c1c2-5bde-492a-a35a-53dd2178376c",
   "metadata": {},
   "source": [
    "It is difficult to represent more than two-dimensional things on a 2d grid.\n",
    "The feature map has a height and width (eg 32x32), and a number of features (a 3rd dimension) represented as colored dots.\n",
    "With a Gram Matrix we flatten out the spatial Dimension, reshape it to have the width times the height.\n",
    "The spatial (features) dimension is on the horizontal (vertical) axis.\n",
    "Each row (e.g, yellow) has a 1 if there is a corresponding (eg yellow) dot in the location else it gets a 0.\n",
    "Once we flattened the feature map, we only care about which types of features we have and do they occur with each other.\n",
    "We get the dot products of each row with all other rows."
   ]
  },
  {
   "attachments": {
    "gram_calculation.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "95fcb86d",
   "metadata": {},
   "source": [
    "![gram_calculation.png](attachment:gram_calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dea6a4-b0ce-47b7-9236-02fb353376a6",
   "metadata": {},
   "source": [
    "For these feature vectors how correlated are they with each other.\n",
    "We end up with a Gram Matrix (correlation of features).\n",
    "We can read this example as: there are 7 Reds in total, only one location red occurs alongside a green.\n",
    "The Gram Matrix has no spatial component, it's just a feature Dimension by feature Dimension.\n",
    "It is a measure of how common these features are (eg an uncommon one is green),\n",
    "and how frequently they occur together with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4d48c-ca2b-46c0-86a3-df80952782db",
   "metadata": {},
   "source": [
    "We implement this diagram in code, use 0 or 1 for simplicity, but we could have different size activations,\n",
    "but not as easy to represent visually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755c371",
   "metadata": {},
   "source": [
    "Re-creating the diagram operations in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c46226",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([[0, 1, 0, 1, 1, 0, 0, 1, 1],\n",
    "            [0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
    "            [1, 0, 1, 1, 1, 1, 1, 1, 0],\n",
    "            [1, 0, 1, 1, 0, 1, 1, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f62486-2365-41ae-927b-76f4751548d0",
   "metadata": {},
   "source": [
    "We do it with an einsum because it makes it easy to add later the (Bastion?) engine.\n",
    "It is the Matrix multiplied with its own transpose, and doing it with `matmul` \n",
    "we get the same result, so that is the Gram Matrix calculation.\n",
    "We can now use it to create a measure and when we look at things like word2Vic, it's got some similarities.\n",
    "The idea of co-occurrence of features also reminds me of the clip loss.\n",
    "It is a dot product with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c154e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('fs, gs -> fg', t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e70db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.matmul(t.T) # Alternate approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3119fa-5cd6-40ff-b8b4-4778faac2507",
   "metadata": {},
   "source": [
    "The idea of multiplying with your own transpose is a common mathematical technique, used \n",
    "in many areas, e.g., protein folding.\n",
    "The difference in each case is the Matrix that we're multiplying by its own transpose.\n",
    "For covariance the Matrix is the Matrix of differences to the mean.\n",
    "In this case the matrix is the flattened picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dbe7f",
   "metadata": {},
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5910728",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_im = download_image(spiderweb_url).to(def_device)\n",
    "show_image(style_im);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59691634-7d98-4e89-9525-de424ac4e070",
   "metadata": {},
   "source": [
    "The `calc_grams` will do that operation we did above, but we're going to add scaling,\n",
    "because we have a feature map and we might pass in images of different sizes.\n",
    "<br>\n",
    "As there is a relation to the number of spatial locations, by scaling by this width times height\n",
    "we get a relative measure as opposed to an absolute measure.\n",
    "Hence the comparisons are going to be valid even for images of different sizes.\n",
    "We have a `chw, dhw ->cd` (channels/features by height by width image).\n",
    "we pass-in two versions of `x` because it's the same image in both times.\n",
    "And we map this down to just features by features.\n",
    "We can't repeat variables in einsum so that's why it is`c` and `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grams(img, target_layers=(1, 6, 11, 18, 25)):\n",
    "    return L(torch.einsum('chw, dhw -> cd', x, x) / (x.shape[-2]*x.shape[-1]) # 'bchw, bdhw -> bcd' if batched\n",
    "            for x in calc_features(img, target_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef4f98-aec8-427c-b59a-33f346c0aa97",
   "metadata": {},
   "source": [
    "We run this on our style image `style_im` and see we are targeting 5 different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a261bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_grams = calc_grams(style_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a31f3-1bb0-4ab5-b63f-cb59d3332b55",
   "metadata": {},
   "source": [
    "For each one the first layer has 64 features and so we get a 64 by 164 gram Matrix.\n",
    "The second one is 128 features so we can hit 128x128 gram Matrix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[g.shape for g in style_grams] # The gram matrices for features from different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9557ed4-992e-41e8-b298-20b54a3baa56",
   "metadata": {},
   "source": [
    "We use the `attrgot` method (it is a `fc.L` \"magic list\"), \n",
    "Let's use `StyleLosstoTarget()` as a loss just like with the content lost before,\n",
    "we're going to take in a Target image which is going to be our style,\n",
    "calculate the Gram matrices and when we get an input to our last function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f598361",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_grams.attrgot('shape') # The magic of fastcore's L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63e350-8100-46dc-928c-cbaa7d6bbf6b",
   "metadata": {},
   "source": [
    "We calculate the Gram matrices for that and do the MSE between the gram matrices.\n",
    "These have no spatial components just what features are there.\n",
    "Comparing the two to make sure that they have the same kinds of features,\n",
    "and the same kinds of correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLossToTarget():\n",
    "    def __init__(self, target_im, target_layers=(1, 6, 11, 18, 25)):\n",
    "        fc.store_attr()\n",
    "        with torch.no_grad(): self.target_grams = calc_grams(target_im, target_layers)\n",
    "    def __call__(self, input_im): \n",
    "        return sum((f1-f2).pow(2).mean() for f1, f2 in \n",
    "               zip(calc_grams(input_im, self.target_layers), self.target_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_loss = StyleLossToTarget(style_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ec0e9-5b89-4ff7-8340-018f428dacc7",
   "metadata": {},
   "source": [
    "The content image `content_im` at the moment has a high loss when we compare it to our style image `style_im`.\n",
    "That means the physical content image doesn't look anything like a spider web in terms of its textures, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_loss(content_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9370b",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba768244-64ff-4783-82fa-59e9a57de612",
   "metadata": {},
   "source": [
    "We set up an optimization. \n",
    "One difference is that now we are starting from the content image itself rather than optimizing from random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e622b3b-7c83-484f-9189-cfa69783d751",
   "metadata": {},
   "source": [
    "We can choose either way. For style transfer it's nice to use the content image as a starting point.\n",
    "At the beginning it just looks like our content image.\n",
    "As we do more steps we maintain the structure, because we're using the content loss as one component of our loss function.\n",
    "And we also have more of the style because at the early layers we're evaluating the style loss.\n",
    "It doesn't have the same layout as the style image spider web, \n",
    "but it has the same kinds of textures and structures.\n",
    "The final result has done our goal of taking one image and do it in the style of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb838ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorModel(content_im) # Start from content image\n",
    "style_loss = StyleLossToTarget(style_im)\n",
    "content_loss = ContentLossToTarget(content_im)\n",
    "def combined_loss(x):\n",
    "    return style_loss(x) + content_loss(x)\n",
    "learn = Learner(model, get_dummy_dls(150), combined_loss, lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\n",
    "learn.fit(1, cbs=[ImageLogCB(30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00c44c-2210-4019-b7c1-da3cb5d621c1",
   "metadata": {},
   "source": [
    "Notice that the spider web is nicely laid out, she's almost picking it out with her fingers,\n",
    "and her face, very important in terms of object recognition, the model didn't mess with the face.\n",
    "match at all so it's kept the spider webs away from that like I think it's the more you look at it the more\n",
    "This is impressive how it's managed to find a way to add spider webs without messing up the\n",
    "overall semantics of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(learn.model().clip(0, 1)); # View the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c14a97",
   "metadata": {},
   "source": [
    "And trying with random starting image, weighting the style loss lower, using different layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3aa3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorModel(torch.rand_like(content_im))\n",
    "style_loss = StyleLossToTarget(style_im)\n",
    "content_loss = ContentLossToTarget(content_im, target_layers=(6, 18, 25))\n",
    "def combined_loss(x):\n",
    "    return style_loss(x) * 0.2 + content_loss(x)\n",
    "learn = Learner(model, get_dummy_dls(300), combined_loss, lr=5e-2, cbs=cbs, opt_func=torch.optim.Adam)\n",
    "learn.fit(1, cbs=[ImageLogCB(60)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e907a-7707-40f2-a626-a45c49567d7e",
   "metadata": {},
   "source": [
    "TODO: find your own pictures and make sure not to steal someone's licensed work.\n",
    "There's a lot of Creative Commons images to try bashing together.\n",
    "Do it at a larger size, higher resolution style, etc.\n",
    "There's so much to experiment with: for example change the content loss to focus on.\n",
    "Maybe an earlier layer can start from a random image  instead of a content image.\n",
    "Or we can start from the style image and optimize towards the content image.\n",
    "We can change how we scale these two components of the loss function, how long to train for, the learning rate, etc. \n",
    "What you can optimize and what you can explore, get different results with\n",
    "different scalings and different focus layers...\n",
    "There is a lot of fun experimentation to finding a set of parameters that give \n",
    "a pleasent result for a given style content pair, and for a given effect on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d898d839",
   "metadata": {},
   "source": [
    "## For Comparison: non-miniai version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267e9a2-5d19-4b53-812a-486ef0dd9e17",
   "metadata": {},
   "source": [
    "On the experimentation version, we're doing all this work setting up these callbacks,\n",
    "and isn't it nicer to just have here's my image that I'm optimizing, \n",
    "set up an Optimizer, a loss function, and an optimization Loop.\n",
    "It is easier when we want to do it once.\n",
    "In a tutorial we keep it as minimal as possible, just show what style loss is...\n",
    "But when we want to try it again, adding a different layer, add some progress, etc., it gets messy quickly.\n",
    "As soon as we want to save images for a video and mess with the loss function,\n",
    "do some sort of annealing on the learning rate, ....\n",
    "Each of these things is going to grow this loop into something messier.\n",
    "It was fun to quickly  convert to being able to experiment with a completely new version \n",
    "with minimal lines of code, minimal changes, \n",
    "having everything in its own piece like the the image logging.\n",
    "Or to make a movie showing the progress that goes in a separate callback.\n",
    "To tweak the model we're just tweaking one thing, \n",
    "all the other infrastructure can stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image to be optimized\n",
    "im = torch.rand(3, 256, 256).to(def_device)\n",
    "im.requires_grad = True\n",
    "\n",
    "# Set up the optimizer\n",
    "opt = torch.optim.Adam([im], lr=5e-2)\n",
    "\n",
    "# Define the loss function\n",
    "style_loss = StyleLossToTarget(style_im)\n",
    "content_loss = ContentLossToTarget(content_im, target_layers=[6, 18, 25])\n",
    "def combined_loss(x):\n",
    "    return style_loss(x) * 0.2 + content_loss(x)\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(300):\n",
    "    loss = combined_loss(im)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "# Show the result\n",
    "show_image(im.clip(0, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f76f93-1490-46fe-a158-b896a3d88a89",
   "metadata": {},
   "source": [
    "## Comments\n",
    "T: May be worthwhile to play with other (non VGG) networks, as some special properties of VGG\n",
    "that are good for style transfer.\n",
    "There are papers that explore how to use other networks for style transfer that maintain some of the properties of VGG.\n",
    "Explore these papers, and use the framework that allows us to Plug and Play different networks.\n",
    "\n",
    "JH: taking a convnet or a resnet, replacing its head with a VGG head would be interesting to try.\n",
    "On the experimentation version, we're doing all this work setting up these callbacks,\n",
    "and isn't it nicer to just have here's my image that I'm optimizing, \n",
    "set up an Optimizer, a loss function, and an optimization Loop.\n",
    "\n",
    "It is easier when we want to do it once.\n",
    "In a tutorial we keep it as minimal as possible, just show what style loss is...\n",
    "But when we want to try it again, adding a different layer, show progress, save images for a video,\n",
    "change the loss function, annealing on the learning rate, etc., the training loop gets messy quickly.\n",
    "With miniAI it was fast to experiment with a completely new version with minimal lines of code, minimal changes, \n",
    "having everything in its own piece like the the image logging.\n",
    "To tweak the model we touch one thing and the other infrastructure can stay the same.\n",
    "\n",
    "JH: When using the fastAI library people jump straight into Data blocks.\n",
    "But they may be working on a custom thing where there isn't a data block already written for.\n",
    "Step One is lets write a data block, and that's not easy.\n",
    "Would be better to focus on building a model.\n",
    "JH tells people to go down a layer of abstraction.\n",
    "JH doesn't often start at the lowest level of abstraction, because he does not do it right, e.g., forget zero_grad.\n",
    "Easier to mess up something, especially when trying to have it run quick.\n",
    "So he starts at a reasonably low level, but not the lowest.\n",
    "<br>\n",
    "With miniAI we understand all the (few) layers. \n",
    "We could write our own e.g., trainCB, and it ensures to use torch.nograd, and to put it in a Val mode, etc.,\n",
    "and it will be done correctly.\n",
    "We can run a learning rate finder, run on Cuda, etc.\n",
    "We can have \"our own\"  framework where there are multiple ways of doing the same thing.\n",
    "For example, Jonna showed with the imageOptCB callback can be implemented in different ways.\n",
    "\n",
    "Using these pre-trained networks as smart feature extractors is powerful.\n",
    "Unlike the \"fun crazy\" example that we're going to look at, they also have very valid uses.\n",
    "For example, for super resolution or even something like difusion,\n",
    "adding in a perceptual loss or even a style loss to the target image can improve things. \n",
    "We've played around with using perceptual loss for diffusion, \n",
    "or even to generate an image that matches a face, image to image with stable diffusion.\n",
    "Maybe you have an extra guidance function that makes sure that structurally it matches \n",
    "is but maybe it actually doesn't.\n",
    "Maybe you want to pass in a style image and have that guide the diffusion process \n",
    "to be a particular style without having to say the style name.\n",
    "For many image to image tasks, using the features from a network like vgg \n",
    "has many practical uses apart from artistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360535f",
   "metadata": {},
   "source": [
    "What do you think are some pros and cons? How would this look once we start displaying progress, testing different configurations and so on?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
