{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A challenge experimenting was that the generated images looked good, hence it was\n",
    "easy to convince ourselves we're improving.\n",
    "But there is no metric that indicates that these generated images would look to a human as pictures of clothes.\n",
    "Only a person can do that.\n",
    "There are some useful metrics which give an approximation, but are not a replacement for humans.\n",
    "We will see *FID* the most common metric, and another metric called *KID*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil,torch,random\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from scipy import linalg\n",
    "\n",
    "from fastcore.foundation import L\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *\n",
    "from miniai.accel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "from torch import distributions\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbddca884ad435f854db32010fe25d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 512\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = xb,yb = next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate them using a saved model from notebook 17_, and getting the FID for it. \n",
    "In notebook 14 we created a `summary()` that shows the different blocks of the model.\n",
    "There are various different output shapes, e.g., in the first block \n",
    "it's a batch size of 1024, 16 channels 28x28 and then we had 32 channels 14x14, ....\n",
    "Before the final linear layer we had 1024 batches and 512 Channels, with no height and width.\n",
    "The idea of FID and KID is that the distribution of these 512 channels for a real image has a particular \n",
    "signature, i.e., looks a particular way.\n",
    "We're going to take our samples, run it through a model that's learned to predict\n",
    "e.g., \"fashioned glasses\", and we're going to grab the \"GlobalAvgPooling\" layer, \n",
    "and then we're going to average it across a batch right to get 512 numbers, that \n",
    "represent the mean of each of those channels.\n",
    "Those channels might represent \"features\", for example, \"does it have a pointed collar\",\n",
    "\"does it have smooth fabric\", \"does it have sharp heels\", etc.\n",
    "We could recognize that something is probably not a normal fashion image if it \n",
    "has sharp heels and flowing fabric.\n",
    "There are certain sets of means of these activations that don't make sense.<br>\n",
    "This is not a metric for an individual image, but for a set of images.\n",
    "We generate fashion images and ask do they look like a bunch of fashion images.\n",
    "If we look at maybe X% have this feature and have that feature.\n",
    "Looking at those means is like comparing the distribution within all these images generated,\n",
    "do they roughly have the same amount, sharp colors as those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start at that level which is this `feats.mean()`. \n",
    "We take our samples and we pass them through a pre-trained model that has learned \n",
    "to predict what \"type of fashion\" something is.\n",
    "We trained some of those in the 14_ notebook, specifically we trained \n",
    "a 20 epoch one in the data augmentation section, which had a 94.3% accuracy.\n",
    "If we pass our samples through this model we expect to get some useful features.\n",
    "This was a bit complicated because this model was trained using data that had gone through \n",
    "a transformation of subtracting the mean and dividing by the standard deviation,\n",
    "and that is not what we're creating in our samples.\n",
    "Most of the diffusion models samples tend to be between -1 and 1.\n",
    "JH added a new section to the bottom of the 14_ notebook which replaces the transform with something\n",
    "that goes from -1 to 1 and just creates those data loaders and\n",
    "then trains  something that can classify fashion, and saved this as `data_aug2.pkl`.\n",
    "It is the same as before but it's a fashion classifier where the inputs are expected to be between -1 and 1.\n",
    "BUT, our image samples are NOT between -1 and 1.\n",
    "In notebook 17_ ddpm2 we use `TF.totensor()`, that makes images that are between 0 and 1. \n",
    "This seems to be \"a bug\", that the images go between 0 and 1, so we'll look at fixing that in a moment.\n",
    "For now we're just trying to get the FID of our existing model.\n",
    "We take the output of our model and we need to multiply by 2, \n",
    "so that it will be between 0 and 2 and subtract 1, to change our samples to be between -1 and 1.\n",
    "Now we can pass them through our pre-trained fashion classifier.\n",
    "How do we get the output of that pooling layer which is what we want?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flex our Pytorch muscles lets show a couple of ways to do it.\n",
    "Lets load the `data_aug2.pkl` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/d0/mqk8_wqx39j87cld43llgktm0000gn/T/ipykernel_33455/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3113811680.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/d0/mqk8_wqx39j87cld43llgktm0000gn/T/ipykernel_33455/3113811680.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">771</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 768 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">'encoding'</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> pickle_load_args.keys():                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 769 │   │   </span>pickle_load_args[<span style=\"color: #808000; text-decoration-color: #808000\">'encoding'</span>] = <span style=\"color: #808000; text-decoration-color: #808000\">'utf-8'</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 770 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 771 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> _open_file_like(f, <span style=\"color: #808000; text-decoration-color: #808000\">'rb'</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> opened_file:                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 772 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _is_zipfile(opened_file):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 773 │   │   │   # The zipfile reader is going to advance the current file position.</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 774 │   │   │   # If we want to actually tail call to torch.jit.load, we need to</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">270</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_open_file_like</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 267 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 268 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_open_file_like</span>(name_or_buffer, mode):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 269 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _is_path(name_or_buffer):                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 270 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _open_file(name_or_buffer, mode)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 271 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 272 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">'w'</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> mode:                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 273 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _open_buffer_writer(name_or_buffer)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">251</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 248 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 249 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">_open_file</span>(_opener):                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 250 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name, mode):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 251 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>(_open_file, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>).<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(name, mode))                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 252 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 253 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__exit__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args):                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 254 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.file_like.close()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">FileNotFoundError: </span><span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> No such file or directory: <span style=\"color: #008000; text-decoration-color: #008000\">'models/data_aug2.pkl'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/var/folders/d0/mqk8_wqx39j87cld43llgktm0000gn/T/ipykernel_33455/\u001b[0m\u001b[1;33m3113811680.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'/var/folders/d0/mqk8_wqx39j87cld43llgktm0000gn/T/ipykernel_33455/3113811680.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m771\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mload\u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 768 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m'\u001b[0m\u001b[33mencoding\u001b[0m\u001b[33m'\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m pickle_load_args.keys():                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 769 \u001b[0m\u001b[2m│   │   \u001b[0mpickle_load_args[\u001b[33m'\u001b[0m\u001b[33mencoding\u001b[0m\u001b[33m'\u001b[0m] = \u001b[33m'\u001b[0m\u001b[33mutf-8\u001b[0m\u001b[33m'\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 770 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 771 \u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m _open_file_like(f, \u001b[33m'\u001b[0m\u001b[33mrb\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m opened_file:                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 772 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _is_zipfile(opened_file):                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 773 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The zipfile reader is going to advance the current file position.\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 774 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m270\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_open_file_like\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 267 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 268 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_open_file_like\u001b[0m(name_or_buffer, mode):                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 269 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m _is_path(name_or_buffer):                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 270 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _open_file(name_or_buffer, mode)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 271 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 272 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m'\u001b[0m\u001b[33mw\u001b[0m\u001b[33m'\u001b[0m \u001b[95min\u001b[0m mode:                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 273 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _open_buffer_writer(name_or_buffer)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/germangoldszmidt/mambaforge/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m251\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 248 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 249 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92m_open_file\u001b[0m(_opener):                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 250 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m, name, mode):                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 251 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m(_open_file, \u001b[96mself\u001b[0m).\u001b[92m__init__\u001b[0m(\u001b[96mopen\u001b[0m(name, mode))                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 252 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 253 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__exit__\u001b[0m(\u001b[96mself\u001b[0m, *args):                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 254 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.file_like.close()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \u001b[32m'models/data_aug2.pkl'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbs = [DeviceCB(), MixedPrecision()]\n",
    "model = torch.load('models/data_aug2.pkl')\n",
    "learn = Learner(model, dls, F.cross_entropy, cbs=cbs, opt_func=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a hook, using `HooksCallback` to create a function `append_outp`\n",
    "which just depends on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_outp(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'outp'): hook.outp = []\n",
    "    hook.outp.append(to_cpu(outp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model is a Sequential, we go through the layers, to find the layer we want, '6'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcb = HooksCallback(append_outp, mods=[learn.model[6]], on_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've hooked that we can pass that as a callback.\n",
    "It is a bit \"weird\" calling `fit()` because `train=False`, we just want \n",
    "to make one batch go through and grab the outputs in our hook's `outp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, train=False, cbs=[hcb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then grab a few (64) of those to have a look, it is a 64 by 512 set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = hcb.hooks[0].outp[0].float()[:64]\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do it.\n",
    "Sequential Modules are Python collections that have an API that they're expected to support.\n",
    "There is a delete \"something\" call for a collection, like a list.\n",
    "We can delete the last 2 layers and be left with just the other layers.\n",
    "That means we can just call `capture_preds`. \n",
    "We delete layers eight and seven, call `capture_preds` and this is\n",
    "going to give us the entire 10 000 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(learn.model[8])\n",
    "del(learn.model[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats,y = learn.capture_preds()\n",
    "feats = feats.float()\n",
    "feats.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betamin,betamax,n_steps = 0.0001,0.02,1000\n",
    "beta = torch.linspace(betamin, betamax, n_steps)\n",
    "alpha = 1.-beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisify(x0, ᾱ):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    ᾱ_t = ᾱ[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = ᾱ_t.sqrt()*x0 + (1-ᾱ_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl], alphabar)\n",
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2 = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "class UNet(UNet2DModel):\n",
    "    def forward(self, x): return super().forward(*x).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate them using the model we changed in the last lesson, DDPMv2, in notebook 17_, where\n",
    "we trained with mixed precision and saved it as `fashion_ddpm_mp.pkl`.\n",
    "We're going to try to get the FID for a model we've already trained.\n",
    "We get the trained samples model `smodel`, \n",
    "with `torch.load`, and then `cuda()` to move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel = torch.load('models/fashion_ddpm_mp.pkl').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample()` is a copy of dbpm from the last time.\n",
    "We're going to sample from that model, and try to calculate the FID score, which \n",
    "indicates how similar are the sample to real images.\n",
    "For the sample images we look at some statistics of some of the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, sz, alpha, alphabar, sigma, n_steps):\n",
    "    device = next(model.parameters()).device\n",
    "    x_t = torch.randn(sz, device=device)\n",
    "    preds = []\n",
    "    for t in reversed(range(n_steps)):\n",
    "        t_batch = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "        z = (torch.randn(x_t.shape) if t > 0 else torch.zeros(x_t.shape)).to(device)\n",
    "        ᾱ_t1 = alphabar[t-1]  if t > 0 else torch.tensor(1)\n",
    "        b̄_t = 1 - alphabar[t]\n",
    "        b̄_t1 = 1 - ᾱ_t1\n",
    "        x_0_hat = ((x_t - b̄_t.sqrt() * model((x_t, t_batch)))/alphabar[t].sqrt())\n",
    "        x_t = x_0_hat * ᾱ_t1.sqrt()*(1-alpha[t])/b̄_t + x_t * alpha[t].sqrt()*b̄_t1/b̄_t + sigma[t]*z\n",
    "        preds.append(x_0_hat.cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "samples = sample(smodel, (256, 1, 32, 32), alpha, alphabar, sigma, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = samples[-1]*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(s[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new data loader which contains no training batches, \n",
    "it contains one validation batch which contains the samples.\n",
    "It doesn't matter what the dependent variable is so we just put in the same dependent variable \n",
    "that we already had `yb`.\n",
    "We use that to extract some features from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearn = TrainLearner(model, DataLoaders([],[(s,yb)]), loss_func=fc.noop, cbs=[DeviceCB()], opt_func=None)\n",
    "feats2,y2 = clearn.capture_preds()\n",
    "feats2 = feats2.float().squeeze()\n",
    "feats2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = feats.mean(0)\n",
    "means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = feats.T.cov()\n",
    "covs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ways of comparing two sets of data to evaluate if they are from the same distribution.\n",
    "The FID metric evaluates if they have similar covariance matrices and similar mean vectors.\n",
    "`_calc_stats` returns the means and the covariance metrics.\n",
    "We call `_calc_stats` for both the sample features and the actual dataset or the test dataset.\n",
    "Given those `s1,s2` features we can calculate the \"Fresher Inception Distance\" (FID). \n",
    "Since we multiply together the two covariance matrices the result is going to be bigger, \n",
    "thus we need to scale it down.\n",
    "When working with scalars, after we multiply two of them, we take the square root to scale back down to\n",
    "the original scale.\n",
    "To \"renormalize\" these matrices we've got to take the matrix square root.\n",
    "We are going to slightly cheat. \n",
    "We've used the float square root from Python's standard library, and it isn't interesting.\n",
    "To calculate the float square root the classic way is to use Newton's method,\n",
    "by solving a*a=x, and solve it using the derivative, and taking a step along the derivative a bunch of times.\n",
    "We can do the same to calculate the matrix square root using the Newton method, but \n",
    "for matrices it is slightly more complicated, see `_sqrtm_newton_schultz()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _sqrtm_newton_schulz(mat, num_iters=100):\n",
    "    mat_nrm = mat.norm()\n",
    "    mat = mat.double()\n",
    "    Y = mat/mat_nrm\n",
    "    n = len(mat)\n",
    "    I = torch.eye(n, n).to(mat)\n",
    "    Z = torch.eye(n, n).to(mat)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        T = (3*I - Z@Y)/2\n",
    "        Y,Z = Y@T,T@Z\n",
    "        res = Y*mat_nrm.sqrt()\n",
    "        if ((mat-(res@res)).norm()/mat_nrm).abs()<=1e-6: break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have implemented it from scratch we can use the one from scipy, `linalg.sqrtm()`,\n",
    "to give us a measure of similarity between the two covariance matrices.\n",
    "And here's the measure of similarity between the two mean matrices,\n",
    "just the sum of squared errors, and it's just normalizing.\n",
    "The \"trace\" is the sum of the diagonal elements.\n",
    "We need to add traces and subtract two times the trace.\n",
    "The result is the Fresher Inception Distance, a number which represents how similar a set of samples \n",
    "are to some real image data.\n",
    "The name \"Fresher Inception Distance\" is peculiar, it has nothing to do with Inception, but as\n",
    "people use the famous Inception model (Imagenet winning model from Google brain).\n",
    "Inception is not a good model to use for this, it just happens to be the one which the original paper used, so everybody now uses it to compare results.\n",
    " \n",
    "We're going to get a more accurate metric by using a model that is good at recognizing fashion MNIST.\n",
    "It's better to use a model that we have trained on our data and we know it's good at that.\n",
    "It is not a \"FID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _calc_stats(feats):\n",
    "    feats = feats.squeeze()\n",
    "    return feats.mean(0),feats.T.cov()\n",
    "\n",
    "def _calc_fid(m1,c1,m2,c2):\n",
    "#     csr = _sqrtm_newton_schulz(c1@c2)\n",
    "    csr = tensor(linalg.sqrtm(c1@c2, 256).real)\n",
    "    return (((m1-m2)**2).sum() + c1.trace() + c2.trace() - 2*csr.trace()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1,s2 = _calc_stats(feats),_calc_stats(feats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(s1[0].data).any(), np.isnan(s2[0].data).any(), np.isnan(s1[1].data).any(), np.isnan(s2[1].data).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_calc_fid(*s1, *s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two caveats of FID: \n",
    "1) it is dependent on the number of samples used, more/less accurate with more/less samples. \n",
    "It is biased, so with less samples it's too high, papers need to report how many samples they used.\n",
    "2) Because of using the Inception Network, all images are at a size 299x299, the size that the Inception model was trained.\n",
    "\n",
    "Applying this Inception Network for measuring the distance means resizing the images to 299x299 which\n",
    "may not make sense, eg Fashion MNIST is 28x28, resize it to 299 .. :)\n",
    "Also for larger, e.g.  512x512 or 1024x1024 images, shrink them to 299x299, loosing a lot of detail.\n",
    "It is a problem for some of these latest papers, the FID scores and how they're comparing them,\n",
    "and then visually they are better images but the FID score doesn't capture that, because it is using shrinked images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _squared_mmd(x, y):\n",
    "    def k(a,b): return (a@b.transpose(-2,-1)/a.shape[-1]+1)**3\n",
    "    m,n = x.shape[-2],y.shape[-2]\n",
    "    kxx,kyy,kxy = k(x,x), k(y,y), k(x,y)\n",
    "    kxx_sum = kxx.sum([-1,-2])-kxx.diagonal(0,-1,-2).sum(-1)\n",
    "    kyy_sum = kyy.sum([-1,-2])-kyy.diagonal(0,-1,-2).sum(-1)\n",
    "    kxy_sum = kxy.sum([-1,-2])\n",
    "    return kxx_sum/m/(m-1) + kyy_sum/n/(n-1) - kxy_sum*2/m/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KID\n",
    "The **KID (Kernel Inception Distance)** metric compares two distributions in a way that is not biased,\n",
    "so it's not necessarily higher or lower if you use more or less samples.\n",
    "It is simpler to calculate than the FID.\n",
    "We create a set of groups or partitions, go through each of those partitions\n",
    "and grab a few `x`'s at a time and a few of `y`'s at a time.\n",
    "Then we calculate the **MMD**, which does a matrix product, we take the cube of it.\n",
    "`k` is the kernel and we do that for the first sample bytes \n",
    "compared to itself the second compared to itself and the first compared to the second.\n",
    "We then normalize them in various ways and add to the two with themselves together \n",
    "and subtract the with the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _calc_kid(x, y, maxs=50):\n",
    "    xs,ys = x.shape[0],y.shape[0]\n",
    "    n = max(math.ceil(min(xs/maxs, ys/maxs)), 4)\n",
    "    mmd = 0.\n",
    "    for i in range(n):\n",
    "        cur_x = x[round(i*xs/n) : round((i+1)*xs/n)]\n",
    "        cur_y = y[round(i*ys/n) : round((i+1)*ys/n)]\n",
    "        mmd += _squared_mmd(cur_x, cur_y)\n",
    "    return (mmd/n).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_calc_kid(feats, feats2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KID does not use the stats (the means and covariance Matrix), it uses the features directly.\n",
    "The final result is the mean of this calculated across different little batches.\n",
    "It gives us a measure of the similarity of these two distributions.\n",
    "JH was unsure why more people weren't using KID, since it doesn't have a bias problem.\n",
    "After using it for a while, the reason is that it has a very high variance,\n",
    "i.e., when we call it multiple times with just samples with different random seeds \n",
    "we get very different values, hence, not useful. \n",
    "\n",
    "We don't have a good unbiased metric.\n",
    "Even if we did it would only tell how similar distributions are to each other, \n",
    "it doesn't tell us whether they look good.\n",
    "That is why all good papers have a section on human testing.\n",
    "Still, this fit is useful for comparing fashion images. \n",
    "Humans are good at looking at faces at a reasonably high resolution, \n",
    "but we're not good at looking at 28x28 fashion images.\n",
    "So it's particularly helpful for stuff that our brains aren't good at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap this up into a `ImageEval` class.\n",
    "We are going to pass in a pre-trained `model` for a classifier, `dls` data loaders,  \n",
    "which we're going to use to calculate the real images.\n",
    "\n",
    "We call `capture_preds` to get the features for the real images, and then we can also calculate the stats for the real images.\n",
    "We call `_calc_fid` passing in the `stats` for the real images and `_calc_stats` for the features from our samples, `samp`.\n",
    "Where the features are given by `get_feats()`, we pass in `samp`, any random `y` value is fine, we \n",
    "have a single tensor `tensor([0])`, and call `capture_preds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ImageEval:\n",
    "    def __init__(self, model, dls, cbs=None):\n",
    "        self.learn = TrainLearner(model, dls, loss_func=fc.noop, cbs=cbs, opt_func=None)\n",
    "        self.feats = self.learn.capture_preds()[0].float().cpu().squeeze()\n",
    "        self.stats = _calc_stats(self.feats)\n",
    "\n",
    "    def get_feats(self, samp):\n",
    "        self.learn.dls = DataLoaders([],[(samp, tensor([0]))])\n",
    "        return self.learn.capture_preds()[0].float().cpu().squeeze()\n",
    "\n",
    "    def fid(self, samp): return _calc_fid(*self.stats, *_calc_stats(self.get_feats(samp)))\n",
    "    def kid(self, samp): return _calc_kid(self.feats, self.get_feats(samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an `ImageEval` object `ie` passing in our classifier, data loaders with the real data,\n",
    "and any other callbacks we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = ImageEval(model, learn.dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `ie.fid`, and 33.9 is the Fid per these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ie.fid(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kid is on a very different scale, e.g. only 0.05, generally much smaller than FIDs.\n",
    "We are mainly going to be looking at FIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ie.kid(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what happens if we call FID on sample 0, then sample 50,  etc.\n",
    "all the way up to 900, and then we also do samples 975 990 and 999.\n",
    "Over time our samples improved so that's a good test.\n",
    "It is curious that they stopped improving....\n",
    "JH has not seen anybody plot this graph before, and it's something to look at because it's telling us\n",
    "if the sampling is making consistent improvements.\n",
    "\n",
    "To clarify this is like the predicted denoised sample at the different stages during sampling.\n",
    "if I was to stop something now and just go straight to the predicted X error what would the FID be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = L.range(0,1000,50)+[975,990,999]\n",
    "plt.plot(xs, [ie.fid(samples[i].clamp(-0.5,0.5)*2) for i in xs]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the KID, and the plots look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = L.range(0,1000,50)+[975,990,999]\n",
    "plt.plot(xs, [ie.kid(samples[i].clamp(-0.5,0.5)*2) for i in xs]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to take the FID of an actual batch of data, to tell us how good we could get.\n",
    "That's a bit unfair because I think the different sizes our data is 512, our sample is 256,\n",
    "but anyway there it's it's a pretty huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.fid(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.kid(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception\n",
    "\n",
    "What does it take to get a real FID with an Inception Network.\n",
    "We are not reimplementing the Inception Network, as it is obsolete, just grab it from Pytorch_fid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid.inception import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor([1,2,3])\n",
    "a.repeat((3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `resize_input` to get  3 Channel 299 by 299 images\n",
    "Created a wrapper `IncepWrap` for an Inception V3 model.\n",
    "We call `forward` with a batch and just replicates the single channel 3 times to create a 3 \n",
    "Channel version of a black and white image.\n",
    "TODO: Flex Pytorch muscles by getting an Inception model working on the Fashion MNIST samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncepWrap(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = InceptionV3(resize_input=True)\n",
    "    def forward(self, x): return self.m(x.repeat(1,3,1,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass that to `ImageEval` and it gives 63.8 and on a real batch of data it gets 27.9,\n",
    "a sign that this is less effective than our real fashion mnist classifier. \n",
    "A difference of a ratio of three, our FID for real data using a real classifier was 6.6, that is encouraging.\n",
    "We now have a FID, more specifically we now have an image eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = ImageEval(IncepWrap(), dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ie.fid(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.fid(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ie.kid(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.kid(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J: Other FIDs reported are CIFAR tiny 32 by 32 pixels resized up to 299.\n",
    "FID is a slightly weird metric, if we saved images as jpegs, and then you load them, the FID may be twice as bad.\n",
    "The takeaway it's useful when using the same backbone model, the same approach, same number of samples, then we can compare apples to apples.\n",
    "<br>\n",
    "For our own experiments these metrics are good but they may not be to compare to other models, then it is best to rely on human studies. \n",
    "This is useful for for us all the time, we're going to use the same set number of samples and we're going to use the same fashion MNIST specific classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
