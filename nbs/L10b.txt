L10 Part 2: From the Foundations up
See also https://mlops.systems/computervision/fastai/parttwo/2022/10/24/foundations-mnist-basics.html

This is going to require tenacity and patience, but we are going to learn a lot. 
We're working now through the course22p2 repo.  
The notebooks are ordered, we start with notebook number one.  
The goal is to get to  Stable Diffusion from the foundations.  
We define the foundations rules as follows: we're allowed to use Python, 
the Python standard library, all the stuff that comes with Python by default. 
We're allowed to use Matplotlib as a plotting library, and Jupyter notebooks.
We use nbdev to create modules from notebooks.
We're going to rebuild everything starting from this foundation. 
We are allowed to use other libraries once we have re-implemented them correctly.
If we re-implement something from NumPy or from Pytorch, we're allowed to use the NumPy or Pytorch version.  
When we create new things they become part of our own library, “miniai”, a little framework as we go. 

The Imports all come from the Python standard library except for two from matplotlib. 
The models used in Stable Diffusion were trained on millions of dollars worth of equipment for months.
As we don't have that time or money, we're going to create smaller versions of them. 
Once we've got them working, we'll then be allowed to use the big pre-trained versions. 
We will get own VAE, U-Net, CLIP encoder, etc. 

We're going to need to be do some matrix multiplication, so lets take a deep dive into matrix multiplication.
For data we use the MNIST handwritten digits, 28 by 28 pixel, grayscale images.
We can download them from this URL. 
Python's pathlib Path object takes a string and turns it into something that we can treat as a directory path.
For example, we can use "/" (slash) to mean: this  file inside this subdirectory. 
So this is how we create a Path object. 
Path objects have, for example, a make directory method.  

To get everything set up, we want to be able to rerun this cell many times without errors.
If we run it a second time it still works, because we put `exist=True`.  
To know what parameters I can pass to `make` we press shift tab. 
If we press it a few times it'll pop it down at the bottom of the screen.
Press escape to get rid of it. 
We can hit tab inside and it'll list  all the things we could type.

To grab the URL Python has the urllib library `urlretrieve`. 
TODO: read the Python documentation, for every single method we use.   
So if we click we get the documentation for urlretrieve. 
We can find what it can take and learn about what it does.
We look at every single option that it takes and then practice with it inside Jupyter. 

Ctrl Shift Hyphen splits a cell into two cells.
Option Enter creates a cell underneath.
Type urlretrieve, Shift Tab and there is the info.  
If we are somewhere in the notebook, and have no idea where urlretrieve comes from,  
we can hit Shift Tab and it tells me.  
To know more about it, we can type question mark (“?”) after the name, 
shift enter and it's going to give me the documentation. 
With a second question mark (“??”)  it gives me the full source code. 
Reading the source code of the Python standard library is often quite revealing, a great way to learn more about it.

Lets use a simple functionality going to the URL to retrieve and the file name to save it as. 
Again, made it so we can run it multiple times, i.e., it's only going to do the urlretrieve if the path doesn't exist. 
If we have already downloaded it I don't want to download it again. 
We can put exclamation mark (“!”) followed by line of Bash and it runs this using Bash.  
(If you're using Windows this won't work, use WSL and then all these notebooks will work.)

This is a gzip file, Python has a gzip module so we can open a gzip file using `gzip.open()` 
and we can pass in the path, then say we're going to  read it as binary —as opposed to text—.  
A Context Manager “with” clause and it's going to open up the gzip file, 
the  gzip object will be called “f” and then it runs  everything inside the block,
and when it's done it  will close the file automatically. 
A “with” block can do different things.

The gzip file contains a pickle object, a Python object that have been saved to disk. 
It's the main way in pure Python to save stuff and it's part of the standard library. 
So this is how we load in from that file. 
The MNIST file contains a couple of tuples.
When we put a tuple on the left hand side of an equal sign it allows us to "destructure" it, 
a way to make code clear and concise.
It puts the first tuple  into two variables called x_train and y_train,
and the second tuple into x_valid and y_valid. 

Lets look at the data. 
We are not allowed to use NumPy —according to our rules—,  but unfortunately this comes as NumPy, we turn it into a list. 
We take the first image and turned it into a list so we can look at a few examples of some values in that list.   
Looks like they're numbers between 0 and 1. 
To learn about a new data set, grab a little bit of it and look at it, and get a sense of what it is. 
The image is a 784 long list because they are 28 by 28 images.  
We need to turn this 784 long list into a list of 28 lists of 28, (because we don't have matrices.) 
  
Lets define a function `chunks`.
`vals` is currently a list of 10 things.  
If we take “vals” and pass it to chunks() with 5 it creates two lists of five. 
It's chunkifying this list and this is the length of each chunk. 

We did it is using a very useful thing (and little known) in Python which is “yield”. 
And what “yield” does is,  you can see here I've got a loop, 
it's going  to go through from zero up to the length of the list and it's going to jump by five at a time.  
That's going to go, in this case, 0 comma 5.  
Yield is similar to return, it's going to return the list from 0 up to 5. 
So it returns the first  bit of the list. 
But yield doesn't just return, it returns a bit and then it continues, and it returns a bit more.  
yield creates an iterator, something that we can call “next” on a bunch of times. 

We can say iterator equals (val_iter =)... 
An iterator (`val_iter`) is something that we can “next” on. 
And next basically says: yield the  next thing. 
So this should yield vals[0:5]. 
If we run that again it's going to give a different answer because it's now 
up to the second part of this loop, so it returns the last five. 
If we pass an iterator to Python's list it runs through the entire (iter_val) iterator until  
it's finished and creates a list of the  results.
When finished, if we call next and get `StopIiteration`, it means there's nothing left in it.  

We now have a way of taking a list and chunkifying it.  
Now we can take an image, chunkify it into chunks of 28 long, turn it into a list, and plot it.

There are other ways to do iterators and generators, so lets look how to do them in Python. 
JH finds that some can throw away huge pieces of enterprise software and replace it with an iterator,  
that lets you stream things one bit at a time, it doesn't store it all in memory.
Python comes with a standard library “itertools” that makes it easier to work with iterators. 
Lets see one example which is “islice”.
So let's grab our 10 values.
We can take any list and turn it into an iterator by passing it to iter().
We can call `next(it)`, it's giving us each item, one at a time.
That's what converting it into an iterator does. 

“islice” converts “it” into a different kind of iterator, let's call it `isit`.  
We create the iterator and then call next a few times.  
It's now only returning the first five,  before it raises `StopIteration`. 
islice grabs the first n things from an iterable (something that  you can iterate).   
If we pass it to list() again this iterator has grabbed the first five things, so it's now up to thing number six.
If we call it again it's the next five things, and if we call it again, then there's nothing left.

Now lets do the "chunks" with `islice`.
To `iter`  we can pass a list (to create an iterator) or we can pass it  a “callable”. 
A callable is something that we can put parentheses after, a function, a class, etc.
So we're going to pass it a function and, in the second form, 
it's going to be called until the function returns this value here, which in this case is empty list. 
And `islice` will return empty list when it's done.  

So this here is going to  keep calling this function again and again, until it gets an empty list.  
So if we do it with 28 then we're going to get our image again. 
So we've now got two different ways of creating the same thing.  

If you've never used iterators before, now's a good time to play with them.
lambdas are anonymous functions defined inline.
Let's replace the lambda with a function and experiment with it.  
So let's create our iterator, then call f() on it…  and we can see there's the first 28. 
And  each time we do it we get another 28. 
The first 2 rows are all empty but then we got some values. 
Each time we get something else, just calling it again and again.  
And that is the value of an iterator. 
  
When we hit something in the code that doesn't look familiar, pause and experiment with that in Jupyter. 
For example, iter(), most people have not used it, certainly not used this two argument form.
So hit shift tab a few times and now you've got at the bottom a description of what it is. 
Or find out more: Python iter, go to the docs.python.org.
Iter returns an iterator object, what's that? click on it, find out.
Iter is important to know.
Here's that Stop Iteration exception. 

Now we've got an image, which is a list of lists, and each list is 28 long,  
we can index into it, so we can say image[20], is a list of 28 numbers and then we could index into that.  
Okay, so we can index into it, image[20][15]
We don't like to do M[i][j] for matrices, we would rather write it like M[i, j]
We have  to create our own class to make that work.  
To create a class in Python, we write “class”  and then write the name of it. 
Then we need to write "dunder" methods, with names of two underscores, a special word,  and then two underscores. 
These Dunder methods are special methods which have particular meanings to Python,
all documented in the Python data model (https://docs.python.org/3/reference/datamodel.html)

For example, click (__init__) and it tells you this is the thing that constructs objects.  
Anytime we want to create a class that we want to construct, 
and it's going to store some stuff (e.g., the image),  we have to define (__init__).
Python requires that in every  method we put “self” here 
—for reasons we don't really need to get into right now—, and then any parameters. 
So we're going to be creating an image passing in the thing to store the “xs”.
And we store it inside the self.  
Once we've got this, we got something that knows how to store the xs inside itself. 

We want to be  able to call ([20,15]). 
As part of the data model, there is `__getitem__`.  
When we call square brackets on an object that's what Python uses. 
And it's going to  pass across the [20,15] here, that's indices.  
We're just going to return this, so the self.xs, with the first index and the second index.   
So let's create that Matrix class and run that... and `m[20,15]` is the same.  

It's somewhat unusual to put definitions of methods on the same line, as the signature.
JH does it quite a lot for one-liners, to be able to see all the code on the screen at once.  
Many programmers have that approach, and works well for some that are very productive. 
It's not common in Python —some people are quite against it—.

Now that we created something that lets us index into things, we can use this feature in Pytorch.   
To create a  tensor, basically like our Matrix, we pass a list into tensor() and get back a tensor version of that list.
More interestingly, we can pass in a list of lists (img), and give this a name, tens.
Now we can say tens[20,15]. 

We've successfully reinvented that, so we can convert all of our lists into tensors. 
A convenient way to do this is to use the map function in the Python standard library.  
`map` takes a function and some iterables, in this case one iterable,  
and it's going to apply this function to each of these four things and return those four things.
We can put four results on the left to receive those four outputs.
It is going to call `tensor(x_train)` and put it in x_train, ditto for `y_train`, etc.
This is converting all of these lists to tensors and storing them back in the same name.  

`x_train` now is a tensor, so it has a `.shape` property: 
it has  50,000 images in it, which are each 784  long. 
We can find out what kind of stuff it contains by calling it  `.type()`, so it contains floats. 

This is the tensor class, read its documentation.  
Pytorch documentation some of it's good, some of it's not good.
So here's [tensor](https://pytorch.org/docs/stable/tensors.html). 
It's worth scrolling through to get a sense of how we can construct it.
This is how we constructed one before, passing it list of lists. 
We can also pass it NumPy arrays, change types, etc.
Its worth reading through, not going to look at every single  method,
but browse through it to get a general sense.
Tensors do just about everything for numeric programming.  
We want to know about every single one of these, or at least to be aware of what  
exists so we know what to search for in the docs.
Otherwise we end up recreating stuff from scratch, which is slower than reading the documentation to find out it's there.

Instead of chunks, or  islice, the reshape method is roughly equivalent in a tensor.
To reshape our 50,000 by 784 tensor into 50,000 28 by 28 tensors, we write `reshape(50,000, 28, 28)`.  
`-1` means just fill this with all the rest, e.g., it can figure out that must be  50,000. 

APL
There is an interesting history of tensor/array programming that goes back to a language called APL.  
Originally a mathematical notation, it was used to as a notation for defining how certain IBM systems would work. 
It's a mathematical notation that was designed to be more consistent expressive by Ken Iverson. 
Implementations that allowed this notation to be executed on a computer were also called  APL. 
Ken Iverson studied an area of physics called tensor analysis.  
As he developed APL he took these ideas from tensor analysis and put them into a programming language. 
In APL you can define a variable and rather than using "equals" use an arrow to  assign values.
We can define a to be a tensor, then look at the contents of “a”, and operations like a x 3, or a - 2,  etc.
APL takes the contents of this tensor and it's multiplying them all by 3, or subtracting  2 from all of them. 
We can put into “b” a different tensor and  we can now do things like “a” divided by “b”,
and it takes each element of “a” and dividing it by each element of “b”.
This is very interesting because we don't have to write loops, we can just express things directly. 
We can multiply vectors by scalars (rank one tensors),  divide one vector by another, etc.
APL didn't call them tensors, but arrays.  
NumPy, influenced  by APL, also calls them arrays, but Pytorch calls them tensors.  
They are rectangular blocks of  numbers, one-dimensional, like  a vector,
two-dimensional like a matrix, 3-dimensional, like a bunch of stacked matrices, etc.
There is an APL and array programming  section on the forums,  
and also a whole set of notes on every single glyph in  APL.
These cover interesting mathematical concepts, like, complex direction and magnitude.
Many people who do APL say that they become better programmers in the process.
There is a set of 17 study sessions of an hour or two each,
covering  the entirety of the language, every single glyph.  

So this batch of 50,000 28x28 images, a rank-3 tensor in Pytorch. 
NumPy would call it an array with three dimensions, those are the same thing.  
The rank is the number of dimensions. 
A particular image is a matrix, a 28 by 28 rank-2 tensor.  
A vector is a rank-1 tensor, a scalar is a rank 0 tensor.
Many languages and libraries don't do it that way, so what is a scalar is dependent on the language.

We can index into the 0th image, 20th row, 15th column to get back this same number.  
We can take `x_train.shape` and destructure it into “n”,  
number of images, “p”, the number of pixels, etc. 
Some example of a bit of the y_train, which is going to be the labels, the digits. 
And this is its shape so there's just 50,000 of these labels. 
We're allowed to use `min()` and `max()` so we can find out in y_train what's the smallest number,  
and what's the maximum number. 
So they go from 0  to 9, which are scalar tensors. 
  
We're allowed to use random numbers, because there is a random number generator in the python standard library.
Still, we're going to do random numbers from scratch ourselves, because it's instructive.
There is no way in pure software, to create a random number, we need a 
[Hardware random number generator](https://en.wikipedia.org/wiki/Hardware_random_number_generator).
The Australian National University quantum random number generator looks at the quantum fluctuations 
of a vacuum and provides an API which will return them.
That is one way to get random numbers.  
Cloudflare has a huge wall full of lava lamps and it uses the pixels of a camera looking at those  
lava lamps to generate random numbers. 
(Some?) Intel chips have RDRAND, which will return random numbers.
These things are slow.  
They can get us one random number from time to time. 
But we want a way of getting lots of random numbers, so we use a 
[Pseudorandom number generator, PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator).
A PRNG is a mathematical function that each time we call it returns it will return a number that "looks" random.  

To show you what I mean by that, I'm going to run some code.  
And I've created a function —which  we'll look at in a moment— called “rand”  
and if I call rand() 50 times and plot it,  there's no obvious relationship between one  call and the next. 
That's one thing that I  would expect to see from my random numbers,  
I would expect that each time I call rand(), the  numbers would look quite different to each other.  
The second thing is: rand() is meant  to be returning uniformly distributed  
random numbers and therefore, if I call  it lots and lots and lots of times, 
and plot its histogram, I would expect to see  exactly this, which is each, from 0 to 0.1  
there's a few, from 0.1 to 0.2 there's a few.  From 0.2 to 0.3 there's a few. 
It's a fairly  evenly spread thing. These are the two key things  I would expect to see. An even distribution of  
random numbers and that there's no (obvious) correlation, from one to the other.

We're going to create `rand()` a function that has the above properties, 
using the Wickman-Hill algorithm, which Python used before version 2.3.  
Random state is a global variable of one or more numbers. 
We start with no random state at all, and define a function `seed()` that we're going to pass something to. 
We can just mash the keyboard to create this number, or get it
from a hardware generator ANU , Cloudflare, RDRAND, etc., or the current tick count in nanoseconds.
In Python pretty much always use a number 42.
There's various  ways of getting some random starting point.  
When we pass it into `seed()`, it's going to do a bunch of modular divisions, 
create a tuple of three things, and store them in this global state, `rnd_state`.

Now this function returns (pseudo) random numbers.  
We pull out the random state at the start,  we do some math ops to it, 
and then we store new random state.  
Each time we call it we get a different number, so this is a random number generator.
Is important to remember that random number generators rely on this state.

“fork()” creates a whole separate copy of this Python process. 
In one copy, the parent, `os.fork()` returns True, and on the other copy, the child, it returns `False`.
In each copy we  call `rand()`, and expect to get 2 different random numbers. 
But they are the same number, because the processes are copies of each other,
and therefore, they  each contain the same numbers in random state.  
In Deep Learning we often do parallel processing, for example,  to generate lots of augmented images at the  
same time, using multiple processes.  
fastai used to have a bug, in fact,  where we failed to correctly initialize the random 
number generator separately, in each process.  
As of  October 2022, `torch.rand()` fails to initialize the random number generator. 

NumPy also does not get it right, but Python does remember to re-initialize the  random stream at each fork.
NB: Even if we've experimented in  Python and we think everything's working  well,
we then switch to Pytorch or NumPy and suddenly its broken. 
This is why we've spent some time re-implementing the random number generator from scratch.
 
`%timeit` is a special IPython function that runs a piece of Python code this many times. 
It will do 7 loops, and each one  will be seven times and it'll take the mean and  standard deviation. 
So here I am going to generate  random numbers, 7,840 times and put them into 10  
long chunks, and if I run that, it takes me three milliseconds per loop. 
If I run it using Pytorch it takes 73 microseconds per  loop. 
Wwe could use our version, but the Pytorch version is much, much faster. 

This is how we can  create a 784 by 10. And why would we want this?  
That's because this is a final layer of our neural  net, where if we're doing a linear classifier,   
our linear weights we need to be  784 —because that's 28 by 28— by 10,  
because that's the number of possible  outputs, the number of possible digits.