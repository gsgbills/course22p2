Lesson 11 2022 - Part One - Reading the DiffEdit paper.

First some of the amazing stuff from the forum this week.
A beautiful video from John Robinson, (Notebook TreeDifussion), very stable shows movement between seasons. 
JH suggested trying interpolating between prompts, and using the last image of the previous 
prompt interpolation as the initial image for the next prompt.  
Later John Robinson wrote that the tree video doesn't actually do what we think it does. 
There's a bug in his code, and despite the bug, it  accidentally worked really well. 
Now he is trying to figure out how did it create such a beautiful video by mistake? 
Reverse engineering what a bug did and then figuring out how to do that more intentionally. 
Bugs often tell us about new ideas. 

SCALING the text embeddings:
@sebderhy, Sebastian, noticed a problem of scale with that the update:  
unconditional embeddings plus guidance times text embeddings minus unconditional embeddings  
`[u + g * (t - u)]`, gets big. 
Lets have a couple of vectors, u and t, and then we add to that some amount of t-u. 
Draw the vector difference between those to see (t-u).
If there's a big difference between t and u, the eventual update g*(t-u) 
is far bigger than the original update, so it jumps too far. 
Scaling the update by the ratio of the norms can ensure that the update is no longer 
than the original unconditioned update would have been. 
This improves images: e.g., see astronaut subtle changes, 
more texture in the background, Earth detail, etc.
Some of the changes resulted in changing the image, e.g., the horse used to be missing a leg.

We started with `p = u + g * (t - u)` 
Next is to take that prediction `p` and scale it according to the difference in the lengths. 
The norms is the length of a vector. 
Looking at the second image in Lesson 9, from 1a to 1b, a boot now has texture, there are proper stars in the sky. 
It's made a big difference.  
The second change is not just to rescale the whole prediction, but to rescale the update. 
To rescale the update changes the image because it changes the direction it goes. 
Is this an improvement?  
This was the difference that  added the correct fourth leg to the horse before. 

Lets do both: rescale the difference and then rescale the result, to get the best of both worlds. 
We get a nice background, a "weird thing" become an arm, the foot improves, details make a difference.
JH shared on Twitter Sebastian's approach, then Ben Poole from Google Brain,
pointed out that this is the same as what's shown in the paper "Guided TTS: A Difussion Model for Text-to-Speech".  
JH hasn't read the paper yet to check whether it's got all the different options, etc.
Maybe this is reinventing something that already existed and putting it into a  new field.  
Folks on the forum help figure out whether this paper is showing the same thing or not. 
 
Rekil Prashanth: What if we take the guidance scale and rather than keeping it fixed at 7.5 all the time, let's reduce it. 
Once the model knows roughly what image it's trying to draw, even if it's noisy, let it do its thing.  
Let's decrease the guidance scale, so at the end, it's basically zero.  
Once it is going in the  right direction, we let it do its thing. 
So this little doggy is with  the normal 7.5 guidance scale. 
Its eye is pretty uninteresting, pretty flat. 
And the next one, has a proper eye. Before, totally glassy black, now proper eye.  
This fur very  textured, previously very out of focus. 
So this is a new technique.  

Check out Alex's notes showing how to study a lesson: 
https://mlops.systems/computervision/fastai/parttwo/2022/10/24/foundations-mnist-basics.html,
He made a list of the steps we did as we started From the Foundations. 
What is the library that it comes from, links to the documentation. 
Alex's background is history, this is a great idea, to be able to look at what are all the things that  
we have to learn and read about.
Also, he tried the lesson on a new data set, the Fashion MNIST data set, 
which we'll be using a lot, because it's a lot like MNIST, and different enough to be interesting. 
He described in his post or his notes, how he went about doing that. 
And he jotted down JH tips, a good way to make sure we don't forget about all the little tricks. 

Johno suggested an interesting new paper called DiffEdit, lets try reading this paper together, 
We get it from arXiv a preprint server, where papers have not been peer reviewed. 
We have code, we can try it, we can see things, whether it works or not, and it gets "peer review" on Twitter.  
If there's a problem soon somebody points it out. 
From arXiv we can save to Zotero, free software to manage readings, has a Chrome connector on the Chrome menu bar. 
After download the paper will automatically appear in Zotero with details: abstract, authors, URL, etc. 
Later we can go back and see the URL.  
Double click on it to bring up the paper. 
In Zotero we can annotate papers, edit them, tag them, put them in folders, and add them to 
a reading list directly from a web browser. 
JH started a diffusion folder as a group library shared with other folks working on the fast diffusion project.
Zotero is better than Mendeley.  
 
Reading a paper, the goal is to understand the basic idea well enough that, 
when we look at the code we can see how the code matches to it.
And try writing our own code to implement parts of it. 
So over on the left, you can  open up the sidebar here. 
Open up the table of  contents and get a bit of a sense of it.
There's some experimental and theoretical results, Introduction, Related work, what is DiffEdit, experiments. 
Standard approach to what we see in papers. 
Always start with the abstract, what's it saying this does? 
Generally a background sentence or two about how interesting this field is, e.g., "image generation is cool".
Then they're going to tell us what they're going to do, create something called DiffEdit. 
What is it for? It's going  to use text condition diffusion models, which we now know.
That's where we type in some text and get  back an image of that, that matches the text. 
But this is going to be different, the goal is to edit an  image based on a text query. 
We're going to edit an image based on text. 
How? With "Semantic image editing", an extension of image generation with an additional constraint, 
that the generated image should be as similar as possible to the given input. 
And there is an input image, originally attached to a caption "A bowl of fruits". 
We want to change it to "A bowl of pears".  
We type the new caption and it generates it.
Another example, to change it from a "bowl of fruit" to a "basket of fruit".
  
The idea is that we can edit an image by typing what we want that image to represent. 
Current ways of doing this require you to provide a mask, i.e., to draw the area to be replaced.  
The main contribution of this work is to automatically generate the mask. 
So they simply type in the new query and get the new image.  
After reading the abstract either skip the paper or look at the results.
Here it claims "Achieves state-of-the-art editing performance", the results look good, we keep going. 

The introduction gives a sense of what they're trying to do. 
The first paragraph repeats what is in the  abstract, and what we see in figure one.  
That we can take a text query, like a basket of fruits, see the examples. 
Academic papers are full of citations, do not expect to read all of them. 
Each of those citations has citations, etc.
Text conditional image generation is undergoing revolution.
Examples are: DALL-E, Latent Diffusion (what we've been using), Imagen.
This area that we're working on is important, we agree so we can skip. 
Diffusion models are interesting, they de-noise starting from Gaussian noise. 
Once you are familiar with the field, you can skip over pretty quickly. 
You can guide it using CLIP guidance.

New: Or by inpainting, by copy pasting pixel values outside a mask. 
That is a new technique that we haven't done.
During that diffusion process, if there are some pixels that we don't want to change 
(e.g., the ones that aren't orange in Fig 1),
we can just paste them from the original after each stage of the diffusion. 
To know more about that we can always look at this paper, but not for now. 
Repeating that those approaches require us to provide a mask. 
When we mask an area, it is a problem, for example, to change a dog into a cat,
you want to keep the animal's color and pose. 
This new technique is not deleting a section of the original and replacing it with something else.
It is going to use knowledge about what that thing looked like. 

At this point, we know what they're trying to achieve. 
If we don't know that when reading a paper, the paper won't make sense. 
That's a point to stop, i.e., this is not the right time to be reading this paper. 
Maybe we need to read some of the references, or look more at the examples.
Maybe we can skip straight to the experiments to better understand, don't always read it from top to bottom.
In this case, no need to because they've put enough experiments on the first page to see what it's doing.  

They've got some examples of conditioning a diffusion model on an input without a mask,
e.g., can use a noise version of the input as a starting point. (we've done that too.)
We've already covered techniques that they're referring to here. 
Something we haven't done is to look at the distance to the input image as a loss function. 
Makes sense and there are some references here.  

The next section is related work, to tell us about other approaches.
If we're doing a deep dive, this is a good thing to study carefully. 
We are not going to do a deep dive right now, so we can skip, do a quick glance... lots of interesting topics.  
For image editing there are different techniques, eg CLIP guidance. 
They can be computationally expensive. 
We can use diffusion for image editing. 
There's a lot of repetition in these papers so we can skip over it pretty quickly. 

They claim that this is not so computationally expensive.
Often at the end of the related work they talk about how somebody else has done “concurrent to ours”. 
Somebody else working at the same time looked at some different approach. 
To do the best possible thing we should study the related work and get the best ideas from each. 

Background is where it starts to "look scary". 
Mathematically, here's how the problem that we're trying to solve is set up. 
We start by looking at Denoising Diffusion Probabilistic Models, DDPM. 
Lesson 9B (Wasim and Tanishq) has some of the math of DDPM. 
Few people are going to look at these text and equations and "get it". 
To understand DDPM, we need to read and study the original paper, 
the papers it's  based on, watch videos and classes like this one. 
Once we understand DDPM, we look at this section and recognize
that they are talking about something we are already familiar with.
Background is meant to be a reminder of something that you already know,
not something we can expect to learn from this section.
Also, background is often written last and tries to look impressive for the reviewers. 

Lets go through these equations as every diffusion paper is going to have them.
We need to find out what the different letters/symbols mean, as they will refer to them later. 
We must know the Greek alphabet, it's much easier to read this. 
The curly L is used for the loss function. 

How do we find out what a math symbol means?
1. Use MathPix to select anything on the screen and it will turn it into LaTeX. 
LaTeX is written as actual stuff that we can search for on Google. 
2. Download the source format of the paper, then open up that LaTeX and have a look at it. 
3. Detexify is a less expensive way to find symbols you don't know. 
4. pix2tex to snip and convert to LaTeX, (instead of paying for Mathpix).

Let's try Detexify https://detexify.kirelabs.org/classify.html for expectation and epsilon.
We draw the thing, and it provides suggestions. 
If there are many options it is not particularly helpful. 

For the two bars, we could try a few things, eg., google looking for two  bars (“||”) with "math notation". 
Wikipedia has a glossary of mathematical symbols. 
A meaning of this in math, it is a norm or vector norm. 
Once we find the term we know what to look for. 
In our case, we've got this surrounding all this stuff, and then there are 2s. 
If we scroll we find that two bars can mean a matrix norm,  otherwise a single for a vector norm. 
And here's the definition, it's equal to root sum of squares, squared.
So the whole thing  is just the sum of squares. 
 
What about the weird E thing? 
On the LaTeX source we find it, it is \mathbb(E), the expected value operator. 
 
The expected value of something is the "likely" value. 
For a coin, heads or tails, we may want to know how often it is heads. 
Lets call heads 1, tails 0. 
We toss and get a sequence X of 1s and 0s. 
Then we can calculate the mean of X, the sum of all the elements divided by the count. 
But the expected value is what do we expect to happen.
We calculate it by adding up for all of the possibilities for each possibility x.  
How likely is x and what score do you get if you get x? 
In the coin toss example, there are 2 possibilities, either heads or tails.  
For the version where x is heads, the probability is 0.5, and the score, if it's an x, is 1.  
For tails the probability is 0.5, and the score for tails is zero. 
Overall the expected is 0.5 times one plus zero which is 0.5.
The expected score if we're tossing a coin is 0.5, if getting heads is a win. 

Another example is  we want to know  what the expected score is if we roll a (6 face) die. 
We can roll it a bunch of times, and compute the mean by adding all dividing it by the count. 
The expected value is the sum of all the probabilities (of each possibile outcome) times that score. 
The outcomes for rolling the die are 1 to 6, the probability of each one is a sixth. 
We multiply all these  together and sum them up, which would be 1/6+2/6+3/6+4/6+5/6+6/6.
And that gives the expected value of rolling a die. 

The paper is telling us what are all the things that we're averaging (the expectations) over. 
There are several symbols, which they define:
X0 is an input image, Epsilon is the noise, has a mean of 0  and a standard deviation of I. 
I it's like a standard deviation of one when doing multiple normal variables. 
Epsilon-theta is a noise estimator function. 
We can tell it's a function as it's got parentheses and stuff right next to it.  
Presumably most functions like this (in these papers) are neural networks. 

We are finally at a point where this makes sense. 
We've got the noise and the prediction of that noise. 
We subtract one from the other, square it and take the expected value. 
This is mean squared error, i.e., the loss function is MSE. 
Xt is the original unnoised image (X0) times some number plus some noise times one minus that number. 
This is where we reduce the value of each pixel and we add noise to each pixel.
The equations make sense, they are background, telling what a DDPM and what a DDIM are. 
DDIM is a more recent version of DDPM, with some minor changes to the way it's set up, 
which allows it to run 10-50x faster. 
Once we keep reading, we find that none of this background matters
For our purposes, it's enough to know that DDPM and DDIM  are foundational papers
on which diffusion models are based. 

DDIM encoding encodes an image onto a latent variable and then adds noise. 
What goes from the input image to the noise image, they call capital Er, where r is the encoding ratios,
something like how much noise are we adding. 
If we use small steps, then decoding that, going backwards back the original image. 
That's what diffusion models are. 

Fig 3. is very useful, it details the 3 steps of DiffEdit. 
Step 1: Add (Gaussian) noise to the input image X0, and then we estimate the noise twice.
One time we we estimate the noise using the reference text R: horse, or unconditional.
We do it once using the word horse, decode it, estimate the noise, 
and then we can remove that noise on the  assumption that it's a horse. 
Then a 2nd time, but we estimate the noise based on query Q, "zebra". 
Those are going to be very different noises, for a horse is the Gaussian pixels,
but if the claim is "zebra", then all of these pixels here are wrong, eg the wrong color.  
The noise that is calculated for "horse" is different to the noise estimated for "zebra". 
We take one minus the other, and get a normalized difference.  
This is “we derive a mask based on the difference in the denoising results”. 
Then take that difference and binarize it, i.e., turn that into ones and zeros. 

That is the key idea, once we have a diffusion model that's trained,
we can do inference on it where we tell it the truth about what it, 
and then do it again but lie about what it is. 
And in the lying version all the stuff that doesn't match zebra must be noise.  
The difference between the noise prediction when we say zebra versus when we say horse,
will be all the pixels that say no, these pixels are not zebra.
But the rest of it is fine, as there's nothing particular about the background that wouldn't work with a zebra. 

Step 2 is to Encode with DDIM until encoding ratio r.
We take the horse X0, and we add noise to it with a "Reverse DDIM step", until the Xr thing.
Step 3 is to “decoding conditioned on the text query” using the mask to replace the background with pixel values. 
During the inference time as we do diffusion from this fuzzy horse,  
we do a step of diffusion inference, and then all these black pixels, 
we replace with the noise version of the original. 
We do that multiple times, then the original pixels in the black area won't get changed,
so the ouput picture has the same background as the original input.  
The only thing that's changed is that the horse has been turned into a zebra. 

Paragraph 3.2 describes it in more detail, which often has little tips about things 
they tried  and things they found.
An interesting note is that the binarized mask,  (difference between the R decoding and the Q decoding),
tends to be a bit bigger than the actual area where the horse is, which we can see with these legs, for example.  
That is a good thing because often we want to slightly change some of the details around the object. 

And then there is the THEORETICAL ANALYSIS, which JH skips, stuff to get a paper past review with fancy math. 
It is proving the insight into why this component yields better editing results than other approaches. 
We don't particularly care about the proofs, because it is intuitive, we can see it works, 
don't need it proven so we skip it. 

EXPERIMENTS
Shows their experiments, what data sets and models they did the experiments on. 
They have metrics with names like LPIPS and CSFID, trying to  score how good their generated images. 
We come across FID a lot, and this is just a version of that.  
We don't normally care about that either. 
They care because they claim their paper has a higher number than others on this area.

Q: from Mikołaj: This technique only works for things that are relatively similar?
Yes, if we can't come up with a mask for the change we want is not going to work. 
For example, to change it from a "bowl of fruits" to "a bowl of fruits with a bokeh background", 
or to "a purple tinged photo of a bowl of fruit", is not going to work, because we're not masking off an area. 
The limitation of DiffEdit is for things where we change a bit and leave everything else the same. 

There are lots of experiments. 
For things that we care about the experiments, say classification for generation, we want to look at the results. 
For the results we often to zoom in to see whether they're good. 
Figure 5 has input images and edits with SDEdit and ablated models variants of DiffEdit.
First example wants to turn a given dog into an English Foxhound. 
SDEdit changes the composition quite a lot, DiffEdit only changed the dog. 
Ditto for all the other examples.
The authors are showing off what they're good at, what this technique is effective at doing.

Then there's a short conclusion at the end, which rarely adds anything. 
Often the appendices are interesting, so don't skip over them. 
Will often find more examples, some examples that didn't work very well, etc.
Sometimes the most interesting examples are there.  

And that's it, our first full on paper walkthrough. 
Not a carefully chosen paper that we've picked  specifically because you can handle it. 
Just the most interesting  paper that came out this week.  

It gives a sense of what it's really like. 
If you are ready to try something that's going to stretch you, see if you can implement any of this paper. 
There are three steps. 
The first step is the most interesting one, which is to automatically generate a mask. 
The information that you have and the code in the Lesson 9 notebook contains everything you need to do it.   
Maybe if you can mask out the area of a horse that does not look like a zebra. 
And that's actually useful in itself, allows to create segmentation masks automatically. 
And then try step two, and step three.   