Lesson 13, back propagation: notebook 03_ in the repo course22p2.  
See also [Simple Neural Net Backward Pass - Deriving the math of the backward pass for a simple neural net](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html)
The forward and backward passes of a simple Multi-Layer Perceptron, a neural network. 
First import and settings from previous notebooks, then load the MNIST data as tensors. 

BACKGROUND REVIEW on DISPLAY
Lets create a basic architecture for a neural network, a linear  model, the simplest example possible.
We're gonna pick a single pixel `x` from MNIST pictures. 
And for our Y values is how likely is the image a specific digit, e.g., "3", 
based on the value of this one pixel `x`. 
Y is the probability of the image being the number "3".
If we just have a linear model,  then it's gonna look like this,
saying that the brighter this pixel is, the more likely it is a "3". 
The first problem is that a linear model is very limiting.
We actually are trying to draw something that looks more like a curve. 
 
If we add additional  lines, the sum of the two lines will itself be a line,
so it's not gonna help us match the actual curve that we want.
Instead, we  can create a line using line segments.
For example, everything to the  left of this point is not changed  
if I add these two lines together, because this  is zero all the way, and everything to the right 
of it is going to be reduced. 
It looks like  they've got similar slopes. So we might end up with, instead, so this would all disappear here.  
And instead, we would end up with something like  this. 
And then we could do that again, right?  We could add an additional line that looks a  
bit like that. So it would go, but this time it  could go even further out here. And it could be  
something like this, see. So what if we added that? Well, again, at  the point underneath here, it's always zero,  
so it won't do anything at all. But after that,  it's going to make it even more negatively sloped.  
And if you can see, using this approach, we could  add up lots of these rectified lines, these lines  
that truncate at zero, and we could create any  shape we want with enough of them. And these  
lines are very easy to create because actually all  we need to do is to create just a regular line,  
just create a regular line, right?  Which we can move up, down, left, right,  change its angle, whatever. 
And then just say, if it's greater than zero,  truncate it to zero. Or we could do the opposite  
for a line going the opposite direction. If it's  less than zero, we could say truncate it at zero.  
And that would get rid of, as we want,  this whole section here, and make it flat.
Okay, so these are rectified lines.   And so we can sum up a bunch of these together  to basically match any arbitrary curve.  
So let's start by doing that. Oh,  the other thing we should mention,   
of course, is that we're going  to have not just one pixel,  
but we're going to have lots of  pixels. So to start with the, kind of,  
most, you know, slightly, the only  slightly less simple approach,  
we could have something where we've got, you  know, pixel number one and pixel number two.   
We're looking at two different pixels to see  how likely they are to be the number three.  
And so, that would allow us to draw  more complex shapes that have  some kind of surface between them.
Then we can do exactly the same thing, is to create these surfaces, we can add  up lots of these rectified lines together,  
but now they're going to be kind of rectified  planes. 
But it's the same  thing. We're going to be adding together a bunch  of lines, each one of which is truncated at zero.
END OF REVIEW
  
Lets start by defining a few variables.  
`n` is the number of training examples, `m` is the number of pixels, `c` is the number of possible values of our digits.
Here there are 50000 samples, 784 pixels and 10 possible outputs.  
Now we decide (ahead of time) how many of these line segments to add up. 
The number that we create in a layer is called the number of hidden nodes or activations, `nh`.
Lets arbitrarily decide `nh=50`.  
To create lots of lines, which we're then going to truncate at zero,  we can do a matrix multiplication.  
We're going to have 50000 rows by 784 columns to multiply by a 784 rows and 10 columns.

Because if we take  this very first line of this first vector we have 784 pixel values of the first image. 
We multiply it by each of the 784 values in the first column, the zero index column. 
And that's going to give us a number in our output that is going to be images by 10 labels.
We'll multiply those together and we'll add them up. 
And that result is going to end up over here in this  first cell. 
Each of these columns, it's going to eventually represent a probability.
The first column will be the  probability of being a 0, the second column of a 1, etc.
That's why we're going to have these 10  columns, each one allowing us to weight the  784 inputs. 

Now we're going to have the 784x50 input going into a 784x50 output to create the 50 hidden layers. 
Then we truncate those at zero and then multiply that by a 50x10 to create our 10 output.  
We do it in two steps.  
The way SGD works is that the weight matrix W is initially filled with 50000x50 random values.  
For a linear function is not enough just to multiply we  also have to add. 
The biases are the things we add, and we start those at zeros. 
We need one bias for each output, ie 50.
That will be layer one.

Layer two will be a matrix that goes from 50 hidden.
To simplify some of the calculations for the calculus we only create one output. 
That's because we are not going to use cross entropy yet, instead, we are going to use MSE.
The one output will be a predictor what digit do I think it is from 0 to 9.
We compare those predictors (with little ^) to the actual labels. 
Let's say we predict 9 and the actual is 2, and we'll compare those together using MSE,  
which will be a stupid way to do it because it's saying that 9 is further away from being 2 than 2.
9 is further away from 2 than it is from 4 in terms of how correct it is, which is not what we want at all.
But this is what we're doing just to simplify our starting point. 
So that's why we're going to have a single  output for this weight matrix and a single output for this bias. 

Let's  create a function for putting x through a linear layer with these weights and these biases. 
It's a matrix multiply and an add.
If we put x_valid through our weights and biases with a linear layer, we end up with a 10,000 by 50. 
So 10,000, 50 long hidden activations, not ready yet because  we have to put them through ReLU.
Clamp at zero So  everything under zero will become zero. 

And so here's what it looks like when we go through the linear layer and then the ReLU.   
And you can see here's a tensor with a bunch of things, some of which is zero or they're positive.  
And so that's the result of  this matrix multiplication. 
To create our basic MLP, multilayer  perceptron from scratch, we will take our  
mini batch of x's, xb is a x batch. 
We create our first layer's output with a linear, and then we will put that through a ReLU,  
and then that will go through the second linear. 
So the first one uses the w1, b1, and the second one uses the w2, b2.  
And so we've now got a simple model. 
And as  we hoped, when we pass in the validation set, we get back 10,000 digits, so 10,000  by 1.
Let's use our ridiculous loss  function of MSE. 
So, our results is 10,000 by 1  and our y_valid is just  a vector. 
Now what's gonna happen if we do `res - y_valid`? 
So before you continue in  the video, have a think about that. 
What's gonna happen if I do res minus y_valid by thinking about the NumPy broadcasting rules we've learned?
We've ended up  with a 10,000 by 10,000 matrix, 100 million  points. 
Now we would expect an MSE to just contain 1,000 points.
The reason it happened is because we have to start out at the last dimension and go right to left. 
And we compare the  10,000 to the 1 and say, are they compatible?  
By broadcasting rules, the answer is that this one will be broadcast over these 10,000.  
So this pair here will give us 10,000 outputs.
And then we'll move to the next one.  
Oh, oh!, there is no next one. What happens?  
Now, if you remember the rules, it inserts a unit  axis for us, so we now have 10,000 by 1. 
That  means each of the 10,000 outputs from here will end up being broadcast across the 10,000 rows here. 
For each  of those 10,000, we'll have another 10,000, a 10,000 by 10,000 output, not what we want. 
So how could we fix that?
We really want this to be 10,000, 1 here. 
If that was 10,000, 1, then we'd  compare these two, right to left, and they're  both 1. 
So those match, and there's nothing to  broadcast because they're the same. And then  
we'll go to the next one, 10,000 to 10,000, those  match. So they just go element wise for those. 
And we'd end up with exactly what we want: 10,000 results. 
Alternatively, we could remove this trailing dimension. 
There's a couple of ways to do it.
One is to grab every row in the zeroth column of res, that's gonna turn it from a 10,000 by 1 into a 10,000. 
Alternatively, we can use .squeeze(). which removes all unit dimensions.
We do the subtraction, now we get 10,000, just like we wanted. 
Let's get our training and validation wise. 
We make them into floats because we're using MSE.   

Let's calculate our predictions for the  training set, which is 50,000 by 1. 
We define an MSE function that does the subtraction and then squares it and then takes the mean.
We now have a loss function being applied to our training set.

Now we need gradients, which are slopes. 
As we increase time a unit, the distance increases by what the slope is.  
A neural network is a function that takes two groups of things: inputs and weights.
We then take the result of that and we put it through some loss function, 
get the predictions and compare it to our actual  dependent variable. 
Given the derivative of the loss with respect to one particular weight, weight number zero, what is that doing? 
As I increase the weight by a little bit, that would make the loss go down.
Then obviously we want to increase the  weight by a little bit. 
And if it makes the loss go up, we want to do the opposite.
The derivative of the loss with respect to (each one of the) weights, tells us how to change  the weights. 
We then change each weight by that derivative times a little bit  (LR) and subtract it from the original weights. 
We do that a bunch of times, and that's called SGD.

In this case there's a single input and a single output, so the derivative is a single number at any point.
Like car speed. 
But consider a more complex function, with one output, but there's two inputs.  
To take the  derivative of this function,  then we must consider what happens to the output
if we increase either of the inputs.   
In that case, the derivative has two numbers, i.e., the derivative of the output Z with respect to each input.
What happens if we change each of these two numbers? 
These could be two different weights in our neural  network, and Z could be our loss.  
We've got 784 inputs so we need 784 of these derivatives. 
We don't normally write them all like that, we use the ∂ symbol to indicate the derivative 
of the loss across all of them with respect to all of the weights. (It's a shorthand way of writing this.)

It gets more complicated if, for example, in the first layer where we've got a weight matrix that's going to
give us 50 outputs.
So for every image we're going to have 784 inputs and 50 outputs to our function.
Then as we increase the first input, how does that change both of the two outputs? 
Ditto for all other input.
Hence we end up with a matrix of derivatives, for every input that we how much does it change every output of that function? 
We're going to calculate these derivatives,  but rather than being single numbers,
they're matrices with a row for every input and a column for every output. 
A single cell in that matrix will tell us as if I change an input by a little bit, how does it change this output?

Eventually, we end up with a single number for every input, because our loss is going to be a single number.
This is a SGD requirement: the loss has to be a  single number. 
We get a single loss number by doing some operation, eg the sum or a mean.
On the way there we're going to deal with these matrix of derivatives.

JH and Terrence Parr wrote a paper "The Matrix Calculus ..."
that only assumes high school calculus, and describes matrix calculus, with many examples. 
A Jacobian matrix is a matrix of derivatives.
We need to be able to calculate derivatives at least of a single variable.
A computer can do that: using SimPy.
For example, we define two symbols X and Y, we tell it to differentiate X squared with respect to X, 
and SimPy will answer is 2X.
If you say differentiate 3X² + 9 with respect  to X, SimPy will tell you that's 6X. 
Wolfram Alpha does something similar. 
With SimPy we can do it inside a notebook and include it in prose. 

The CHAIN RULE

The derivative of 3X² + 9  equals 6X.
The derivative of A to the B with respect to A equals B times A. 
The derivative of X squared with respect to X equals 2X. 
For 3X² + 9 we can rewrite this as y = 3u + 9, and then write u = X². 
The derivative of two things being added together is simply the sum of their derivatives.
The derivative of any constant with respect to a variable is 0 because if I change something,  
an input, it doesn't change the constant.  
It's always 9. So that's going to end up as 0.  
We end up with dY/dU equals something plus 0.  
And the derivative of 3U with respect to U is 3 because it's just a line, that's its slope.
We want to  dY/dX....  and dY/dX  is equal to dY/dU dU/dX.  
dU/dX is 2X, so we can multiply these, 2X times 3, is 6X, which is what SimPy told us.

This is the chain rule. 
INTUITIVE ANIMATION
To understand it intuitively see an interactive animation at  
webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html
We've got a wheel spinning around, and each time it spins around, this is X going up. 
There's some change in X, dX over a period of time. 
This wheel is eight times bigger than this wheel. 
So each time this goes round once, if we connect the two together, this wheel would be going four times faster, 
because the difference between, the multiple between 8  and 2 is 4. 
Maybe I'll bring this up to here.
So now that this wheel has got twice as big a  circumference as the u-Wheel, each time this  
goes round once, this is going round two times.  
Each time X goes round once, the change in U will be 2. 
So that's what dU/dX is saying: The change in U for each change in X is 2.  
Now, we could make this interesting by  connecting this wheel to this wheel.
Now, this wheel is twice as small as this wheel.  
Each time this  spins round once, this spins round twice,  
because this has twice the circumference  of this, so therefore, dY/dU equals 2.
But now that means every time this goes round once, this goes round twice, 
every time this one goes round once, this gun goes  round twice. 
Every time this one goes round once, this one goes round four times, dY/dX equals 4. 
The dU/dX has to be multiplied with the  dY/dU to get the total. 
So this is what's going on in the chain rule. 
This is what you  want to be thinking about, is this idea that we've got one function that is kind of intermediary.   
And so you have to multiply the two impacts to  get the impact of the X wheel on the Y wheel.  
So I hope you find that useful. I find this,  personally, I find this intuition quite useful.
END OF ANIMATION

We want to calculate the gradient of our MSE applied to our model. 
The inputs are going through a linear, a ReLU, another linear, and then an MSE. 
So there's four different steps going on. 
We combine those all together with the chain rule.
It’s going  to be a function of the ReLU activations.  
And the ReLU activations are a function of the first layer.
And the first layer is a function of the inputs. 
And it also has weights and biases, and we're going to calculate the derivative of that.  
Now this is itself a  function, so we'll need to multiply that derivative by the derivative of that. 
But that's also a function, so we have to multiply that derivative by this. and so forth.

BACKPROPAGATION
Our approach is to start at the end, take its derivative, and gradually keep multiplying as we go each step through. 
This is back propagation, using the chain rule and taking advantage of a computational trick 
of memorizing  some things on the way.
So here's going to do a forward pass and a backward pass. 

The forward pass is where we calculate the loss. 
The loss is the output of the neural net minus our target squared and then take the mean. 
And the output is going to be the output of the second linear layer.  
The second linear layer's input will be  the ReLU. 
The ReLU's input will be the first layer. 
So we're going to take our  input, put it through a linear layer,  
put that through a ReLU, put that through  a linear layer and calculate the MSE.

The backward pass, we store the gradients of each layer (e.g., loss with respect to inputs), in the layer itself.
We define a new attribute, `g`. 
We define a new layer,`out` with a new attribute called out.g, to contain the gradients. 
Here the derivative is two times the difference, because we've got difference squared.  
We took the mean when computing the loss, so we have to do  the same thing here, i.e., divided by the input shape.  
And so that's those gradients. 
And now we need to multiply by the gradients of the previous layer, `lin(l2, w2, b2)`.
What are the gradients of a linear layer? 
`lin_grad` defines it.  
Per the chain rule, we need to the weights and the biases of the layer, 
and also the input to and the output from the linear layer.

We store the gradients of our input.
This would be the gradients of our output with respect to the input. 
A matrix multiplier is just a whole bunch of linear  functions. 
So each one slope is just its weight.  
But we have to multiply it by the gradient  of the outputs because of the chain rule.  
The gradient of the outputs with respect to the weights is going to be the input times the output summed up.
I'll talk more about that in a moment. 
The  derivatives of the bias is the gradients of the output added together because the bias is just a constant value. 
And so for the chain rule, we simply  just use output times one, which is output.  
So for this one here, again, we have to  do the same thing we've been doing before,  
which is multiply by the output gradients because of the chain rule. 
And then we've got the input weights. So every single one  of those has to be multiplied by the outputs.  
And so that's why we have to  do an unsqueeze minus one. 

iPDB
Lets experiment with this code in order to understand it.
It is harder to do this cell by cell because we want to put it all into this function. 
We need a way to explore the calculations interactively. 
And the  way we do that is by using the Python debugger, pdb.
If we add `pdb.set_trace()` it tells the debugger to set a breakpoint to stop  execution when it reaches this line. 
If we call `forward_and_backward`,  it stops at the breakpoint and the interactive Python debugger, 
ipdb, has popped up an arrow pointing at the line of code it's about to run.
At this point, there's a whole range of things  we can do to find out what they are.
We hit h for help. 
Understanding how to  use the Python debugger is important. 
A most useful command is to print something. 
Single letters indicate shortcuts, eg just `p` instead of typing `print`.   
Let's take a look at the shape of the input: > p inp.shape.  
So I've got a 50,000 by 50 input to the  last layer. So that makes sense. 
These  are the hidden activations coming into the  last layer for every one of our images.
What about the output gradients?  > p out.g.shape
And there's that as well.  
You don't have to use the p at all if your variable name is not the same as any of these commands.  
So we could have just typed > out.g.shape.  

We can also put in expressions.  
Let's have a look at the shape of this (inp.unsqueeze...  )  
So the output of this is, let's see if it makes sense. 
We've got the input, 50,000 by 50. 
We put a new axis on the end, unsqueeze(-1) is the same  as doing dot, as indexing it with [..., None].
Let's put a new axis at the end. 
So that would have become 50,000 by 50 by 1.  
And then the out.g.unsqueeze() we're putting in  the first dimension. 
So we're going to have 50,000  by 50 by 1 times 50,000 by 1 by 1. 
We're  only going to end up getting this broadcasting  happening over these last two dimensions, 
which is why we end up with 50,000 by 50 by 1.  
And then with summing up over all of the inputs, as each image is individually contributing to the derivative. 
So we want to add them all up to find their total impact, because  
the derivative of a sum of functions is the sum of the derivatives of the functions. 
So we can just sum them up.

Now, this is one of these situations where if you see a times and a sum and an unsqueeze, 
think about Einstein summation notation as a way to simplify this.  
So first of all, let's just see how we can do some more stuff in the debugger. 
Now press c for continue, and it keeps running until it comes back again to the same spot. 
And the reason we've come to the same spot twice is because lin_grad() is called two times.
So we would expect that the second time we're going to get a different bunch of inputs and outputs. 
We can print out a  tuple of the inputs and output gradient. 
This is the first layer going into the second layer.
So that's exactly what we'd expect.

To find out what called this function, type w. 
w is where am I? And so  you can see here where am I? 
 ----> forward_and_backward 
indicates who called  lin_grad() the  second time.
Now we're in the line 
----> w.g = (inp.unsquueze....

If we want to find out what w.g ends up being equal to, 
I can press n to say go to the next line, so we moved from line 5 to line 6.
The instruction point is now looking at line six. 
We can now print out, for example, w.g.shape,  and there's the shape of our weights.

NB: In Python we can use breakpoint instead of import pdb business. 
Unfortunately, the breakpoint  keyword doesn't currently work in Jupyter or in  IPython, 
so we actually can't, sadly. That's why  we do it the old-fashioned way.  
maybe they'll fix the bug at some point,  but for now, we have to type all this.

So if I just press continue again,  it keeps running all the way to the end, and it's  
now finished running forward and backward.  
So when it's finished, we can return to a Jupyter cell and find that there will now be, for example, 
a w1.g because this is the gradient that it just calculated, and also a x_train.g and so forth.

Lets simplify this by taking the input and output expressions giving them their own variable names, i and o.
Better to do it before debugging so it is easier to type.  
We get rid of the breakpoint and double check that we've got our gradients.
Before we rerun it, we  should set the gradients to zero with `x_train.g.zero_()`
To try things at the breakpoint inside `lin_grad`.   
The `w.g = (i*o).sum(0)` line is doing the same thing as an einsum, so lets try it.
We are multiplying the first dimension of each and then summing over that dimension. 
We try `torch.einsum('ij,ik->jk`,inp,out.g`), and it works. 

We've multiplied the repeating index `i`, the first dimensions together,
and then summing over them, so there's no `i` in the output.
That is not the same thing as a matrix multiplication, 
but we can turn it into the same thing as a matrix multiplication 
by swapping i and j so that they're the other way around, i.e., ‘ji, ik->jk’. 
And we can swap the dimensions with the transpose, and then it would become a matrix multiplication. 
In NumPy the  transpose is the capital T attribute. 
So `inp.T@out.g` is the same thing using a matrix  multiply and a transpose.
We've checked in the debugger that we can actually replace  `w.g = (i*o).sum(0)` by `inp.T@out.g`. 
Pdb is a handy tool for playing with numeric programming ideas or coding in general.  

Break. 
So we've  calculated our derivatives and we want to test them, by comparing them
with the same derivatives calculated by PyTorch.
(We will later do this from scratch). 
We just run it all through PyTorch and check that their derivatives are the same.

Lets simplify things with some refactoring, and define a class for ReLU and for the linear function. 
We do this by defining a Dunder `__call__`. 
We define a class just to print hello.
We create an instance of that class and then we can call it as if it was a function.
In Python we can change how a class behaves, make it work like a function, by defining `__call__. 
It is a syntactic sugar to treat a class as if it's a function without any method  at all. 
We can still do it the method way, but why do that?

Lets define the ReLU class and add `__call__` so we can treat it as a function. 
It's going to take its input and do the ReLU on it. 
There's something interesting about the backward pass:
it has to know about the intermediate calculation gets passed over here.
It is needed because of the chain rule, and because of how the derivatives are calculated. 
We need to store each of the layer intermediate calculations. 
The ReLU class stores its output, and its input. 
Then, when we call the `backward` method, we know how to calculate that. 
We set the inputs  gradient, `self.inp.g` to be, by the chain rule the product of 2 derivatives, A * B.
A is `(self.inp>0).float(), the derivative of a ReLU, and B is self.out.g.

That is how we can calculate the forward pass and the backward pass for ReLU, 
and avoid storing all the intermediate stuff in `backwards_and_forward` separately.
It's going to happen automatically. 

Lets do the same thing for a linear layer. 
A linear layer needs some additional state, weights and  biases. (ReLU doesn't). 
When we define a linear layer `Lin` we indicate its weights and biases, and store them.
When we call it on the forward pass we store the input. 
We calculate the output, store it and return it.
For the backward pass, the input gradients we calculate just like before. 
`.t()` is the same with a little `t` as big `T` is as a property: the transpose.  
Calculate the gradients of the weights with a chain rule, 
and the gradient of the bias, just like before.
And they are  all being stored in the appropriate `.g` places.

For MSE we do the same thing we calculate it and store it in `self.out`. 
MSE needs input and target, so we store them. 
In the backward pass we can calculate its gradient of the input as being two times the difference. 

Th model is much easier to define.  
We define a bunch of layers, linear, w1,  b1, ReLU, linear, w2, b2. 
And we can store  an instance of the MSE. (not calling Mse, just creating an instance of the Mse class). 
These are instance of the Lin class and of the Relu class. 
They're just being stored, so when we call the model,  we pass it our inputs and our target. 
We go through each layer, set x equal to the result  of calling that layer, and then pass that  to the loss. 

Notice that we don't have two separate functions, the loss function being applied to a separate neural net.  
Rather we integrated the loss function into the model, i.e., the loss is calculated inside the model.
That's neither better nor worse than having it separately just different.   
A lot of HuggingFace stuff does it this way, put the loss inside the forward.
Fastai and other libraries does it separately, i.e., loss is a whole separate function, 
and the model only returns the result of putting it through the layers.
For this model the loss function is inside the model.

For backward, `self.loss` is the Mse() object. 
So that's going to call backward, and it's stored when it was called here.  
It stores  the inputs, the targets, the outputs, so it can calculate the backward().  
And then we go through each layer in reverse.
This is back propagation, backwards reversed, calling backward on each one. 
Now we can calculate the  model,  calculate the loss, call backward. 
And then we can check that each of the gradients that we stored earlier are equal to each of our new gradients.

Q: if we put the loss inside here how do you actually get predictions?  
A: HuggingFace models do it: 
They say self.preds equals x, and then they'll say self.final_loss equals that. 
And then  return self.final_loss. 
And that way you don't even need that last bit. 
That is what they do, we'll leave it there. 
That way you can  kind of check, like, model.preds, for example.
Then return not just the loss, but both as a dictionary, i.e., {loss=..., preds=...}, something like that. 
Anyway, there's a few different ways to do it.

Every class now can be separately considered and can be combined however we want, 
we can create layers, a bigger neural net, etc.
 
Repeated code,  e.g., `self.inp =inp`, `self.inp=inp`, etc., is a sign you can refactor things.  
We can define a new class called `Module()` to do those things that are repeated. 
It's going to store the inputs, call `self.forward` to create  the `self.out`, and then return it. 
There's going to be a method `forward` which in this class it doesn't do anything,
as the purpose of this Module is to be inherited.

When we call backward, it's going to call `self.bwd` passing in 2 arguments:
1. `self.out` because all `backwards()` wanted to get `self.out` because of the chain rule. 
2. The arguments that we stored earlier. 
`*` in a signature (e.g., `def __call__(self, *args)`
means take all of the arguments, regardless of number, and put them into a list. 
When we call a function using `*`, e.g., `self.bwd(self.out, *self.args) 
it says take this list and expand them into separate arguments,
and pass them (e.g. to `self.bwd`)  each one separately.

Relu is much simpler. The previous version had to store stuff manually, etc.
Now we can get rid of all of that and just implement forward and backward.
Relu's forward just does the one thing we want, with more understandable code. 
Ditto for  backward, just does the one thing we want.  
We still have to multiply  it by the chain rule manually.  
Same thing for linear `Lin` and for `Mse`. 

There are often  opportunities to manually speed things by defining custom autograd functions in PyTorch.  
For example, in 'Mse` a calculation  `inp.squeeze() - targ` is being done twice.
At the cost of some memory, we could instead store that calculation as, e.g., `self.diff`.
And at the cost of that memory, we could now remove this redundant calculation 
because we've done it once before already and  stored it and just use it directly. 
This is something that you can often do in neural nets, 
a compromise between memory use and then the computational speedup of not having to recalculate it.  
   
Now we can call it in the same way, create the model, passing in all of those layers. 
The model  hasn't changed at this point. 
The definition was up here, we just pass in the weights for the layers,
calculate the loss, call backward, and it's the same.

Thankfully PyTorch has  written all this for us, and since we've reimplemented it,
we're allowed to use PyTorch's version.  
PyTorch calls their version nn.Module.  

To create a linear layer rather than inheriting from our Module, we will inherit from `nn.Module`.
Here rather than passing in the already randomized weights, we're  generate the random weights and the zeroed biases.
And then here's our linear  layer, 
We define forward but we don't need to define backward.
PyTorch already knows the derivatives of all the functions in PyTorch, and how to use the chain rule. 
So we don't have to do the backward, Pytorch will do it. 

Let's define a model that uses nn.Module, it's the same as before.  
Now we use PyTorch's `mse_loss()` (because we've already implemented ourselves).
It's very common to use `torch.nn.functional` as `F`.  
Many handy functions live there including mse_loss(). 
We know why we need the colon, `[:,None]` as we saw the problem if we don't have it. 

We create the model, call backward.
We stored our gradients in `.g`, PyTorch stores them in `.grad`.
The same values.

So let's take stock of where we're up to.  
We've created a matrix multiplication from scratch. 
We've created linear layers. 
We've  created a complete backprop system of modules we  
can now calculate both the forward pass and the  backward pass for linear layers and values so  
we can create a multilayer perceptron. 
So we're  now up to a point where we can train a model.

ZZZZZZZZZZZ
Lets train a model notebook 04G_minibatch_training.  
First few cells just as before, just rerunning all.

Lets improve the loss function so it's not "total rubbish" anymore. 

REVIEW of Cross Entropy from part1 excel
In Part 1, there is an Excel notebook which is an entropy example.
Rather than outputting a single number for each image, we're going to output ten probability numbers for each image. 
And the targets will be one-hot encoded. 
We don't even need to actually do the one-hot encoding, thanks to some tricks. 
We can just  directly store the integer, but we can treat it as if it's one-hot encoded. 
So we can just  store the actual target zero as an integer.
Lets say, for example, for a single output, it could be cat, dog, plane, fish, building.  
The neural net spits out a bunch of outputs.  
For softmax we go e to the power of each of those outputs, sum up all of those. 
And then we divide each one by the sum. 
That gives us our softmaxs.  

For the loss function, we compare those softmaxs to the one-hot encoded version.
Let's say it was a dog, then it has a 1 for dog and 0 everywhere else.  
And then softmax, is the calculation sum of the ones and  zeros. 
So each of the ones and zeros multiplied  by the log of the probabilities. 
So here is the  log probability times the actuals. 
And since the actuals are either 0 or 1 and only one of them is going to be a 1, 
we end up with one value here.   
And so if we add the up, it's  all 0 except for one of them. 
So that's cross entropy. 
So in this special  case where the output is one-hot encoded,  
then doing the one-hot encoded multiplied by the log softmax is identical to simply saying, 
dog is in this row, let's just look it up directly and take its  log softmax. 
To just index directly into it is the same thing.
That is just review. 

So here's our softmax calculation.  
It's e to the power of each output divided by the sum of them, or we can use sigma notation to say the same thing.  

LATEX
Jupyter lets us use LaTeX: just put "$" signs around the equations, backslash "\" is for functions, "{}" for arguments.
e^x  is e to the power of x, underscore "_" is used for a subscript. 
So this is x subscript i, and power of is used for superscripts, \cdots are dots, etc.

Cross-entropy
In cross-entropy, we don't want softmax, we want log of softmax:
`(x.exp()/(x.exp().sum(-1,keepdim=True))).log()` 
So we've got `x.exp()`, divided by `x.exp().sum(-1)`, to sum up over the last dimension.  
And we want to keep that dimension `keepdim=True` so that when we do the divided by, 
we want a trailing unit axis for the same reason as when we did our MSE loss function. 
A `sum()` with `keepdim=True` leaves a unit axis in that last position,
so we don't have to put it back to avoid the broadcasting product issue.
And that's log of softmax with the predictions.  

Log and exponent rules:
log(a/b) = log(a) - log(b)
log(a * b) = log(a) + log(b)
These are very useful, for  example, division can take a long time, 
multiply can create big numbers that have floating point errors, etc.
Being able to replace multiplications/divisions with pluses and minuses is very useful. 

JH used to give an interview question:
SQL only has a sum function for group  by clauses, "how to deal with calculating a compound interest column?".
Because the compound interest is taking products, the answer is exp(sum(log(x))) where x is the column.  

We're going to take advantage of that because we've got a divided by that's being logged.  
Also we're going to have, the log of x.exp() minus the log of this.
But exp and log are inverse functions, so going to end up just being x minus.
So log(softmax) is just x minus all this logged.  
See the simplified version. 

LogSumExp
Problem: we've got x.exp.sum(), and x could be big numbers, and exp(x) is going to be really big numbers.   
There is less precision in the computer's floating point handling the further we get away from zero. 
We don't want big numbers, particularly because we're going to be taking  derivatives. 
When we are in an area that's not very floating point precise, then the derivatives are going to be a "disaster". 
They might even be zero because we may have two numbers that the computer can't recognize as different. 
Instead, we can calculate `a=max(xi)`. 
Rather than doing the log of the sum of exp(xi), 
we define `a`, our biggest number, as being the the maximum of all of our x values.
If we then subtract `a` from every number, then none of the numbers are going to be that big,
because we've subtracted it from all of them. 
The  problem is that's given us a different result. 
But let's expand this sum. 
It's exp(x1) + exp(x2) + ....
We subtracted a from our exponents, which has meant we're now wrong. 
By exponent rules, $x^{a+b} = x^a \times x^b$,  $x^{a-b} = x^a \divide x^b$
We can take advantage of this is equal to e^x1 over  e^{a + x2} ... over e^a. 
This is a common  denominator, so we can put all that together, e^a. 
If we now multiply that all by e^a,  these would cancel out and we get the thing we originally wanted.   
But this is no longer  going to be a giant number. 
We're doing extra calculations, it's  not a simplification, it's a complexification.  
But it's one that's going to make it  easier for our floating point unit. 

So that's the trick, rather than doing log of the sum, 
we do is log(e^a) \times the sum of e to the x minus  a. 
Since it is the log of a product, that is the sum of the logs, and log(e^a)  is just a. 
So it's a plus that. 
So this is  called the log sum exp trick.

We find the maximum on the last dimension, and then here is the m plus that exact thing.
That's just another way of doing that. 
So that's the logsumexp().
Now we can rewrite `log_softmax()` as `x-logsumexp()`, and we're going to use PyTorch's version. 
We check, and here's our results. 

The cross  entropy loss is the sum of the outputs times the log probabilities. 
Our outputs are just the numbers, the integer indices.
So we can simply rewrite that as negative log of the target. (That's what we have in Excel.)
In PyTorch (and Numpy) we use array indexing.
Here is the first three actual values  in y_train: 5, 0, and 4.  
We want to find in our softmax predictions:
the 5th prediction in the zeroth row, the 0 prediction in the first row, and the 4th prediction in the index two row.

This is going to be what we add up for the first two rows of our loss function. 
So how do we  do that in all in one go? A cool trick. 
See here I've got 0, 1, 2. 
If we index using two lists, we can in the first index `[0, 1, 2]`, 
and for the second index list we put `y_train[:3]`.
So the indeces are going to be `[0,5]`, `[1,0]`, and `[2,4]`,  which is the same thing. 

Therefore, this is giving us what we need for the cross entropy loss. 
So if we take range of our target's  first dimension, or zero index dimension,  
which is all this is, and the target, and  then take the negative of that `.mean`,  
that gives us our cross entropy loss.

PyTorch calls this negative log likelihood `nll` loss.
If we take the `nll`  and we pass that to the log soft max, then we get the loss. 
This combination in PyTorch is called F.cross_entropy(). 
We check that `F.cross_entropy()` gives us the same thing.  

We have now reimplemented the cross entropy loss, and there's a lot of confusing things going  on there, a lot. 
This is one of those  places where you should pause the video and go  
back and look at each step and think not just  like what is it doing, but why is it doing it,  
and also try typing in lots of different values  yourself to see if you can see what's going on,  
and then put this aside and test  yourself by reimplementing log_softmax()  
and nll_loss() and cross_entropy() yourself  and compare them to PyTorch's values.
And so that's a piece of  homework for you for this week.  

Training Loop
Now we can create a training loop.
Let's set our loss function to be cross entropy, define a batch size of 64. 
xb is the first mini batch,from 0 up to 64 from the training set. 
We can now calculate the predictions, 
a 64 by 10 tensor, for each of the 64 images in the minibatch, we have 10 probabilities, one for each digit.
Ys are the first 64 target values, the actual digits. 

We start with a bad loss because it's entirely random at this point.  
What did we predict? 
For each one of the 64 rows, we have to go in and see what is the highest prediction number,
we've to find the index of the highest number.
The function to find the index of the highest  number is `argmax()`. 
 
We want to calculate accuracy as a metric for understanding. 
So we take the argmax, we compare it to the actual, returns a bunch of booleans. 
If we turn those into floats, they will be 1.0s and 0.0s and the `mean` of those floats is the accuracy. 
The current accuracy, as expected, is around 10%, because it is random. 

Let's train our first neural net. 
Set a learning rate and do a few epochs. 
So we're going to go through each epoch and we're going to go through from  0 up to n. 
That's the 50,000 training rows.  
And skipping by 64, the batch size, each time.  
We create a slice that starts at `i`, so starting at 0, and goes up to  64,
unless we've gone past the end, in which case we'll just go up to n. 
We slice into our training set for the x and for the y to get our x and y batches.  

We will then calculate our predictions,  our loss function, and do our backward.
Once we've got  done backward, we can then, with `torch.no_grad()`, 
go through each layer and if that's a layer that has weights,  
we'll update them to the existing weights minus the gradients times the learning rate.  
And then zero out the gradients of the weights and biases. 
An underscore means do it in place.  So that sets this to zero.
There we go. It's finished.  

So you can see that our accuracy on  the training set, it's a bit unfair,  
but it's only three epochs, is nearly  97%. So we now have a digit recognizer.  
It trains pretty quickly and is not terrible  at all. So that's a pretty good starting point.
All right, so what we're going to do next time is  we're going to refactor this training loop to make  
it dramatically, dramatically, dramatically  simpler, step by step until, eventually,  
we will get it down to, so we'll get it down  to something much, much shorter. And then  
we're going to add a validation set to it and  a multi-processing data loader, and then, yeah,  
we'll be in a pretty good position, I think,  to start training some more interesting models.

Now, try to recreate them without peaking as much as possible. 
TODO: recreate matrix  multiply, forward and backward passes, something that steps through layers.
Recreate the idea of `.forward` and `.backward`.  
Make sure it's all in your head really clearly so that you fully understand what's going on. 
If you don't have time for that, pick  out a smaller part of that, 
the piece that you're   more interested in, or you could just go through  and look really closely at these notebooks. 
So if  you go to kernel, restart and clear output, it'll  delete all the outputs and like try to think like  
what are the shapes of things? 
Can you guess  what they are? Can you check them? And so forth.