Lesson 18 part 2

Changes to Notebook 9
Previously we were putting the learner object itself into self.learn in each callback,
and that meant we were using self.learn.model and self.learn.opt and self.learn.xxx 
As it was ugly, JH modified the learner to instead pass in the learner 
when it calls the Callback in run_cbs.
As it passes the learner as a parameter to the method, 
the learner no longer goes through the callbacks and sets .learn attribute.
Instead in our callbacks we have to put `learn` as an argument and all of the math in
all of the Callback methods.
For example, `deviceCB` has a before fit, so now it's got ( ..,learn).
It does make a lot of the code less yucky to not have all the `self.learn.xxx ... self.learn.yyy, etc.
Also it is good because we don't generally want to have the learner has a reference to the callbacks and
also the callbacks having a reference back to the learner.
That created a cycle.

A few other little changes we've made to the code. 
To find quickly all of the changes that we've made to the code in the last week.
We go to the course repo, on any repo, we can add /compare in GitHub,  
and then we can compare across different things.
One of the examples is to compare across different times, e.g., 
look at the master Branch now versus one day ago.
We can change versus seven days ago so just change in the URL to 7, and there are all the commits.
We can see the changes from last week, for example, all the self.learns became learns.

Calling the LRfinder is now easier because we added a `@patch` to the learner
`@fc.patch` decorator lets you take a function and it will turn that function into a method 
of the class we put after the colon (here 'Learner`).
This has created a new method called `lr_find` or `Learner.lr_find'.
`lr_find` calls `self.fit` where `self` is a `learner`,
passing in `max_epochs` as maximum number of epochs, 
what to start the learning rate 'start_lr`, 
and to use as `cbs` callbacks the LRFinderCB` callback.

Also new, `self.learn.fit` didn't used to have a callbacks parameter.
That is convenient because it adds those callbacks just during the `fit`.
If we pass in `cbs` to `fit` (see the `Learner` code),
then the `Learner.fit` goes through each `cb in cbs` and appends it `self.cbs.append(cb)`.
Then `finally:`, when it's finished fitting it removes them 
`for cb in cbs: self.cbs.remove(cb)`
Hence these are callbacks that are just added for the period of this one fit,
which is what we want for a LRfinder.
It should just be added for that one fit, so with this `@patch` in place,
it says this is all that's required to do the `LRfinder` is now to create your learner and call `lr_find`.

`@patch` is a very convenient thing. 
Python has a lot of "folk wisdom" about what is and isn't considered "pythonic" or good.
A lot of people don't like patching.
In other languages it's used widely and is considered good.
In this situation it's very nice to be able to add in this additional functionality to our class.

XXXX
Also added to the learner a few more parameters to fit.
It used to take just `n_epochs` and `cbs`. 
It now also has a `lr` learning rate parameter, so we can provide it to the constructor.
And we can override the learning rate `lr` for one `fit`.
If we pass in the learning rate `lr` it will use it, else it will use the `lr` passed into the constructor.
Also added booleans `train` and `valid` to say do you want to do the training loop 
and do you want to do the validation Loop.
By default it'll do both. 

TODO: think about why we didn't have to say `with torch.nograd` but instead call `torch.nograd()`
Great if you understand why that works and what it does.
It will help understanding of decorators.

NB: 13_ResNet.

In the 13_resnet NB, we do the usual import and setup initially.
The model that we've been using for a while it's a convolution, an activation and an optional batchNorm.
In our models we were using `batchNorm2d` and applying Ximing weight initialization.
Then convs that take the channels from 1 to 8 to 16 to 32 to 64,
and each one stride-2 and at the end a flatten, so we ended up with a 1x1.
That's been the model we've been using for a while.
The number of layers is 4 convolutional layers (1-8, 8-16, 16-32, 32-64),
with a maximum of 64 channels in the last one.
Can we beat our last accuracy?

Before we do a resnet, let's try to improve the architecture thoughtfully.
Generally more depth and more channels gives the neural net more opportunity to learn.
Since we're good at initializing our neural Nets and using batchNorm we should be able to do deeper.

Lets review the previous version so we can compare. 
We can go up to 128 parameters, we could make our first convolutional layer have a stride-1 
so that would go from 1 input channel to 8 output channels (filters).
That stride-1 allows us to have one extra layer, which can double the number of channels to 128,
making it a deeper and wider.

We can do a normal `BatchNorm2D` and `OneCircleLR` with our scheduler `sched`.
For the callbacks `cbs` we use the `DeviceCB` `metrics` `ProgressCB` and our activation stats `astats`
looking for `GeneralRelu`.
Wth this deeper and eventually wider Network get up to .92,
a big improvement with the only difference to our previous model is line of code :
`layers = [conv(1, 8, stride=1, act=act, norm=norm)]`
which allowed us to go `nfs=(8,16,32,64,128)`.
A small change but  the error rates gone down by over 10 percent.

Now we're going to make it deeper. 
But there comes a point where making neural Nets deeper stops working well.
Even with good initialization there comes a time where adding more layers becomes problematic.
Kaiming He pointed out something particularly interesting in "Deep residual learning for image recognition". 
Let's take a 20 layer ANN and train it for tens of thousands of iterations and track its test error.
Then let's do the same thing on an identical but deeper 56 layer Network.
The 56 layer Network had a worse error than the 20 layer.
It wasn't just a problem of generalizations it was worse on the training set too.
The Insight is that if we set the additional 36 layers to just identity matrices they would do nothing at all.
A 56 layer network is a superset of a 20 layer Network, so it should be at least as good.
But it is worse, so clearly the problem is something about training it.
Insight: create a 56 layer Network which has the same training dynamics as a 20 layer Network or even less.
They realized we could add a "shortcut connection".

Normally when we have our inputs coming into our convolution so let's say that's 
that was our inputs and here's our convolution and here's our outputs
If we do this 56 times that's a lot of stacked up convolutions which are Matrix multiplications 
with a lot of opportunity for gradient explosions and all that fun.
How could we make it so that we have convolutions but with the training dynamics of a much shallower Network?
Let's put two convs to make it twice as deep.
Let's add a skip connection, such that:
`out = conv2(conv1(in)) + in` 
(assume that these include activation functions)

If we initialize `conv2` and `conv1` first to have weights of zero,
then `out = + in` and therefore out equals in,
which is what we wanted, i.e., if there is no extra layers.
This way we end up with a network which can be deep but at least when we start
training behaves as if it's shallow.
This is called a <mark> residual connection>/mark>
because if we subtract in from both sides, out then we would get out minus n equals con one of conv2 within.
In other words the difference (`out-in`) between the end point and the starting point which is the <mark>residual</mark>.

This only works if we can add these together.
If `conv1` and `conv2` both have the same number of filters and also have stride-1, then that will work fine.
We end up with the same output shape as the input shape and you can add them together.
But if they are not the same shape, then we are in trouble.
The answer is to add an identity `conv(in)` and make it as simple as possible.
It's not really an identity,  but we're trying to make it as simple as possible,
and the simplest possible convolution is a one by one filter block (1x1 kernel).
Using that we can also add a stride or whatever if we want.

Lets create a `conv_block`, which does the 2 convs, with `ni` number of input filters,
`nf` number of output filters, `stride`, `act` activation functions,
possibly a normalization `norm` and possible a kernel size `ks`.
The second conv is going to go from output filters `nf` to Output filters `nf`
because the first conv is going to be from `ni` to `nf`.
By the time we get to the second column it's going to be NF to NF.
The first conv we set stride-1e and then the second conv will have the requested `stride`.
That way the two columns back to back are going to overall have the requested stride.
The combination of these two columns is going to take us from Ni to NF 
in terms of the number of filters. and it's going to have the `stride` that we requested.

A `conv_block` is a sequential block, consisting of a convolution followed by another convolution.
Each conv has the requested: kernel size `ks`, activation function `act`, and normalization layer `norm`.
But the second conv won't have an activation function. (explain why in a moment).
One way to make this as if it didn't exist would be to set the convolutional weights and biased to zero.
But we would like to have correctly randomly initialized weights.

If we're using `batchNorm` (`if norm:`) we can initialize `conv2[1]` we can initialize
the `batchNorm` weights to zero. (See implementation from scratch of batchNorm).
Because the `batchNorm.weight` is the thing we multiply by.
In batchNorm we subtract the exponential moving average mean, 
we divide by the exponential moving average standard deviation but then multiplied by weights
and add back the  batchNorms bias layer. 
If we set the batchNorm layers weights to zero, we're multiplying by zero,
and the initial conv block output will be just all zeros.
That's going to give us what we wanted, so we end up with just the input.

A `ResBlock` contains those convolutions in the `_conv_block`.
Then we need the `idconv`, which is going to be a `fc.noop`, (nothing)
if the number of channels in `ni` is equal to the number of channels out `nf`.
Otherwise, we use a convolution with `ks=1, stride=1`, which changes the number of filters so that they match.
What if the stride is not 1?
If the stride is 2, we average using `nn.AvgPool2d` (i.e., take the mean of every set of two items in the grid.
That is pool of idcon of n if te stride is 2 and the filter number is changed.
And that's the minimal amount of work.

In `forward` we get the input `x`.
On the identity connection `idconv` we call `pool(x)` and if `stride==1` that's a noop.
We do `idconv` and if `ni==nf` (number of filters has not changed), that's also a noop.
So `self.idconv` is just the input (in that situation).
We add that to the result of the convs (and here's something interesting) 
we then apply the activation function `.act` to the result of the whole resnet block.
(This is the reason we did not earlier add the `act` to the `conv2`)

We copied the `get_model` and everywhere that previously had a `conv` replaced it with a `ResBlock`.
Previously we started with `conv(1,8, ...)`, now we have  `ResBlock(1,8, ....)`
Then we added `conv(nfs[i], nfs[i+1], ...)`, now we have `ResBlock(nfs[i], nfs[i+1], ...)`,
so it's (exactly?) the same (stride=2 added?).
One change is that previously the very last `conv` at the end went from 128 filters down to 10 followed by `flatten`.
But this `conv` is working on a 1x1 input, so an alternate way is flatten first and then use a linear layer.
Because a `conv` on a 1x1 input is identical to a linear layer.

<mark> TODO: Think about why a `conv` on a 1x1 is the same.  Look at the `conv` from scratch we did.</mark> 

This is an important insight so it's very useful with a more complex model like this to take a good
look at it to see exactly what the inputs and outputs of each layer are.
`print_shape` takes the things that a `hook` takes.
We will print out for each layer the name of the class, the shape of the input and the shape of the output.
We can get our `model` create our learner `learn`, and use our `Hawks` context manager 
to call the `_print_shape` function.
Then we will call `fit` for one epoch just doing the evaluation. 

If we use the SingleBatch callback it'll just do a single batch pass it the hook and
print out each layer, the inputs shape and the output shape.
We're starting with an input of a batch size of 1024, 1 Channel, 28x28.
Our first `ResBlock` was stride-1 so we end up with 28x28, with eight filters.
Then we gradually decrease the grid size to 14x14, 7x7, 4x4, 2x2 to 1x1,
while we gradually increase the number of filters up to 256.
We then flatten it which gets rid of that 1x1 and allows us then to do linear, to count over the 10.
And then (JH found it useful in this case) so we've got a BatchNorm1d at the end.

We create a `@fc.patch` for `learner.summary` that would do the same thing as a markdown table.
If we create a trained learner with our model and called `.summary`, 
this method is now available because it's been patched into the Learner.
It's going to do the same thing as our `_print_shape` but using a markdown table
if it's in a notebook, otherwise it'll just print it.
*fastcore* has a handy thing for keeping track if you're in a notebook, (`fc.IN_NOTEBOOK`).
In a notebook to make something marked down we use `Markdown` from `Ipython.display`.
Also added the number of parameters, `nparms`, calculated by summing up the number 
of elements for each parameter in that module (`mod.parameters`).
At the end we can also print out the total number of parameters.
Here we've got a 1.2 million parameter model.

There are few parameters in the input, nearly all the parameters are actually in the last `ResBlock` layer.
Why? 
For every input Channel we have a set of parameters, they're all going to get added up 
across each of the 3x3 in the kernel.
And then that's going to be done for every output filter.

We call `LRfind` and get a sense of what kind of `lr` to use, e.g., `2e-2`.

Using Resnet we've gone up from .917 to .922 in five epochs.
And this resnet is the simplest `Resblock`. 
We just replaced each `conv` with `ResBlock`.
We've just been thoughtful about it. 

We can try other ResNets by grabbing `timm` (Ross Whiteman's Pytorch image model Library).
If we call `timm.list_models('resnet*')` there are a lot of resnets.
JH tried a few of them.
In the source code for *timm* you'll see that the various different
resnets, like `resnet18`, `resnet18D`,  `resnet10t`, etc. 
they're defined in a very elegant configuration, so we can see what's different.

Basically one line of code different between each different type of resnet for the main resnets.
JH tried all the 10 models.
Also tried importing the underlying pieces and building ResNets with them.
The best was `resnet18d`. 

Training it in exactly the same way JH got to .92, less than our .922.
A thoughtfully designed basic architecture goes a very long way.
It's better for this problem than any of the Pytorch image model resnets.
It shows that we can create a state-of-the-art architecture by using common sense.


NB 14_augmentation.

Before data augmentation lets try to improve our model.
We took the convnet and replaced it with a resnet so it's effectively twice as deep,
because each conv block has two convolutions. 
But resnets train better than convNets, so we could go deeper and wider.
Previously we were going from 8 up to 256. 
Could we get up to 512?
One way to do that would be to make our very first res block not have a kernel size `ks=3` 
but a `ks=5` so that each grid is going to be 5x5, 25 inputs.
So it's fair enough then to have 16 outputs.
If we `ks=5`, 16 outputs means if I keep doubling as before we end up at 512 rather than 256.
The only change was to add `ks=5` and then change to double or the sizes.
After training accuracy is .927. 

Lets try something to make our resnet more flexible.
The current resnet is a bit awkward as the number of stride-2 layers has to be big
enough that the last of them ends up with a 1x1 output so we can flatten it and do the linear.
That's not flexible, as if we've got something of different size.
28x28 is a pretty small image.
Lets create `get_model2` which goes less far, it has one less layer,
so it only goes up to 256 despite starting at 16. 
Because it's got one less layer, it's going to end up at 2x2 not 1x1. 
We can take the mean over the 2x2. 
A mean over the 2x2 it's going to give us batch size by Channel's output 
which is what we can then put into our linear layer.
This is called a global average pooling layer.
In Pytorch it's called an Adaptive average pooling layer, but n Pytorch we 
can cause it to have an output other than 1x1.
Nobody ever uses it that way so they're basically the same thing.
This is a little bit more convenient than the Pytorch version 
because we don't have to flatten it.
After our last resblock which gives us a 2x2 output we have global average pool,
which takes the mean, and then we can do the linear batchNorm as usual.

Lets improve the summary patch to include not only the number of parameters,
but also the approximate number of Megaflops. 
A flop is a floating operation per second. 
This calculation is not exact, it's not really flops, but actually counted the number of multiplications, which is indicative.
We added the `_flops` function where you pass in the weight Matrix and the height and the width of the grid.
If the number of dimensions of the weight Matrix is less than three then we're just doing like a linear layer or something.
Then the number of elements is the number of "flops" because it's just a matrix multiply.
If we're doing a convolution, so the dimension is four, then we 
actually do that Matrix multiply for everything in the height by width grid.
That's how we calculate this kind of flops equivalent number. 

When we run the model the number of parameters went from 1.2 million up to 4.9 million,
because of a `ResBlock` that gets all the way up to 512. 
The way we did this is we made a stride-1  layer.
So it's gone 2x2 and it stayed at 2x2, to make it as similar as possible with the last ones.
It's got the same 512 final number of filters, so most of the parameters are in that last block.
Interestingly, it's not as clear for the Megaflops. 
It is the greatest of them, but it has more parameters than all the other ones added together.
But that's not the same for the Megaflops, because the first (input) layer has to be done 28x28 times,
while the last ResBlock only has to be done 2x2 times.

Training that got a similar result .926

What would reduce the number of parameters (memory) and the megaflots (compute).
In `model3` lets remove the last ResBlock (that takes it up to 512), 
and the number of parameters goes down from 4.9 million to 1.2 million.
Not a big impact on the megaflops but a large reduction on the parameters.
On the first Resnet block, we have 5.3 megaflops because the 2nd conv of the ResBlock is going to be a 16x16 by 5x5.

This is showing how to investigate what's going on in a model.
TODO: try these investigations.

Can we make it faster?
The obvious place to look at is the first (input) Resnet block, because that's where most megaflops are,
because the second conv is 16x16 channels out by 5x5 kernels, and it's doing it across the whole 28x28 grid.
That's the bulk of the the biggest compute.
We could replace this first `ResBlock` with just one convolution, and we've got rid of the 16x16 by 5x5. 
We just got the 16x1 by 5x5 so the number of Megaflops has gone down from 18.3 to 13.3.

The number of parameters hasn't changed (it was only 6884).
Careful when people say "my model has less parameters", as that doesn't mean it's faster.
The relationship between parameters and speed, even counting Megaflops, doesn't always work well,
because it doesn't take account of the amount of things moving through memory.
`model4` has less Megaflops and about the same accuracy.
We've built a model that has less parameters and less Megaflops and has the same accuracy.
An important thing to keep in mind.
And this model is better than the resnet18d from `timm`. 
We've built something that is fast small and accurate.

What if we train for longer? 
If we train for 20 epochs the training accuracy gets up to .999, but the validation accuracy is worse, .924.
The reason is that after 20 epochs it's seen the same image so many times it's just memorizing them, and things go downhill.
We need to regularize.
We have claimed that we can regularize using weight decay.
But here weight Decay doesn't regularize at all.

If you use batchNorm
(*for years people didn't notice this, and then somebody wrote a paper that pointed this out and people wow that's weird*)
A batchNorm layer has a single set of coefficients which multiplies an entire layer.
That set of coefficients could just be the number 100 in every place.
That's going to multiply the entire previous weight Matrix or convolution kernel matrix by 100.
As far as weight Decay is concerned that's not much of an impact at all,
because the batchNorm layer has very few weights.
So it doesn't really have a huge impact on weight decay, 
but it massively increases the effective scale of the weight Matrix.

batchNorm lets the ANN "cheat" by increasing the coefficients the parameters 
nearly as much as it wants indirectly just by changing the batch normally as weights.
Weight decay is not going to save us. 
Important to recognize, weight Decay is not.
batchNorm layers I don't see the point of it.
There's been some studies of what it does and it does have some second order effects 
on the learning rate but I don't think you should rely on them.
We should use a scheduler for changing the learning rate rather than second order effects caused by weight decay.

Data augmentation: We're going to modify every image a little bit, by a random change,
so that the ANN doesn't see the same image each time.
No reason to implement these from scratch, we can look them up in Fastai,
but it's actually a separate area to what we're meant to be learning.
In fastAI vision augment we can see, for example, how to do flip, cropping and padding, etc.
FastAI probably has the best implementation but `torchvision`'s are fine, so we'll use them.

We've created before a `BatchTransformCB` callback for normalization.
We could create a transform batch function `tfm_batch` which transforms the inputs `tfm_x` 
and transforms the outputs `tfm_y` using two different functions.
That would be an augmentation callback.
We want to transform our X's using `tfms`, a `Sequential` module,
which first does a random crop `RandomCrop` and then a random horizontal flip `RandomHorizontalFlip`.
It is weird to randomly crop a 28x28 image to get a 28x28 image,  
but we can add padding to it and so it's going to randomly add padding on one or both sides
to do this kind of random crop.

The `BatchTransformCallback` has `on_train` and `on_val` so that it only does it 
if we say we want to do it on training and it's training,
or we want to do it on validation and it's not training.

Data augmentation shouldn't be done on validation so we set `on_val=False`.
First we use `SingleBbatchCB()` and `fit(1)` just doing training.
The we grab the batch out of the learner, to see exactly what the model sees.
This is not relying on any approximations.
When we fit it puts it in the batch that looks at into learn.batch.
So if we fit for a single batch we can then grab that batch back, and we can show images.
We can see a little crop it's added.
Notice that every single image in this batch (lets grab the first 16) has exactly the same augmentation,
as we're applying a batch transform.
This is good because it is running on the GPU, and it's really hard to get enough CPU to feed a fast GPU.
Particularly on some platforms (kaggle, collab) that are really underpowered for CPU.
This way all of Transformations augmentation is happening on the GPU.
On the downside, there's less variety as every minibatch has the same augmentation.
The downside may not matter because it's going to see lots of minibatches, and
each minibatch is going to have a different augmentation.
If we run this multiple times it's got a different augmentation in each minibatch.

We are going to use one padding, a small amount of data augmentation, 20 epochs using onecycleLR.
Takes a while to train and gets .940.
Asked Twitter if anybody beat this in 20 epochs any model/Library and nobody's got close.
In papers with code there are better models but use 250 or more epochs.
TODO: find a way to beat this in 20 epochs.
Since this takes a while to train, we should `torch.save` it so we can load that back later.

*Test Time Augmentation* (TTA)
TTA does the batch transform callback on validation too.
In this case just a very simple TTA.
We're going to add a batch transform callback that runs on validate.
It's not random, just does a horizontal flip.
We're going to create a new callback called `CapturePpreds()`.
Aafter each batch it's just going to append to a list `all_preds` the predictions, 
and it's going to append to a list `all_targs` the targets.
We can call `learn.fit(1, train=False, cbs=cp) and it will show us the accuracy.
This is the same number that we saw before but then we can call it the same thing
but this time with a different callback which is with the horizontal flip callback.

That way it does the same thing as before, but every time it's going to do a horizontal flip.
Accuracy is slightly higher.
And we've now got two sets of predictions: with and without the flipped version.
We could stack those together and take the average of the flipped and unflipped predictions.
That gives us a better result .942, which is better because we are looking at
the image from multiple (here 2) different directions, (flipped and unflipped), 
it gives it more opportunities to understand what this is a picture of.
This is a bit like Random Forest, it's a kind of bagging that we're doing.
We're getting multiple predictions and bringing them together.

.942 is the best 20 epoch result and notice we didn't have to do any additional training.
It still counts as a 20 Epoch result.
We can do test time augmentation with a much wider range of different augmentations that you trained with.
And then you can use them at test time too, crops, rotations, warps, etc.

Random Erasing
We're going to delete a little bit of each picture and replace it with some random gaussian noise.
In this case we've just got one patch but eventually we're going to do more than one patch.
We have to implement everything from scratch and this one's a bit less trivial than the previous transforms.
Let's grab a batch out of the training set, and let's grab the first 16 images,
Let's grab the mean and standard deviation.

Deleting a patch would change the statistics.
But if we replace the image with exactly the same mean and standard deviation pixels that the picture has,
then it won't change the statistics.
Let's say we want to delete `pct=0.2` of the height and width.
Let's find out how big that size is.
0.2 of the shape is the height and of the width that's the size of the X and Y.
Forn the starting point we're just going to randomly grab some starting point.
Here we've got the starting point for X `stx` is 14 and starting point for y `sty` is 0.
The patch is going to be a 5x5 spot.

We do a gaussian or normal initialization of our minibatch.
Everything in the batch, every channel for this x slice this y slice,
and we're going to initialize it with this mean and standard deviation normal random noise.
Don't start by writing a function, start by writing single lines of code that can run independently,
make sure that they work, e.g., look at the pictures.

One thing that's wrong here is that some of the backgrounds looks black and others gray.
At first this was confusing, what's the change because the original images didn't look like that 
The problem is that the minimum and the maximum have changed.
The previous `xbt.min` and `xbt.max` were -0.8 to 2, but now -3 to 3. 
The noise we added has the same mean and standard deviation but it doesn't have the same range,          
because the pixels were not normally distributed originally.
So normally distributed noise is wrong.

`_rand_erase1` fixes this. It does all the same as before,
but it `x.clamps_` the random pixels to be between `mn` and `mx`.
It makes sure that it doesn't change the range, important as the range impacts the activations a lot.
Now all of the backgrounds have that nice black.

And it's still giving us random pixels.
Because of the clamping the mean and standard deviation aren't 0-1, but they're very very close.
And `min` and `max` haven't changed because we clamped them.

That randomly erases one block, and we could create a random arrays which will randomly choose up to (e.g., 4 blocks)
That's what this data augmentation looks like.
We can create a class to do this data augmentation so you'll pass in what percentage to do in each block,
what is the maximum number of blocks to have it store that away,
and then in the forward we call the `rand_erase()` function passing in the input `x`, and the arguments.

Now we can use `RandomCrop` `RandomHorizontalFlip` and `RandErase`.
Now we're going to go all the way up to 50 epochs, and we get .946.

How do we actually get the correct distribution?
We don't end up with zero-one.
clamping it all feels a bit weird.
How do we replace these pixels with something that is guaranteed to be the correct distribution?
A: we can copy another part of the picture over to here.
If we copy another part of the picture we're guaranteed to have the correct distribution of pixels.
It wouldn't be random erasing, it would be random copying.
We're going to implement it manually.
Let's get our X Mini batch and let's get our our size and again 
let's get the X Y that this time we're not erasing we're copying.
We'll randomly get a different X Y to copy from.
Now instead of random noise we say replace this slice of the batch with this slice of the batch.
We end up with copied little bits across some.
We can't really see it all, some of them are black and it's replaced black..
I guess it's knocked off the end of this shoe, added a little bit extra here, etc.

Lets turn it into a function... once it was tested above) (in the repl??)
to make sure the function works.
And it's copying it largely from something that's largely black for a lot of them.

Again we do it multiple times, and get a couple of random copies.
Turn that into a class, create our transforms.
We look at a batch to make sure it looks sensible.
and do it for 25 epochs, gets to .94.
XXX
Why 25 epochs? Think how to the 50 Epoch record which was .946...
We could train for 25 epochs, and then train a whole new model for a different 25 epochs,
and put in a different Learner `learn2`. 
learn2 is 94.1 and `learn` was 94. 
We can ensemble them, grab the predictions of `learn` and `learn2`, 
stack them up and take their mean. 
The ensemble returns .944, better than either of the models.
We still didn't beat our best (.946) but it's a particularly useful trick.

Was trying to see if we can get a better result by using ensembling (with same number of epochs)
instead of training for longer.
We couldn't, maybe because the random copy is not as good, 
or maybe we are using too much augmentation.
"cut mix" is similar to this,  cut mix copies it from different images rather
than from the same image.

That brings us to the end of the lesson.
Even in our previous courses we've never gone from scratch step by step to an absolute
state-of-the-art model where we build everything ourselves, and it runs this quickly.
We're even using our own custom Resnet and using common sense at every stage.
Going up to larger datasets nothing changes on these techniques.
We should do most (99%) of the research on very small data sets,
because we can iterate much more quickly, we can understand them better.
And on a bigger dataset the findings continue to hold true.

Homework: create own own schedulers that work with Python's optimizers.
It's the tricky bit that will make sure that you understand the pytorch API well.
Study this carefully, create our own cosine annealing scheduler from scratch,
and then create our own onecycle scheduler from scratch,
and make sure that they work correctly with this batch scheduler callback.
This will be a very good exercise for you.
Hopefully getting extremely frustrated as things don't work the way you hoped,
they would and being mystified for a while and then working through it 
using a step-by-step approach, lots of experimentation lots of exploration.

And then figuring it out that's that's the journey I'm hoping you you have if it's all super easy and
and you get it first go then you know if I you have to find something else to do
um but um yeah I'm hoping you'll find it actually you know surprisingly tricky to get it
all working properly and in the process of doing so you're going to have to do a lot of exploration 
and experimentation but you'll realize that it requires no um like prerequisite knowledge at all okay so um
if it doesn't work first time it's not because there's something that you didn't learn in graduate school 
if I knew you had done a PhD whatever it's
just that you need to dig through you know slowly and carefully to see how it all works
and you know then see how Niche and concise you can get it
then the other homework is to try and beat me I really really want people to
beat me try to beat me on the 5e park or the 20 Epoch or the 50 Epoch fashion mnist 
ideally using mini AI with things that you've added yourself um
But we can try grabbing other libraries.
If you do grab another library and you can beat my Approach try to re-implement that library that way you
are still within the spirit of the game.

In our next couple of lessons jono, tanishq and I are going to be putting this all together to create a diffusion model
from scratch.
Not just a diffusion model but a variety of interesting generative approaches.
So we're kind of starting to come full circle.